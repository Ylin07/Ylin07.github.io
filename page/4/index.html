<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>


  <meta property="og:type" content="website">
<meta property="og:title" content="Ylin&#39;s Blog">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Ylin&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Ylin">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Ylin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ylin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/26/78-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/26/78-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4/" class="post-title-link" itemprop="url">78:初窥深度学习(4)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-26 13:33:07" itemprop="dateCreated datePublished" datetime="2025-08-26T13:33:07+08:00">2025-08-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-27 16:13:36" itemprop="dateModified" datetime="2025-08-27T16:13:36+08:00">2025-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我们先前实现了卷积神经网络的各层，以及基本的前向传播，现在我们要进一步的完善整个神经网络，通过反向传播实现对权重的更新，从而提高神经网络的准确性。</p>
<h2 id="反向传播">反向传播</h2>
<p>现在我们已经完成了神经网络的前向传播，现在我们需要对每个层进行反向传播以更新权重，来寻来你神经网络。进行反向传播，我们需要注意两点：</p>
<ul>
<li>在前向传播的阶段，我们需要在每一层换从它需要用于反向传播的数据（如中间值等）。这也反映了，任意反向传播的阶段，都需要有着相应的前向阶段。</li>
<li>在反向传播阶段，每一层都会接受一个梯度，并返回一个梯度。其接受其输出（<span
class="math inline">$\frac{\partial L}{\partial
out}$</span>）的损失梯度，并返回其输入（<span
class="math inline">$\frac{\partial L}{\partial
in}$</span>）的损失梯度</li>
</ul>
<p>我们的训练过程应该是这样的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">out = conv.forward((image / <span class="number">255</span>) - <span class="number">0.5</span>)</span><br><span class="line">out = pool.forward(out)</span><br><span class="line">out = softmax.forward(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化梯度</span></span><br><span class="line">gradient = np.zeros(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">gradient = softmax.backprop(gradient)</span><br><span class="line">gradient = pool.backprop(gradient)</span><br><span class="line">gradient = conv.backprop(gradient)</span><br></pre></td></tr></table></figure>
<p>现在我们将逐步构建我们的反向传播函数：</p>
<h3 id="softmax层反向传播">Softmax层反向传播</h3>
<p>我们的损失函数是： <span class="math display">$$
\begin{align*}
L = -ln(p_c)
\end{align*}
$$</span>
所以我们首先要计算的就是对于<code>softmax</code>层反向传播阶段的输入，其中<code>out_s</code>就是<code>softmax</code>层的输出。一个包含了10个概率的向量，我们只在乎正确类别的损失，所以我们的第一个梯度为：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial out_s(i)} =
\begin{cases}
0 \space\space\space\space\space \text{ if i!=c} \\
-\frac{1}{p_i} \text{ if i=c}
\end{cases}
\end{align*}
$$</span> 所以我们正确的初始梯度应该是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gradient = np.zeros(<span class="number">10</span>)</span><br><span class="line">gradient[label] = -<span class="number">1</span> / out[label]</span><br></pre></td></tr></table></figure>
<p>然后我们对softmax层的前向传播阶段进行一个缓存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">    <span class="comment"># 输入体积的形状</span></span><br><span class="line">    <span class="variable language_">self</span>.last_input_shape = <span class="built_in">input</span>.shape</span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.flatten()</span><br><span class="line">    <span class="comment"># 展平后的输入向量</span></span><br><span class="line">    <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line">    </span><br><span class="line">    totals = np.dot(<span class="built_in">input</span>,<span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">    <span class="comment"># 输出结果（提供给激活函数）</span></span><br><span class="line">    <span class="variable language_">self</span>.last_totals = totals</span><br><span class="line">    exp = np.exp(totals)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> exp/np.<span class="built_in">sum</span>(exp,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们可以开始准备softmax层的反向传播了：</p>
<h4 id="计算">计算</h4>
<p>我们已经计算出，损失对于激活函数值的梯度，我们现在需要进一步的推导，最终我们希望得到<span
class="math inline">$\frac{\partial L}{\partial input} \frac{\partial
L}{\partial w} \frac{\partial L}{\partial b}$</span></p>
<p>的梯度，以用于对权重的梯度训练。根据链式法则，我们应该有： <span
class="math display">$$
\begin{align*}
\frac{\partial L}{\partial w} &amp;= \frac{\partial L}{\partial out} *
\frac{\partial out}{\partial t} * \frac{\partial t}{\partial w} \\
\frac{\partial L}{\partial b} &amp;= \frac{\partial L}{\partial out} *
\frac{\partial out}{\partial t} * \frac{\partial t}{\partial b} \\
\frac{\partial L}{\partial input} &amp;= \frac{\partial L}{\partial out}
*
\frac{\partial out}{\partial t} * \frac{\partial t}{\partial input}
\end{align*}
$$</span> 其中
<code>t = w * input + b</code>，<code>out</code>则是softmax函数的输出值，我们可以依次求出。对于<span
class="math inline">$\frac{\partial L}{\partial out}$</span>我们有：
<span class="math display">$$
\begin{align*}
out_s(c) &amp;= \frac{e^{t_c}}{\sum_{i}e^{t_i}} = \frac{e^{t_c}}{S} \\
S &amp;= \sum_{i}e^{t_i} \\
\to out_s(c) &amp;= e^{t_c}S^{-1}
\end{align*}
$$</span> 现在我们求<span class="math inline">$\frac{\partial
out_s(c)}{\partial
t_k}$</span>，需要分别考虑<code>k=c</code>和<code>k!=c</code>的情况，我们依次进行求导：
<span class="math display">$$
\begin{align*}
\frac{\partial out_s(c)}{\partial t_k} &amp;= \frac{\partial
out_s(c)}{\partial S}
*\frac{\partial S}{\partial t_k} \\
&amp;= -e^{t_c}S^{-2}\frac{\partial S}{\partial t_k} \\
&amp;= -e^{t_c}S^{-2}(e^{t_k}) \\
&amp;= \frac{-e^{t_c}e^{t_k}}{S^2} \\
\\
\frac{\partial out_s(c)}{\partial t_c} &amp;=
\frac{Se^{t_c}-e^{t_c}\frac{\partial S}{\partial t_c}}{S^2} \\
&amp;= \frac{Se^{t_c}-e^{t_c}e^{t_c}}{S^2} \\
&amp;= \frac{e^{t_c}(S-e^{t_c})}{S^2} \\
\to
\frac{\partial out_s(k)}{\partial t} &amp;=
\begin{cases}
\frac{-e^{t_c}e^{t_k}}{S^2} \space\space\space\space \text{ if k!=c}  \\
\frac{e^{t_c}(S-e^{t_c})}{S^2} \text{ if k=c}
\end{cases}
\end{align*}
$$</span> 最后我们根据公式<code>t = w * input + b</code>得到: <span
class="math display">$$
\begin{align*}
\frac{\partial t}{\partial w}&amp;=input \\
\frac{\partial t}{\partial b}&amp;=1 \\
\frac{\partial t}{\partial input}&amp;=w
\end{align*}
$$</span> 现在我们可以用代码实现这个过程了</p>
<h4 id="实现">实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self,d_L_d_out</span>):</span><br><span class="line">    <span class="comment"># d_L_d_out是这一层的输出梯度,作为参数</span></span><br><span class="line">    <span class="comment"># 返回d_L_d_in作为下一层的参数</span></span><br><span class="line">    <span class="keyword">for</span> i,gradient <span class="keyword">in</span> <span class="built_in">enumerate</span>(d_L_d_out):</span><br><span class="line">        <span class="keyword">if</span> gradient == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># e^totals</span></span><br><span class="line">        t_exp = np.exp(<span class="variable language_">self</span>.last_totals)</span><br><span class="line">        <span class="comment"># S = sum(e^totals)</span></span><br><span class="line">        S = np.<span class="built_in">sum</span>(t_exp)</span><br><span class="line">        <span class="comment"># total对out[i]的梯度关系</span></span><br><span class="line">        <span class="comment"># 第一次是对所有的梯度进行更新</span></span><br><span class="line">        d_out_d_t = -t_exp[i]*t_exp / (S**<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 第二次是只对 =i 的梯度进行更新 从而使第一次的更新只针对 !=i 的梯度</span></span><br><span class="line">        d_out_d_t[i] = t_exp[i]*(S-t_exp[i]) / (S**<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 权重对total的梯度关系</span></span><br><span class="line">        d_t_d_w = <span class="variable language_">self</span>.last_input</span><br><span class="line">        d_t_d_b = <span class="number">1</span></span><br><span class="line">        d_t_d_input = <span class="variable language_">self</span>.weights</span><br><span class="line">        <span class="comment"># total对Loss的梯度关系</span></span><br><span class="line">        d_L_d_t = gradient * d_out_d_t</span><br><span class="line">        <span class="comment"># 权重对Loss的梯度关系</span></span><br><span class="line">        d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]</span><br><span class="line">        d_L_d_b = d_L_d_t * d_t_d_b</span><br><span class="line">        d_L_d_input = d_t_d_input @ d_L_d_t</span><br><span class="line">        <span class="comment"># 梯度训练</span></span><br><span class="line">    <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.learn_rate * d_L_d_w</span><br><span class="line">    <span class="variable language_">self</span>.biases -= <span class="variable language_">self</span>.learn_rate * d_L_d_b</span><br><span class="line">    <span class="comment"># 返回梯度</span></span><br><span class="line">    <span class="keyword">return</span> d_L_d_input.reshape(<span class="variable language_">self</span>.last_input_shape)</span><br></pre></td></tr></table></figure>
<p>由于softmax层的输入是一个输入体积，在一开始被我们展平处理了，但是我们返回的梯度也应该是一个同样大小的输入体积，所以我们需要通过<code>reshape</code>确保这层的返回的梯度和原始的输入格式相同。</p>
<p>我们可以测试一下softmax反向传播后的训练效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">from</span> maxpool <span class="keyword">import</span> MaxPool2</span><br><span class="line"><span class="keyword">from</span> softmax <span class="keyword">import</span> Softmax</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">test_images = x_test[:<span class="number">1000</span>]</span><br><span class="line">test_labels = y_test[:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">pool = MaxPool2()</span><br><span class="line">softmax = Softmax(<span class="number">13</span>*<span class="number">13</span>*<span class="number">8</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">image,label</span>):</span><br><span class="line">    out = conv.forward((image / <span class="number">255</span>) - <span class="number">0.5</span>)</span><br><span class="line">    out = pool.forward(out)</span><br><span class="line">    out = softmax.forward(out)</span><br><span class="line"></span><br><span class="line">    loss = -np.log(out[label])</span><br><span class="line">    acc = <span class="number">1</span> <span class="keyword">if</span> np.argmax(out) == label <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out,loss,acc</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">image,label</span>):</span><br><span class="line">    out, loss, acc = forward(image,label)</span><br><span class="line">    gradient = np.zeros(<span class="number">10</span>)</span><br><span class="line">    gradient[label] = -<span class="number">1</span> / out[label]</span><br><span class="line">    gradient = softmax.backprop(gradient)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss,acc</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start!&quot;</span>)</span><br><span class="line">loss = <span class="number">0</span></span><br><span class="line">num_correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,(im,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(test_images,test_labels)):</span><br><span class="line">    _, l, acc = forward(im,label)</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&#x27;[Step %d] Past 100 steps :Average Loss %.3f | Accuracy %d%%&#x27;</span> %</span><br><span class="line">            (i+<span class="number">1</span>,loss/<span class="number">100</span>,num_correct)</span><br><span class="line">        )</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        num_correct = <span class="number">0</span></span><br><span class="line">    l,acc = train(im,label)</span><br><span class="line">    loss += l</span><br><span class="line">    num_correct += acc</span><br></pre></td></tr></table></figure>
<p>可以看到准确率有明显的提升，说明我们softmax层的反向传播在很好的进行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Start!</span><br><span class="line">[Step <span class="number">100</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.112</span> | Accuracy <span class="number">24</span>%</span><br><span class="line">[Step <span class="number">200</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.940</span> | Accuracy <span class="number">37</span>%</span><br><span class="line">[Step <span class="number">300</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.686</span> | Accuracy <span class="number">50</span>%</span><br><span class="line">[Step <span class="number">400</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.606</span> | Accuracy <span class="number">51</span>%</span><br><span class="line">[Step <span class="number">500</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.451</span> | Accuracy <span class="number">58</span>%</span><br><span class="line">[Step <span class="number">600</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.362</span> | Accuracy <span class="number">65</span>%</span><br><span class="line">[Step <span class="number">700</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.264</span> | Accuracy <span class="number">66</span>%</span><br><span class="line">[Step <span class="number">800</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.057</span> | Accuracy <span class="number">75</span>%</span><br><span class="line">[Step <span class="number">900</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">0.978</span> | Accuracy <span class="number">81</span>%</span><br><span class="line">[Step <span class="number">1000</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">0.966</span> | Accuracy <span class="number">78</span>%</span><br></pre></td></tr></table></figure>
<h3 id="池化层传播">池化层传播</h3>
<p>在前向传播的过程中，最大池化层接收一个输入体积，然后通过2x2区域的最大池化，将宽度和高度都减半。而在反向传播中，执行相反的操作：我们将损失梯度的宽度和高度都翻倍，通过将每个梯度值分配到对应的2x2区域的最大值位置：</p>
<figure>
<img src="https://s2.loli.net/2025/08/27/EYAtgKoLmV4yhfU.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>每个梯度都被分配到原始最大值的位置，然后将其他梯度设置为0.</p>
<p>为什么是这这样的呢？在一个2x2区域中，由于我们只关注区域内的最大值，所以对于其他的非最大值，我们可以几乎忽略不计，因为它的改变对我们的输出结果没有影响，所以对于非最大像素，我们有<span
class="math inline">$\frac{\partial L}{\partial
inputs}=0$</span>。另一方面来看，最大像素的<span
class="math inline">$\frac{\partial output}{\partial
input}=1$</span>，这意味着<span class="math inline">$\frac{\partial
L}{\partial output}=\frac{\partial L}{\partial input}$</span></p>
<p>所以对于这一层的反向传播，我们只需要简单的还原，并且填充梯度值到最大像素区域就行了</p>
<h4 id="实现-1">实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self,d_L_d_out</span>):</span><br><span class="line">    <span class="comment"># 这里的self.last_input是前向阶段的数据缓存</span></span><br><span class="line">    d_L_d_input = np.zeros(<span class="variable language_">self</span>.last_input.shape)</span><br><span class="line">    <span class="keyword">for</span> im_region,i,j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">        h,w,f = im_region.shape</span><br><span class="line">        amax = np.amax(im_region,axis=(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i2 <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">            <span class="keyword">for</span> j2 <span class="keyword">in</span> <span class="built_in">range</span>(w):</span><br><span class="line">                <span class="keyword">for</span> f2 <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                    <span class="comment"># 搜寻区域内的最大值并赋梯度值</span></span><br><span class="line">                    <span class="keyword">if</span> im_region[i2,j2,f2] == amax[f2]:</span><br><span class="line">                        d_L_d_input[i*<span class="number">2</span>+i2, j*<span class="number">2</span>+j2,f2] = d_L_d_out[i,j,f2]</span><br><span class="line">    <span class="keyword">return</span> d_L_d_input</span><br></pre></td></tr></table></figure>
<p>这一部分并没有什么权重用来训练，所以只是一个简单的数据还原。</p>
<h3 id="卷积层反向传播">卷积层反向传播</h3>
<p>卷积层的反向传播，我们需要的是卷积层中的滤波器的损失梯度，因为我们需要利用损失梯度来更新我们滤波器的权重，我们现在已经有了<span
class="math inline">$\frac{\partial L}{\partial
output}$</span>，我们现在只需要计算<span
class="math inline">$\frac{\partial output}{\partial
filters}$</span>，所以我们需要知道，改变一个滤波器的权重会怎么影响到卷积层的输出？</p>
<p>实际上修改滤波器的任意权重都可能会导致滤波器输出的整个图像，下面便是很好的示例：</p>
<figure>
<img src="https://s2.loli.net/2025/08/27/iKdATqH2W1Xro37.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/27/AhM2BWCHNaULFDQ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>同样的对任何滤波器权重+1都会使输出增加相应图像像素的值，所以输出像素相对于特定滤波器权重的导数应该就是相应的图像元素。我们可以通过数学计算来论证这一点</p>
<h4 id="计算-1">计算</h4>
<p><span class="math display">$$
\begin{align*}
out(i,j) &amp;= convolve(image,filiter) \\
&amp;= \sum_{x=0}^3{}\sum_{y=0}^{3}image(i+x,j+y)*filiter(x,y) \\
\to \frac{\partial out(i,j)}{\partial filiter(x,y)} &amp;=image(i+x,i+y)
\end{align*}
$$</span></p>
<p>我们将输出的损失梯度引进来，我们就可以获得特定滤波器权重的损失梯度了：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial filiter(x,y)} =
\sum_{i}\sum_{j}\frac{\partial L}{\partial out(i,j)} *
\frac{\partial out(i,j)}{\partial filter(x,y)}
\end{align*}
$$</span> 现在我们可以实现我们卷积层的反向传播了：</p>
<h4 id="实现-2">实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">    d_L_d_filters = np.zeros(<span class="variable language_">self</span>.filters.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_filters):</span><br><span class="line">            d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.filters -= <span class="variable language_">self</span>.learn_rate * d_L_d_filters</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>现在我们可以对我们的神经网络进行一个完整的训练了，我们可以看到训练的结果如下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/27/jWL4NGbgkKSMmnq.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>效果还是非常不错的。</p>
<h2 id="完善">完善</h2>
<p>和之前的网络不同，CNN的训练集比较庞大，如果每次启动都要训练遍参数就太麻烦了，所以我们可以再每次训练之后将参数保存下来。下次再要使用就可以直接加载而不用重复训练。所以我们可以编写保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelSaver</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">&#x27;MNIST_CNN&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.model_name = model_name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;conv_filters&#x27;</span>: conv.filters,</span><br><span class="line">            <span class="string">&#x27;softmax_weights&#x27;</span>: softmax.weights,</span><br><span class="line">            <span class="string">&#x27;softmax_biases&#x27;</span>: softmax.biases</span><br><span class="line">        &#125;</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(data, f)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;保存参数到<span class="subst">&#123;filename&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                data = pickle.load(f)</span><br><span class="line"></span><br><span class="line">            conv.filters = data[<span class="string">&#x27;conv_filters&#x27;</span>]</span><br><span class="line">            softmax.weights = data[<span class="string">&#x27;softmax_weights&#x27;</span>]</span><br><span class="line">            softmax.biases = data[<span class="string">&#x27;softmax_biases&#x27;</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;模型参数加载成功&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;无可用模型参数&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>如果我们想要自己尝试手写输入，来测试模型的效果，我们可能希望有个手写板，所以我们可以再写一个手写板的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DrawingBoard</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="variable language_">self</span>.root = root</span><br><span class="line">        <span class="variable language_">self</span>.root.title(<span class="string">&quot;画板&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建一个 Canvas 作为画板</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas = tk.Canvas(root, width=<span class="number">280</span>, height=<span class="number">280</span>, bg=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.pack()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绑定鼠标事件</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas.bind(<span class="string">&quot;&lt;B1-Motion&gt;&quot;</span>, <span class="variable language_">self</span>.paint)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化绘图工具</span></span><br><span class="line">        <span class="variable language_">self</span>.image = Image.new(<span class="string">&quot;RGB&quot;</span>, (<span class="number">280</span>, <span class="number">280</span>), <span class="string">&quot;white&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.draw = ImageDraw.Draw(<span class="variable language_">self</span>.image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化画笔颜色和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_color = <span class="string">&quot;black&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_width = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加输出按钮</span></span><br><span class="line">        <span class="variable language_">self</span>.output_button = tk.Button(root, text=<span class="string">&quot;输出&quot;</span>, command=<span class="variable language_">self</span>.output_and_exit)</span><br><span class="line">        <span class="variable language_">self</span>.output_button.pack()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">paint</span>(<span class="params">self, event</span>):</span><br><span class="line">        x1, y1 = (event.x - <span class="variable language_">self</span>.brush_width), (event.y - <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        x2, y2 = (event.x + <span class="variable language_">self</span>.brush_width), (event.y + <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.create_oval(x1, y1, x2, y2, fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line">        <span class="variable language_">self</span>.draw.ellipse([x1, y1, x2, y2], fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_image</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 将图像调整为 28x28 像素</span></span><br><span class="line">        processed_image = <span class="variable language_">self</span>.image.resize((<span class="number">28</span>, <span class="number">28</span>), Image.Resampling.LANCZOS)</span><br><span class="line">        processed_image = ImageOps.grayscale(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将图像转换为 NumPy 数组</span></span><br><span class="line">        image_array = np.array(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确保像素值是整数</span></span><br><span class="line">        image_array = image_array.astype(np.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">output_and_exit</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 处理图像并获取数组</span></span><br><span class="line">        <span class="variable language_">self</span>.image_array = <span class="variable language_">self</span>.process_image()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存图像</span></span><br><span class="line">        processed_image = Image.fromarray(<span class="variable language_">self</span>.image_array)</span><br><span class="line">        processed_image.save(<span class="string">&quot;temp.png&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;图片已保存为 temp.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 退出程序</span></span><br><span class="line">        <span class="variable language_">self</span>.root.destroy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在我们就可以使用它了，我们先进行训练，然后用保存的参数，来进行手写数字识别，我把整个网络的源代码放在下面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>:</span><br><span class="line">    <span class="comment"># 使用3x3滤波器的卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_filters, learn_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_filters = num_filters</span><br><span class="line">        <span class="variable language_">self</span>.filters = np.random.randn(num_filters, <span class="number">3</span>, <span class="number">3</span>) / <span class="number">9</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.learn_rate = learn_rate</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self, image</span>):</span><br><span class="line">        <span class="comment"># 返回所有可以卷积的3x3的图像区域</span></span><br><span class="line">        h, w = image.shape</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(w - <span class="number">2</span>):</span><br><span class="line">                im_region = image[i:(i + <span class="number">3</span>), j:(j + <span class="number">3</span>)]</span><br><span class="line">                <span class="keyword">yield</span> im_region, i, j</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 执行卷积层的前向传播 输出一个26x26x8的三维输出数组</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line">        h, w = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h - <span class="number">2</span>, w - <span class="number">2</span>, <span class="variable language_">self</span>.num_filters))</span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            output[i, j] = np.<span class="built_in">sum</span>(im_region * <span class="variable language_">self</span>.filters, axis=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">        d_L_d_filters = np.zeros(<span class="variable language_">self</span>.filters.shape)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_filters):</span><br><span class="line">                d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.filters -= <span class="variable language_">self</span>.learn_rate * d_L_d_filters</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaxPool2</span>:</span><br><span class="line">    <span class="comment"># 池化尺寸为2的最大池化层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self, image</span>):</span><br><span class="line">        h, w, _ = image.shape</span><br><span class="line">        new_h = h // <span class="number">2</span></span><br><span class="line">        new_w = w // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">                im_region = image[i * <span class="number">2</span>:(i + <span class="number">1</span>) * <span class="number">2</span>, j * <span class="number">2</span>:(j + <span class="number">1</span>) * <span class="number">2</span>]</span><br><span class="line">                <span class="keyword">yield</span> im_region, i, j</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line">        h, w, num_filters = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h // <span class="number">2</span>, w // <span class="number">2</span>, num_filters))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            output[i, j] = np.amax(im_region, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">        d_L_d_input = np.zeros(<span class="variable language_">self</span>.last_input.shape)</span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">            h, w, f = im_region.shape</span><br><span class="line">            amax = np.amax(im_region, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i2 <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">                <span class="keyword">for</span> j2 <span class="keyword">in</span> <span class="built_in">range</span>(w):</span><br><span class="line">                    <span class="keyword">for</span> f2 <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                        <span class="keyword">if</span> im_region[i2, j2, f2] == amax[f2]:</span><br><span class="line">                            d_L_d_input[i * <span class="number">2</span> + i2, j * <span class="number">2</span> + j2, f2] = d_L_d_out[i, j, f2]</span><br><span class="line">        <span class="keyword">return</span> d_L_d_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Softmax</span>:</span><br><span class="line">    <span class="comment"># 全连接softmax激活层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_len, nodes, learn_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(input_len, nodes) / nodes</span><br><span class="line">        <span class="variable language_">self</span>.biases = np.zeros(nodes)</span><br><span class="line">        <span class="variable language_">self</span>.learn_rate = learn_rate</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.last_input_shape = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_totals = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.last_input_shape = <span class="built_in">input</span>.shape</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.flatten()</span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line">        totals = np.dot(<span class="built_in">input</span>, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">        <span class="variable language_">self</span>.last_totals = totals</span><br><span class="line">        exp = np.exp(totals)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> exp / np.<span class="built_in">sum</span>(exp, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">        <span class="comment"># d_L_d_out是这一层的输出梯度,作为参数</span></span><br><span class="line">        <span class="comment"># 返回d_L_d_in作为下一层的参数</span></span><br><span class="line">        d_L_d_w = np.zeros(<span class="variable language_">self</span>.weights.shape)</span><br><span class="line">        d_L_d_b = np.zeros(<span class="variable language_">self</span>.biases.shape)</span><br><span class="line">        d_L_d_input = np.zeros(<span class="variable language_">self</span>.last_input.shape)</span><br><span class="line">        <span class="keyword">for</span> i, gradient <span class="keyword">in</span> <span class="built_in">enumerate</span>(d_L_d_out):</span><br><span class="line">            <span class="keyword">if</span> gradient == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># e^totals</span></span><br><span class="line">            t_exp = np.exp(<span class="variable language_">self</span>.last_totals)</span><br><span class="line">            <span class="comment"># S = sum(e^totals)</span></span><br><span class="line">            S = np.<span class="built_in">sum</span>(t_exp)</span><br><span class="line">            <span class="comment"># total对out[i]的梯度关系</span></span><br><span class="line">            <span class="comment"># 第一次是对所有的梯度进行更新</span></span><br><span class="line">            d_out_d_t = -t_exp[i] * t_exp / (S ** <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 第二次是只对 =i 的梯度进行更新 从而使第一次的更新只针对 !=i 的梯度</span></span><br><span class="line">            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 权重对total的梯度关系</span></span><br><span class="line">            d_t_d_w = <span class="variable language_">self</span>.last_input</span><br><span class="line">            d_t_d_b = <span class="number">1</span></span><br><span class="line">            d_t_d_input = <span class="variable language_">self</span>.weights</span><br><span class="line">            <span class="comment"># total对Loss的梯度关系</span></span><br><span class="line">            d_L_d_t = gradient * d_out_d_t</span><br><span class="line">            <span class="comment"># 权重对Loss的梯度关系</span></span><br><span class="line">            d_L_d_w += d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]</span><br><span class="line">            d_L_d_b += d_L_d_t * d_t_d_b</span><br><span class="line">            d_L_d_input += d_t_d_input @ d_L_d_t</span><br><span class="line">            <span class="comment"># 梯度训练</span></span><br><span class="line">        <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.learn_rate * d_L_d_w</span><br><span class="line">        <span class="variable language_">self</span>.biases -= <span class="variable language_">self</span>.learn_rate * d_L_d_b</span><br><span class="line">        <span class="comment"># 返回梯度</span></span><br><span class="line">        <span class="keyword">return</span> d_L_d_input.reshape(<span class="variable language_">self</span>.last_input_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelSaver</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">&#x27;MNIST_CNN&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.model_name = model_name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;conv_filters&#x27;</span>: conv.filters,</span><br><span class="line">            <span class="string">&#x27;softmax_weights&#x27;</span>: softmax.weights,</span><br><span class="line">            <span class="string">&#x27;softmax_biases&#x27;</span>: softmax.biases</span><br><span class="line">        &#125;</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(data, f)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;保存参数到<span class="subst">&#123;filename&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                data = pickle.load(f)</span><br><span class="line"></span><br><span class="line">            conv.filters = data[<span class="string">&#x27;conv_filters&#x27;</span>]</span><br><span class="line">            softmax.weights = data[<span class="string">&#x27;softmax_weights&#x27;</span>]</span><br><span class="line">            softmax.biases = data[<span class="string">&#x27;softmax_biases&#x27;</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;模型参数加载成功&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;无可用模型参数&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tkinter <span class="keyword">as</span> tk</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image,ImageDraw,ImageOps</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DrawingBoard</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="variable language_">self</span>.root = root</span><br><span class="line">        <span class="variable language_">self</span>.root.title(<span class="string">&quot;画板&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建一个 Canvas 作为画板</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas = tk.Canvas(root, width=<span class="number">280</span>, height=<span class="number">280</span>, bg=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.pack()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绑定鼠标事件</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas.bind(<span class="string">&quot;&lt;B1-Motion&gt;&quot;</span>, <span class="variable language_">self</span>.paint)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化绘图工具</span></span><br><span class="line">        <span class="variable language_">self</span>.image = Image.new(<span class="string">&quot;RGB&quot;</span>, (<span class="number">280</span>, <span class="number">280</span>), <span class="string">&quot;white&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.draw = ImageDraw.Draw(<span class="variable language_">self</span>.image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化画笔颜色和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_color = <span class="string">&quot;black&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_width = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加输出按钮</span></span><br><span class="line">        <span class="variable language_">self</span>.output_button = tk.Button(root, text=<span class="string">&quot;输出&quot;</span>, command=<span class="variable language_">self</span>.output_and_exit)</span><br><span class="line">        <span class="variable language_">self</span>.output_button.pack()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">paint</span>(<span class="params">self, event</span>):</span><br><span class="line">        x1, y1 = (event.x - <span class="variable language_">self</span>.brush_width), (event.y - <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        x2, y2 = (event.x + <span class="variable language_">self</span>.brush_width), (event.y + <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.create_oval(x1, y1, x2, y2, fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line">        <span class="variable language_">self</span>.draw.ellipse([x1, y1, x2, y2], fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_image</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 将图像调整为 28x28 像素</span></span><br><span class="line">        processed_image = <span class="variable language_">self</span>.image.resize((<span class="number">28</span>, <span class="number">28</span>), Image.Resampling.LANCZOS)</span><br><span class="line">        processed_image = ImageOps.grayscale(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将图像转换为 NumPy 数组</span></span><br><span class="line">        image_array = np.array(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确保像素值是整数</span></span><br><span class="line">        image_array = image_array.astype(np.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">output_and_exit</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 处理图像并获取数组</span></span><br><span class="line">        <span class="variable language_">self</span>.image_array = <span class="variable language_">self</span>.process_image()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存图像</span></span><br><span class="line">        processed_image = Image.fromarray(<span class="variable language_">self</span>.image_array)</span><br><span class="line">        processed_image.save(<span class="string">&quot;temp.png&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;图片已保存为 temp.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 退出程序</span></span><br><span class="line">        <span class="variable language_">self</span>.root.destroy()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/25/77-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/25/77-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3/" class="post-title-link" itemprop="url">77:初窥深度学习(3)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-25 11:07:52" itemprop="dateCreated datePublished" datetime="2025-08-25T11:07:52+08:00">2025-08-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-26 13:28:22" itemprop="dateModified" datetime="2025-08-26T13:28:22+08:00">2025-08-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一篇文章中我们学习了循环神经网络，我们现在已经基本理解了神经网络怎么去处理数据/序列。可是对于图片、音频、文件之类的数据，我们该怎么去处理呢？相较于数据、序列，对图片使用传统神经网络会导致更大的开销。其他的数据类型也是同理，所以接下来我们将要认识<strong>卷积神经网络</strong>。</p>
<h1 id="卷积神经网络简介">卷积神经网络简介</h1>
<p>卷积神经网络的一个经典应用场景是对图像进行分类，可是我们可不可以使用普通的神经网络来实现呢？可以，但是没必要。对于图像数据处理，我们需要面临两个问题：</p>
<ul>
<li><strong>图像数据很大</strong>
假如我们要处理的图像大小是100x100甚至更大。那么构建一个处理100x100的彩色图像的神经网络，我们需要<code>100x100x3 = 30000</code>个输入特征。我们用一个1024个节点的中间层，意味着我们在一层中就要训练<code>30000x1024 = 30720000</code>个权重。这样会导致我们的神经网络十分庞大</li>
<li><strong>图像特征的位置会改变</strong>
同一个特征可能是在图像中的不同位置，你可能可以训练出一个对于特定图像表现良好的网络。但是当你对图像进行一定的偏移，可能就会导致结果发生错误的改变</li>
</ul>
<p>使用传统的神经网络来解决图像问题，无异于是浪费的。它忽视了图像中任意像素与其邻近像素的上下文关系，图像中的物体是由小范围的局部特征组成的，对每个像素都进行分析，是毫无意义的。</p>
<p>所以我们需要使用卷积神经网络来解决这些问题。</p>
<h2 id="目标">目标</h2>
<p>这一次我们的目标是实现一个手写数字识别的卷积神经网络，用到的是MNIST的手写数字数据集。也就是给定一个图像，将其分类为一个数字。</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/sMCW7taiUzfoTLE.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>MNIST数据集中的每张图片都是28*28的大小，包含一个居中的灰度数字。我们将根据这个数据集来对神经网络进行训练。</p>
<h2 id="卷积">卷积</h2>
<p>我们首先要理解卷积神经网络中的<strong>卷积</strong>是什么意思。卷积实际上是一种加权平均的操作。它的相当于一个滤波器，能够提取原始数据中的某种特定特征。我们往往使用卷积核来进行这个操作。</p>
<p>而神经网络中的卷积层则是根据过滤器实现对局部特征的处理，我们以下面这个操作为例：</p>
<p>对于一个垂直特征的卷积核，我们可以计算出这里的特征值</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/vHyiLxACUaI37Sz.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/25/ANzH7ilPxuv2hY9.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们们可以通过对图像中的数据进行卷积操作从而实现对局部特征的提取。这就和我们将要用到的卷积核有关了。</p>
<h3 id="卷积核">卷积核</h3>
<figure>
<img src="https://s2.loli.net/2025/08/25/Z9CguILrM1hWP7l.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这是一个垂直sobel滤波器，通过它对图像进行卷积操作，我们可以提取出图像的垂直特征：</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/Nrh93wdMmyjUDzK.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>同样的，我们有对应的水平SObel卷积核，可以提取出图像的水平特征：</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/mLjtAvOKxquh9yb.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/25/J7UdFDqpAXrI3CZ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>而Sobel滤波器，我们可以理解成边缘检测器。通过提取手写数字边缘的特征，有利于网络在后续更好的进行图像识别。</p>
<h3 id="填充">填充</h3>
<p>对于卷积这一步，我们对一个4x4的输入图像使用一个3x3的滤波器，我们会得到一个2x2的输出图像。如果我们希望输出图像和输入图像保持相同的大小。我们则需要向周围添加0，使得滤波器可以在更多的位置上覆盖</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/C93UiGz2PS4dLMV.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这种操作，我们称之为相同填充。如果不适用任何填充，我们称之为有效填充。</p>
<h3 id="卷积层的使用">卷积层的使用</h3>
<p>我们现在知道卷积层通过使用一组滤波器将输入图像转换为输出图像的卷积层了。我们将使用一个具有8个滤波器的小卷积层作为我们网络中的起始层，意味着，它将28x28的输入图像转换为26x26x8的输出体积：</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/tLZaDENvhopsPH2.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>每个卷积层的8个过滤器产生一个26x26的输出，这是因为我们用到的是3x3的卷积核作为我们的滤波器，所以我们需要训练的权重有<code>3x3x8 = 72</code>个权重</p>
<h3 id="实现">实现</h3>
<p>现在我们尝试用代码实现一个卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>:</span><br><span class="line">    <span class="comment"># 使用3x3滤波器的卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_filters</span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_filters = num_filters</span><br><span class="line">        <span class="comment"># 这里除以3是为了对权重进行初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.filters = np.random.randn(num_filters,<span class="number">3</span>,<span class="number">3</span>) / <span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>我们注意到我们对生成的卷积核中做了一个权重初始化的工作，这是因为：</p>
<ul>
<li>如果初始权重太大，那么输入数据经过卷积计算之后会变得很大，在反向传播的过程中梯度值也会变得很大，从而导致参数无法收敛，即<strong>梯度爆炸</strong></li>
<li>如果初始权重太小，由于激活函数的作用，输入的数据会层层缩小，导致反向传播过程中的梯度值变得绩效。难以实现对权重的有效更新，我们称之为<strong>梯度消失</strong></li>
</ul>
<p>这里我们用到Xavier初始化来解决这个问题，他指出，在保持网络层在初始化时，其输入核和输出的方差应该尽可能的相同。这样信号就可以在网络中稳定的传播。</p>
<p>我们设输入为y输出为x，权重矩阵为W。则有： <span
class="math display">$$
\begin{align*}
Var(W) = \frac{1}{n_{in}}
\end{align*}
$$</span>
其中<code>n_in</code>是输入的节点数量，这里就是<code>3x3</code>，所以初始化时需要<code>/9</code></p>
<p>接下来是实际的卷积部分的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>:</span><br><span class="line">    <span class="comment"># 使用3x3滤波器的卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_filters</span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_filters = num_filters</span><br><span class="line">        <span class="variable language_">self</span>.filters = np.random.randn(num_filters,<span class="number">3</span>,<span class="number">3</span>) / <span class="number">9</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self,image</span>):</span><br><span class="line">        <span class="comment"># 返回所有可以卷积的3x3的图像区域</span></span><br><span class="line">        h,w = image.shape</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h-<span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(w-<span class="number">2</span>):</span><br><span class="line">                im_region = image[i:(i+<span class="number">3</span>),j:(j+<span class="number">3</span>)]</span><br><span class="line">                <span class="keyword">yield</span> im_region,i,j</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 执行卷积层的前向传播 输出一个26x26x8的三维输出数组</span></span><br><span class="line">        h,w = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h-<span class="number">2</span>,w-<span class="number">2</span>,<span class="variable language_">self</span>.num_filters))</span><br><span class="line">        <span class="keyword">for</span> im_region,i,j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            <span class="comment"># 这里用到的是numpy中隐藏的广播机制，详情参考numpy</span></span><br><span class="line">            <span class="comment"># 这里im_region*self.filters的大小是(8,3,3),求和是对行列求和,所以axis=(1,2)</span></span><br><span class="line">            output[i,j] = np.<span class="built_in">sum</span>(im_region * <span class="variable language_">self</span>.filters,axis=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这里我们很多用法涉及到<code>numpy</code>的一些高级使用，可以在这里参考<a
target="_blank" rel="noopener" href="https://numpy.net.cn/learn/">NumPy</a>现在我们可以检查我们的卷积层是否输出了我们理想的结果:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf		<span class="comment"># 由于MNIST数据集URL地址有问题，所以这里使用keras库</span></span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">output = conv.forward(x_train[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(output.shape)	<span class="comment"># (26,26,8) </span></span><br></pre></td></tr></table></figure>
<h2 id="池化">池化</h2>
<p>图像中的相邻元素往往是相似的，所以卷积层输出中，通常相邻元素产生相似的值。结果导致卷积层输出中包含了大量的冗余信息。为了解决这个问题我们需要对数据进行<strong>池化</strong></p>
<p>它所做的事情很简单，往往是将输出中的值聚合称为更小的尺寸。池化往往是通过简单的操作，如<code>max,min,average</code>实现的。比如下面就是一个池化大小为2的最大池化操作</p>
<figure>
<img src="https://s2.loli.net/2025/08/26/lrUcqBJ4A6MItC7.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>池化将输入的宽度和高度除以池化大小。在我们的卷积神经网络中，我们将在初始卷积层之后放置一个池化大小为2的最大池化层，池化层将<code>26x26x8</code>的输入转化为<code>13x13x8</code>的输出：</p>
<figure>
<img src="https://s2.loli.net/2025/08/26/nkzo2HSwGaCx367.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="实现-1">实现</h3>
<p>我们现在用代码实现和conv类相似的<code>MaxPool2</code>类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaxPool2</span>:</span><br><span class="line">    <span class="comment"># 池化尺寸为2的最大池化层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self,image</span>):</span><br><span class="line">        h,w,_ = image.shape</span><br><span class="line">        new_h = h // <span class="number">2</span></span><br><span class="line">        new_w = w // <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">                im_region = image[i*<span class="number">2</span>:(i+<span class="number">1</span>)*<span class="number">2</span>,j*<span class="number">2</span>:(j+<span class="number">1</span>)*<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">yield</span> im_region,i,j</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        h,w,num_filters = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h//<span class="number">2</span>,w//<span class="number">2</span>,num_filters))        </span><br><span class="line">        <span class="keyword">for</span> im_region,i,j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            <span class="comment"># 这里im_region的大小是(3,3,8)因此我们只需要对行列求最大，故axis=(0,1)</span></span><br><span class="line">            output[i,j] = np.amax(im_region,axis=(<span class="number">0</span>,<span class="number">1</span>))            </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这个类和之前实现的<code>Conv3x3</code>类类似，关键在于从一个给定的图像区域中找到最大值，我们使用数组的最大值方法<code>np.amax()</code>来实现。我们来测试一下池化层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">from</span> maxpool <span class="keyword">import</span> MaxPool2</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">pool = MaxPool2()</span><br><span class="line"></span><br><span class="line">output = conv.forward(x_train[<span class="number">0</span>])</span><br><span class="line">output = pool.forward(output)</span><br><span class="line"><span class="built_in">print</span>(output.shape)	<span class="comment"># (13,13,8)</span></span><br></pre></td></tr></table></figure>
<h2 id="softmax层">Softmax层</h2>
<p>现在我们通过前两层，已经提取出了数字特征，现在我们希望能够赋予其实际预测的能力。对于多分类问题，我们通常使用<code>Softmax</code>层作为最终层——这是一个使用<code>Softmax</code>函数作为激活函数的全连接层（全连接层就是每个节点都与前一层的每个输入相联）</p>
<p>我们将使用一个包含10个节点的<code>Softmax</code>层作为CNN的最后一层，每个节点代表一个数字。层中的每个节点都连接到之前的输出中。在Softmax变化之后，概率最高的数字就是我们的输出。</p>
<figure>
<img src="https://s2.loli.net/2025/08/26/sCU9YwafPzxJlZ4.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="交叉熵损失">交叉熵损失</h3>
<p>我们现在既然可以输出最终的预测结果了，它输出的结果是一个概率，用来量化神经网络的对其预测的信心。同样的，我们也需要一种方法来量化每次预测的损失。这里我们使用交叉熵损失来解决这个问题：
<span class="math display">$$
\begin{align*}
L = -ln(p_c)
\end{align*}
$$</span> 其中c指的是正确的类别，即正确的数字。而<span
class="math inline"><em>p</em><sub><em>c</em></sub></span>代表类别c的预测概率。我们希望损失越低越好，对网络的损失进行量化，有利于后续的神经网络训练。</p>
<h3 id="实现-2">实现</h3>
<p>我们同上步骤，实现一个<code>Softmax</code>层类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Softmax</span>:</span><br><span class="line">    <span class="comment"># 全连接softmax激活层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_len, nodes</span>):</span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(input_len,nodes) / nodes</span><br><span class="line">        <span class="variable language_">self</span>.biases = np.zeros(nodes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 由于输入是一个输入体积,我们用flatten将其展平,变成一个一维的输出向量</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.flatten()</span><br><span class="line">        totals = np.dot(<span class="built_in">input</span>,<span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">        exp = np.exp(totals)</span><br><span class="line">        <span class="keyword">return</span> exp/np.<span class="built_in">sum</span>(exp,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>现在，我们已经完成了CNN的整个前向传播，我们可以简单的测试一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">from</span> maxpool <span class="keyword">import</span> MaxPool2</span><br><span class="line"><span class="keyword">from</span> softmax <span class="keyword">import</span> Softmax</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">test_images = x_test[:<span class="number">1000</span>]</span><br><span class="line">test_labels = y_test[:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">pool = MaxPool2()</span><br><span class="line">softmax = Softmax(<span class="number">13</span>*<span class="number">13</span>*<span class="number">8</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">image,label</span>):</span><br><span class="line">    out = conv.forward((image / <span class="number">255</span>) - <span class="number">0.5</span>)</span><br><span class="line">    out = pool.forward(out)</span><br><span class="line">    out = softmax.forward(out)</span><br><span class="line"></span><br><span class="line">    loss = -np.log(out[label])</span><br><span class="line">    acc = <span class="number">1</span> <span class="keyword">if</span> np.argmax(out) == label <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out,loss,acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start!&quot;</span>)</span><br><span class="line">loss = <span class="number">0</span></span><br><span class="line">num_correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,(im,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(test_images,test_labels)):</span><br><span class="line">    _, l, acc = forward(im,label)</span><br><span class="line">    loss += l</span><br><span class="line">    num_correct += acc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&#x27;[Step %d] Past 100 steps :Average Loss %.3f | Accuracy %d%%&#x27;</span> %</span><br><span class="line">            (i+<span class="number">1</span>,loss/<span class="number">100</span>,num_correct)</span><br><span class="line">        )</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        num_correct = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>我们可以得到下面的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Start!</span><br><span class="line">[Step <span class="number">100</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.566</span> | Accuracy <span class="number">13</span>%</span><br><span class="line">[Step <span class="number">200</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.447</span> | Accuracy <span class="number">13</span>%</span><br><span class="line">[Step <span class="number">300</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.500</span> | Accuracy <span class="number">13</span>%</span><br><span class="line">[Step <span class="number">400</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.520</span> | Accuracy <span class="number">10</span>%</span><br><span class="line">[Step <span class="number">500</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.431</span> | Accuracy <span class="number">9</span>%</span><br><span class="line">[Step <span class="number">600</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.477</span> | Accuracy <span class="number">6</span>%</span><br><span class="line">[Step <span class="number">700</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.491</span> | Accuracy <span class="number">11</span>%</span><br><span class="line">[Step <span class="number">800</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.598</span> | Accuracy <span class="number">7</span>%</span><br><span class="line">[Step <span class="number">900</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.545</span> | Accuracy <span class="number">7</span>%</span><br><span class="line">[Step <span class="number">1000</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.610</span> | Accuracy <span class="number">10</span>%</span><br></pre></td></tr></table></figure>
<p>这是因为我们对权重进行了随机初始化，所以现在神经网络的表现更像是随机猜测，所以准确率趋近于10%</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/24/76-%E5%AF%B9%E6%9A%91%E5%81%87%E7%9A%84%E5%8F%8D%E6%80%9D%E4%B8%8E%E4%B8%8B%E4%B8%AA%E5%AD%A6%E6%9C%9F%E7%9A%84%E5%B1%95%E6%9C%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/24/76-%E5%AF%B9%E6%9A%91%E5%81%87%E7%9A%84%E5%8F%8D%E6%80%9D%E4%B8%8E%E4%B8%8B%E4%B8%AA%E5%AD%A6%E6%9C%9F%E7%9A%84%E5%B1%95%E6%9C%9B/" class="post-title-link" itemprop="url">76:对暑假的反思与下个学期的展望</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-24 10:42:33" itemprop="dateCreated datePublished" datetime="2025-08-24T10:42:33+08:00">2025-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-29 13:47:38" itemprop="dateModified" datetime="2025-11-29T13:47:38+08:00">2025-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%88%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">月报</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天是8月24号，暑假已然过去了2/3，我打算明天就提前返校。我想对我的暑假做一个总结，这个暑假学了很多东西，相较于平时有更多的集中连续的时间可以用了学习，提升个人的能力。在校期间，我更多是用一些零散的时间来通过做项目来学习。但感觉难以长时间的专注在一个知识点上。始终是项目驱动学习，被动的去了解一些内容，暑假的话有大把的时间，可以去了解自己感兴趣的内容。我这个暑假主要学习了这些内容：</p>
<ul>
<li>处理器 及其 流水线机制</li>
<li>程序性能优化简单了解</li>
<li>存储器层次结构的认识</li>
<li>简单算法入门 与 少量的刷题</li>
<li>链接编译过程</li>
<li>深度学习简单神经网络的构造 与 原理认识</li>
<li>异常控制流机制</li>
<li>Linux shell编程 以及 简单指令的使用</li>
</ul>
<p>这么一看学到的东西很多，但是大多数都是浅尝辄止，我打算多多接触之后再来明确自己感兴趣的方向。接下来是对我暑假的批评和反思。从7.3暑假开始到8.24今天，一共是52天，自由可支配（除去睡觉吃饭休息）的时间大概有600+个小时，我每次的学习时长可能也就4~5个小时，零零散散学了25天左右。剩下的时间浪费在打游戏，看动漫，看电影，看小说上面。</p>
<p>我暑假前定了很多目标，基本一个都没达到，超级灰心。但是，这也是意料之中的事情，取乎其上、得乎其中。所以我打算早点返校，好好用这最后的几天拯救一下我的暑假。但是怎么评价这个暑假呢，一直以来我都对自己有着很高的期望，但是通过这个暑假，我很好的认识清楚了自己的懒惰性，我也不太喜欢这样，但是自控力还是太差了，我也会好好的反思自己的行为。其次是这个暑假开拓了很多眼界，我意识到了和别人的差距，我总是把目光拘于学校，年级，班级，寝室，甚至是自己。我在网上看到了，认识到了很多优秀的人，感受到了差距，所以我打算继续前行，去学习各种感兴趣的内容。</p>
<p>对于下个学期的，我也是略有想法。下个学期课程十分繁重，但是大多是专业课程。这个是我的优势，专业课程内容我有很好的基础，我打算不跟着学校的计划走，我打算跟着南京大学的课程设计进行学习，计组方面打算跟着它们的PA学习，操作系统打算跟着JYY老师的课程深入了解一下。数据结构与算法，我打算制定一个刷题计划，要保证每天有一定的算法学习时间。还有科研方面，我打算联系LLF老师尝试写出我的第一篇论文。暑假我看着两位学长保研，我一直以为保研就是对绩点要求很高，但是实际上本科期间的科研成果是及其重要的，我这个暑假一直疏于对这方面的学习。下个学期，我打算花大部分的时间在这上面。</p>
<p>总结下来，下个学期的几个主要任务：</p>
<ul>
<li>利用专业知识的优势，提升绩点成绩，加深对专业知识的掌握理解</li>
<li>提高算法能力，争取在下一学年中能够掌握常见的算法题型</li>
<li>深入学习逆向工程和二进制漏洞审计的，多刷题，强化竞赛能力</li>
<li>掌握最基本的科研能力，要开始入手自己的第一篇论文</li>
<li>学习计算机网络，完善技术栈</li>
</ul>
<p>不过我还是很期待9月份的到来，一个是9.3有大阅兵，我特别想看。还有一个是丝之歌发售，我特别想玩，我打算一出就开始玩，通宵玩，累了就睡，饿了就吃，醒了就玩。我初步估计可能通关需要20~30小时，也就是<code>[9.3,9.6]</code>好好玩四天。剩下的时间就对暑期的计划做一个收尾。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/17/75-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/17/75-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2/" class="post-title-link" itemprop="url">75:初窥深度学习(2)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-17 14:14:27" itemprop="dateCreated datePublished" datetime="2025-08-17T14:14:27+08:00">2025-08-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-24 10:41:42" itemprop="dateModified" datetime="2025-08-24T10:41:42+08:00">2025-08-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上一篇博客中，我们完成了一个简单的前馈神经网络，完成了对根据身高体重对性别进行猜测的神经网络，以及对他的训练。但是我们不该止步于此，接下来我们将尝试编写一个RNN循环神经网络，并认识它背后的原理。</p>
<h1 id="循环神经网络简介">循环神经网络简介</h1>
<p>循环神经网络是一种专门用于处理序列的神经网络，因此其对于处理文本方面十分有效。且对于前馈神经网络和卷积神经网络，我们发现：它们都只能处理预定义的尺寸——接受固定大小的输入并产生固定大小的输出。但是循环神经网络可以处理任意长度的序列，并返回。它可以是这样的：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/d2UGIDEpmSKtrv9.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这种处理序列的方式可以实现很多功能。例如，文本翻译，事件评价…
我们的目标是让它完成对一个评论的判断（是正面的还是负面的）。将待分析的文本输入神经网络然后，然后给出判断。</p>
<h2 id="实现方式">实现方式</h2>
<p>我们考虑一个输入为<code>x0,x1,x2,...,xn</code>，输出为<code>y0,y1,y2,..,yn</code>的多对多循环神经网络。这些xi和yi是向量，可以是任意维度。RNNs通过迭代更新一个隐藏状态<code>h</code>，重复这些步骤：</p>
<ul>
<li>下一个隐藏状态h<sub>t</sub>是前一个状态h<sub>t-1</sub>和下一个输入x<sub>t</sub>计算得出的</li>
<li>输出y<sub>t</sub>是由当前的隐藏状态h<sub>t</sub>计算得出的</li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/08/14/BbSIeWQf2KVi1Na.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这就是RNNs为什么是<strong>循环</strong>神经网络的原因，对于上面步骤的每一步中，都使用的是同一个权重。对于一个典型的RNNs，我们只需要使用3组权重就可以计算：</p>
<ul>
<li><strong>W<sub>xh</sub></strong> 用于所有x<sub>t</sub> -&gt;
h<sub>t</sub>的连接</li>
<li><strong>W<sub>hh</sub></strong> 用于所有h<sub>t-1</sub> -&gt;
h<sub>t</sub>的连接</li>
<li><strong>W<sub>hy</sub></strong> 用于所有h<sub>t</sub> -&gt;
y<sub>t</sub>的连接</li>
</ul>
<p>同时我们还需要为两次输出设置偏置：</p>
<ul>
<li><strong>b<sub>h</sub></strong> 计算h<sub>t</sub>时的偏置</li>
<li><strong>b<sub>y</sub></strong> 计算y<sub>t</sub>时的偏置</li>
</ul>
<p>我们将权重表示为矩阵，将偏置表示为向量，从而组合成整个RNNs。我们的输出是：
<span class="math display">$$
\begin{align*}
h_t &amp;= tanh(W_{xh}x_t + W_{hh}h_{t-1}+b_h) \\
y_t &amp;= W_{hy}h_t + b_y
\end{align*}
$$</span>
我们使用<code>tanh</code>作为隐藏状态的激活函数，其图像函数如下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/sg3wfaHoRbJqPeZ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="目标与计划">目标与计划</h2>
<p>我们要从头实现一个RNN，执行一个情感分析任务——判断给定的文本是正面消息还是负面的。</p>
<p>这是我们要用的训练集：<a
href="%5Brnn-from-scratch/data.py%20at%20master%20·%20vzhou842/rnn-from-scratch%5D(https://github.com/vzhou842/rnn-from-scratch/blob/master/data.py)">data</a></p>
<p>下面是一些训练集的样例：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/zHjVRN6fu9Msmlh.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>由于这是一个分类问题，所以我们使用多对一的循环神经网络，即最终只使用最终的隐藏状态来生成一个输出。每个<code>xi</code>都是一个代表文本中一个单词的向量。输出<code>y</code>是一个二维向量，分别代表正面和负面。我们最终使用<code>softmax</code>将其转换为概率。</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/CiG5xHMEQAy1RTV.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="数据集预处理">数据集预处理</h2>
<p>神经网络无法直接识别单词，我们需要处理数据集，让它变成能被神经网络使用的数据格式。首先我们需要收集一下数据集中所有单词的词汇表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = <span class="built_in">list</span>(<span class="built_in">set</span>([w <span class="keyword">for</span> text <span class="keyword">in</span> train_data.keys() <span class="keyword">for</span> w <span class="keyword">in</span> text.split(<span class="string">&quot; &quot;</span>)]))</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure>
<p><code>vocab</code>是一个包含训练集中出现的所有的单词的列表。接下来，我们将为每一个词汇中的单词都分配一个整数索引，因为神经网络无法理解单词，所以我们要创造一个单词和整数索引的关系：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_to_idx = &#123;w:i <span class="keyword">for</span> i,w <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">idx_to_word = &#123;i:w <span class="keyword">for</span> i,w <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br></pre></td></tr></table></figure>
<p>我们还要注意循环神经网络接收的每个输入都是一个向量<code>xi</code>，我们需要使用<code>one-hot</code>编码，将我们的每一个输入都转换成一个向量。对于一个<code>one-hot</code>向量，每个词汇对应于一个唯一的向量，这种向量出了一个位置外，其他位置都是0，在这里我们将每个<code>one-hot</code>向量中的<code>1</code>的位置，对应于单词的整数索引位置。</p>
<p>也就是说，我们的词汇表中有n个单词，我们的每个输入<code>xi</code>就应该是一个n维的<code>one-hot</code>向量。我们写一个函数，以用来创建向量输入，将其作为神经网络的输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">createInputs</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> text.split(<span class="string">&quot; &quot;</span>):</span><br><span class="line">        v = np.zeros((vocab_size,<span class="number">1</span>))	<span class="comment"># 创建一个vocab_size*1的全零向量</span></span><br><span class="line">        v[word_to_idx[w]] = <span class="number">1</span></span><br><span class="line">        inputs.append(v)</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br></pre></td></tr></table></figure>
<h2 id="向前传播">向前传播</h2>
<p>现在我们开始实现我们的RNN，我们先初始化我们所需的3个权重和2个偏置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> randn	<span class="comment"># 正态分布随机函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size, hidden_size=<span class="number">64</span></span>):</span><br><span class="line">        <span class="comment"># weights</span></span><br><span class="line">        <span class="variable language_">self</span>.Whh = randn(hidden_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Wxh = randn(hidden_size,input_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Why = randn(output_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="comment"># biases</span></span><br><span class="line">        <span class="variable language_">self</span>.bh = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.by = np.zeros((output_size,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>我们通过<code>np.random.randn()</code>从标准正态分布中初始化我们的权重。接下来我们将根据公式：
<span class="math display">$$
\begin{align*}
h_t &amp;= tanh(W_{xh}x_t + W_{hh}h_{t-1}+b_h) \\
y_t &amp;= W_{hy}h_t + b_y
\end{align*}
$$</span> 实现我们的向前传播函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">    h = np.zeros((<span class="variable language_">self</span>.Whh.shape[<span class="number">0</span>],<span class="number">1</span>)) <span class="comment"># 在刚开始我们的h是零向量，在此之前没有先前的h</span></span><br><span class="line">    <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        h = np.tanh(<span class="variable language_">self</span>.Wxh @ x + <span class="variable language_">self</span>.Whh @ h + <span class="variable language_">self</span>.bh) <span class="comment"># @是numpy中的矩阵乘法符号</span></span><br><span class="line">    y = <span class="variable language_">self</span>.Why @ y + <span class="variable language_">self</span>.by</span><br><span class="line">    <span class="keyword">return</span> y,h</span><br></pre></td></tr></table></figure>
<p>现在我们的RNNs神经网络已经可以运行了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> randn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createInputs</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> text.split(<span class="string">&quot; &quot;</span>):</span><br><span class="line">        v = np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">        v[word_to_idx[w]] = <span class="number">1</span></span><br><span class="line">        inputs.append(v)</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / <span class="built_in">sum</span>(np.exp(x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size, hidden_size=<span class="number">64</span></span>):</span><br><span class="line">        <span class="comment"># weights</span></span><br><span class="line">        <span class="variable language_">self</span>.Whh = randn(hidden_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Wxh = randn(hidden_size,input_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Why = randn(output_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="comment"># biases</span></span><br><span class="line">        <span class="variable language_">self</span>.bh = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.by = np.zeros((output_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        h = np.zeros((<span class="variable language_">self</span>.Whh.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">            h = np.tanh(<span class="variable language_">self</span>.Wxh @ x + <span class="variable language_">self</span>.Whh @ h + <span class="variable language_">self</span>.bh)</span><br><span class="line">        y = <span class="variable language_">self</span>.Why @ h + <span class="variable language_">self</span>.by</span><br><span class="line">        <span class="keyword">return</span> y,h</span><br><span class="line"></span><br><span class="line">RNNs = RNN(vocab_size,<span class="number">2</span>)</span><br><span class="line">inputs = createInputs(<span class="string">&#x27;i am very good&#x27;</span>)</span><br><span class="line">out, h = RNNs.forward(inputs)</span><br><span class="line">probs = softmax(out)</span><br><span class="line"><span class="built_in">print</span>(probs)</span><br><span class="line"><span class="comment"># [[0.50000228],[0.49999772]]</span></span><br></pre></td></tr></table></figure>
<p>这里我们用到了softmax函数，softmax函数可以将任意的实值转换为概率（主要用于多分类任务）。它的核心作用是将网络的原始输出，转换为各类别的概率，使得所有概率之和为1。其公式如下
<span class="math display">$$
\begin{align*}
Softmax(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C}e^{z_j}}
\end{align*}
$$</span></p>
<h2 id="反向传播">反向传播</h2>
<p>为了训练我们RNNs，我们首先需要选择一个损失函数。对于分类模型，Softmax函数经常和交叉熵损失函数配合使用。它的计算方式如下：
<span class="math display">$$
\begin{align*}
L = -ln(p_c)
\end{align*}
$$</span>
其中<code>pc</code>是我们的RNNs对正确类别的预测概率（正面或负面）。例如，如果一个正面文本被我们的RNNs预测为90%的正面，那么可以计算出损失为：
<span class="math display">$$
\begin{align*}
L = -ln(0.90) = 0.105
\end{align*}
$$</span></p>
<p>既然有损失函数了，我们就可以使用梯度下降来训练我们的RNN以减小损失。</p>
<h3 id="计算">计算</h3>
<p>首先从计算<span class="math inline">$\frac{\partial L}{\partial
y}$</span>开始，我们有： <span class="math display">$$
\begin{align*}
L &amp;= -ln(p_c) = -ln(softmax(y_c)) \\
\frac{\partial L}{\partial y} &amp;= \frac{\partial L}{\partial p_c}*
\frac{\partial p_c}{\partial y_i} \\
\frac{\partial L}{\partial p_c} &amp;= -\frac{1}{p_c} \\
\frac{\partial p_c}{\partial y_i} &amp;= \begin{cases}
\frac{\partial p_i}{\partial y_i} =
\frac{e^{y_i}\sum_{j}e^{y_j}-(e^{y_i})^2}{(\sum_{j}e^{y_j})^2} =
p_i(1-p_i)&amp;\text{if c=i} \\
\frac{\partial p_c}{\partial y_i} =
\frac{e^{y_i}\sum_{j}e^{y_j}-(e^{y_i})^2}{(\sum_{j}e^{y_j})^2} =
-p_cp_i&amp;\text{if c!=i}
\end{cases} \\
\frac{\partial L}{\partial y} &amp;= \begin{cases}
-\frac{1}{p_i} * p_i(1-p_i) = p_i-1 &amp; \text{if c=i} \\
-\frac{1}{p_c} * (-p_cp_i) = p_i &amp; \text{if c!=i}
\end{cases}
\end{align*}
$$</span>
接下来我们尝试对<code>Why</code>和<code>by</code>的梯度，它们将最终隐藏状态转换为RNNs的输出。我们有：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial W_{hy}} &amp;=
\frac{\partial L}{\partial y} *\frac{\partial y}{\partial W_{hy}} \\
y &amp;= W_{hy}h_n + b_y \\
\\
\frac{\partial y}{\partial W_{hy}} &amp;= h_n \to
\frac{\partial L}{\partial W_{hy}} = \frac{\partial L}{\partial y}h_n \\
\frac{\partial y}{\partial b_{y}} &amp;= 1 \to
\frac{\partial L}{\partial b_{y}} = \frac{\partial L}{\partial y}
\end{align*}
$$</span>
最后我们还需要<code>Whh,Wxh</code>和<code>bh</code>的梯度。由于梯度在每一步中都会被使用，所以根据时间展开和链式法则，我们有：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial W_{xh}} &amp;=
\frac{\partial L}{\partial y}\sum_{t=1}^{T}\frac{\partial y}{\partial
h_t}*\frac{\partial h_t}{\partial W_{xh}} \\
\end{align*}
$$</span>
这是因为L会被<code>y</code>所影响，而<code>y</code>被<code>hT</code>所影响，而<code>hT</code>又依赖于<code>h(T-1)</code>直到递归到<code>h1</code>，因此<code>Wxh</code>通过所有中间状态影响到L，所以在任意时间t，<code>Wxh</code>的贡献为：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial W_{xh}} \Big|_t &amp;=
\frac{\partial L}{\partial y}*\frac{\partial y}{\partial
h_t}*\frac{\partial h_t}{\partial W_{xh}} \\
\end{align*}
$$</span> 现在我们对其进行计算： <span class="math display">$$
\begin{align*}
h_t &amp;= tanh(W_{xh}x_t + W_{hh}h_{t-1}+b_h) \\
\frac{dtanh(x)}{dx} &amp;= 1-tanh^2(x) \\
\\
\frac{\partial h_t}{\partial W_{xh}} &amp;= (1-h_t^2)x_t \\
\frac{\partial h_t}{\partial W_{hh}} &amp;= (1-h_t^2)h_{t-1} \\
\frac{\partial h_t}{\partial b_h} &amp;= (1-h_t^2) \\
\end{align*}
$$</span> 最后我们需要计算出<span class="math inline">$\frac{\partial
y}{\partial h_t}$</span>。我们可以递归的计算它： $$ $$
由于我们是反向训练的，<span class="math inline">$\frac{\partial
y}{\partial h_{t+1}}$</span>是已经计算的最后一步的梯度<span
class="math inline">$\frac{\partial y}{\partial
h_n}=W_{hh}$</span>。至此为止我们的推导就结束了</p>
<h3 id="实现">实现</h3>
<p>由于反向传播训练需要用到前向传播中的一些数据，所以我们将其进行存储：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">    h = np.zeros((<span class="variable language_">self</span>.Whh.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 数据存储</span></span><br><span class="line">    <span class="variable language_">self</span>.last_inputs = inputs</span><br><span class="line">    <span class="variable language_">self</span>.last_hs = &#123;<span class="number">0</span>:h&#125;</span><br><span class="line">    <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        h = np.tanh(<span class="variable language_">self</span>.Wxh @ x + <span class="variable language_">self</span>.Whh @ h + <span class="variable language_">self</span>.bh)</span><br><span class="line">        <span class="variable language_">self</span>.last_hs[i+<span class="number">1</span>] = h   <span class="comment"># 更新存储</span></span><br><span class="line">    y = <span class="variable language_">self</span>.Why @ h + <span class="variable language_">self</span>.by</span><br><span class="line">    <span class="keyword">return</span> y,h</span><br></pre></td></tr></table></figure>
<p>现在我们可以开始实现<code>backprop()</code>了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self,d_y,learn_rate=<span class="number">2e-2</span></span>):</span><br><span class="line">    <span class="comment"># d_y: 是损失函数对于输出的偏导数 d_L/d_y 的结果</span></span><br><span class="line">    n = <span class="built_in">len</span>(<span class="variable language_">self</span>.last_inputs)</span><br><span class="line">    <span class="comment"># 计算dL/dWhy,dL/dby</span></span><br><span class="line">    d_Why = d_y @ <span class="variable language_">self</span>.last_hs[n].T</span><br><span class="line">    d_by = d_y</span><br><span class="line">    <span class="comment"># 初始化dL/dWhh,dL/dWxh,dL/dbh为0</span></span><br><span class="line">    d_Whh = np.zeros(<span class="variable language_">self</span>.Whh.shape)</span><br><span class="line">    d_Wxh = np.zeros(<span class="variable language_">self</span>.Wxh.shape)</span><br><span class="line">    d_bh = np.zeros(<span class="variable language_">self</span>.bh.shape)</span><br><span class="line">    <span class="comment"># 计算dL/dh</span></span><br><span class="line">    d_h = <span class="variable language_">self</span>.Why.T @ d_y  <span class="comment"># 因为dy/dh = Why 所以 dL/dh = Why * dL/dy</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随时间的反向传播</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(n)):</span><br><span class="line">        <span class="comment"># 通用数据 dL/dh * (1-h^2)</span></span><br><span class="line">        temp = (d_h * (<span class="number">1</span> - <span class="variable language_">self</span>.last_hs[t+<span class="number">1</span>] ** <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># dL/db = dL/dh * (1-h^2)</span></span><br><span class="line">        d_bh += temp</span><br><span class="line">        <span class="comment"># dL/dWhh = dL/dh * (1-h^2) * h_&#123;t-1&#125;</span></span><br><span class="line">        d_Whh += temp @ <span class="variable language_">self</span>.last_hs[t].T</span><br><span class="line">        <span class="comment"># dL/dWxh = dL/dh * (1-h^2) * x</span></span><br><span class="line">        d_Wxh += temp @ <span class="variable language_">self</span>.last_inputs[t].T</span><br><span class="line">        <span class="comment"># Next dL/dh = dL/dh * (1-h^2) * Whh</span></span><br><span class="line">        d_h = <span class="variable language_">self</span>.Whh @ temp</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度剪裁(防止梯度过大导致梯度爆炸问题)</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> [d_Wxh,d_Whh,d_Why,d_by,d_bh]:</span><br><span class="line">        np.clip(d,-<span class="number">1</span>,<span class="number">1</span>,out=d)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降训练</span></span><br><span class="line">    <span class="variable language_">self</span>.Whh -= learn_rate * d_Whh</span><br><span class="line">    <span class="variable language_">self</span>.Wxh -= learn_rate * d_Wxh</span><br><span class="line">    <span class="variable language_">self</span>.Why -= learn_rate * d_Why</span><br><span class="line">    <span class="variable language_">self</span>.bh -= learn_rate * d_bh</span><br><span class="line">    <span class="variable language_">self</span>.by -= learn_rate * d_by</span><br></pre></td></tr></table></figure>
<p>由于这一部分的编写涉及到矩阵的变换，所以在编写时，一定要清楚每个变量的状态，以免造成数学错误。例如，以上程序中<code>@</code>的左乘右乘顺序不能随意改变。</p>
<h2 id="训练">训练</h2>
<p>我们现在需要写一个接口，将我们的数据”喂”给神经网络，并量化损失和准确率，用于训练我们的神经网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">processData</span>(<span class="params">data, backprop=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 打乱数据集 避免顺序偏差</span></span><br><span class="line">    items = <span class="built_in">list</span>(data.items())</span><br><span class="line">    random.shuffle(items)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    num_correct =<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> items:</span><br><span class="line">        inputs = createInputs(x)</span><br><span class="line">        target = <span class="built_in">int</span>(y)</span><br><span class="line">        <span class="comment"># 前向传播计算</span></span><br><span class="line">        out,_ = RNN.forward(inputs)</span><br><span class="line">        probs = softmax(out)</span><br><span class="line">        <span class="comment"># 计算损失与准确度</span></span><br><span class="line">        loss -= np.log(probs[target])</span><br><span class="line">        num_correct += <span class="built_in">int</span>(np.argmax(probs) == target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> backprop:</span><br><span class="line">            d_L_d_y = probs</span><br><span class="line">            d_L_d_y[target] -= <span class="number">1</span></span><br><span class="line">            RNN.backprop(d_L_d_y)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss/<span class="built_in">len</span>(data),num_correct /<span class="built_in">len</span>(data)</span><br></pre></td></tr></table></figure>
<p>这里对于<span class="math inline">$\frac{\partial L}{\partial
y}$</span>的初始化我们需要重点关注一下。由于我们使用的是，交叉熵损失+Softmax函数来进行处理。对于输出层，我们有一个简洁的表达式来进行处理:
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial y} = probs - onehot(target)
\end{align*}
$$</span> 这里我选用AI的解释来直观的感受为什么这么做:</p>
<figure>
<img src="https://s2.loli.net/2025/08/17/u6G9YxHyca78VNA.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们在前面也推导过这个原因 <span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial y} &amp;= \begin{cases}
-\frac{1}{p_i} * p_i(1-p_i) = p_i-1 &amp; \text{if c=i} \\
-\frac{1}{p_c} * (-p_cp_i) = p_i &amp; \text{if c!=i}
\end{cases}
\end{align*}
$$</span> 最后我们编写训练循环，来对我们的内容进行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    train_loss, train_acc = processData(train_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--- Epoch %d&#x27;</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Train:\tLoss %.3f | Accuracy: %.3f&#x27;</span> % (train_loss, train_acc))</span><br><span class="line"></span><br><span class="line">        test_loss, test_acc = processData(test_data, backprop=<span class="literal">False</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Test:\tLoss %.3f | Accuracy: %.3f&#x27;</span> % (test_loss, test_acc))</span><br></pre></td></tr></table></figure>
<p>执行可以看到完整的训练过程。</p>
<h2 id="使用">使用</h2>
<p>既然完成了训练，那么我们可以尝试与其进行沟通，我们可以写一个接口用于和它进行对话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">probs, mid=<span class="number">0.5</span></span>):</span><br><span class="line">    positive_prob = probs[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Yes,you are positive ^_^&quot;</span> <span class="keyword">if</span> positive_prob &gt; mid <span class="keyword">else</span> <span class="string">&quot;No,you are negative qwq&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;please wait some time to train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    processData(train_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    text = <span class="built_in">input</span>(<span class="string">&quot;please input a sentence: &quot;</span>).lower()</span><br><span class="line">    inputs = createInputs(text)</span><br><span class="line">    out, _ = rnn.forward(inputs)</span><br><span class="line">    probs = softmax(out)</span><br><span class="line">    <span class="built_in">print</span>(predict(probs))</span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2025/08/17/Zt65XDQbBa9yOPT.png" /></p>
<p>哈哈效果还可以，只不过只能检测到训练集中用过的单词。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/14/74-%E8%83%8C%E6%99%AF%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%8F%8C%E6%80%81%E5%9B%BE%E5%83%8F%E6%98%BE%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/14/74-%E8%83%8C%E6%99%AF%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%8F%8C%E6%80%81%E5%9B%BE%E5%83%8F%E6%98%BE%E7%8E%B0/" class="post-title-link" itemprop="url">74:背景驱动的双态图像显现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-14 09:46:14" itemprop="dateCreated datePublished" datetime="2025-08-14T09:46:14+08:00">2025-08-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-29 13:47:19" itemprop="dateModified" datetime="2025-11-29T13:47:19+08:00">2025-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E9%A1%B9/" itemprop="url" rel="index"><span itemprop="name">杂项</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在网上冲浪的时候我们经常会看到一些点开与不点开会呈现不同结果的图片，我们称这种图片为”幻影坦克图”。因为它利用到了光学欺骗的原理，与红警中的幻影坦克相似，故得其名。今天，尝试了解下背后的原理，并自己动手实现一下这个功能。</p>
<h2 id="原理">原理</h2>
<p>由于幻影坦克图需要涉及到<code>alpha混合</code>，所以我们使用的图像格式必须带有<code>alpha</code>通道。所以我们的首选就是<code>png</code>图像格式，我们将利用它的透明度来实现。最为核心的部分就是<code>alpha</code>通道作用于图片的计算公式：
<span class="math display">$$
\begin{align*}
Color_{合} = Color_{前}*Alpha + Color_{后}*(1-Alpha)
\end{align*}
$$</span> 其中<span
class="math inline"><em>C</em><em>o</em><em>l</em><em>o</em><em>r</em><sub><em>合</em></sub></span>是混合后所被看到的颜色，<span
class="math inline"><em>C</em><em>o</em><em>l</em><em>o</em><em>r</em><sub><em>前</em></sub></span>是前景色，<span
class="math inline"><em>C</em><em>o</em><em>l</em><em>o</em><em>r</em><sub><em>后</em></sub></span>是背景色</p>
<p>我们可以通过改变背景色从而得到图像在背景上呈现的颜色，为了更好的背景效果，我们选择黑底与白底，分别是：<code>(r_w,g_w,b_w,a_w) = (1,1,1,1)</code>和<code>(r_b,g_b,b_b,a_b) = (0,0,0,1)</code>。现在我们可以计算出指定的带有透明度的图片分别在白底<code>(r1,g1,b1)</code>和黑底<code>(r2,g2,b2)</code>上呈现的颜色:
<span class="math display">$$
\begin{cases}
r_1 = r*\alpha + (1-\alpha) \\
g_1 = g*\alpha + (1-\alpha) \\
b_1 = b*\alpha + (1-\alpha) \\
\\
r_2 = r*\alpha \\
g_2 = g*\alpha \\
b_2 = b*\alpha
\end{cases}
$$</span>
但是实际上我们想要把两张图片做成幻影坦克图，我们的<code>(r1,g1,b1,1)</code>和<code>(r2,g2,b2,1)</code>都应该是已知的，实际上我们要求解的应该是幻影坦克图<code>(r,g,b,a)</code>，所以我们调整方程组可得：
<span class="math display">$$
\begin{cases}
\alpha = 1-r_1+r_2 \\
\alpha = 1-g_1+g_2 \\
\alpha = 1-b_1+b_2 \\
\\
r = \frac{r_2}{\alpha} = \frac{r_2}{1-r_1+r_2} \\
g = \frac{g_2}{1-g_1+g_2} \\
b = \frac{b_2}{1-b_1+b_2}
\end{cases}
$$</span>
现在我们发现透明度有三种不同的计算方式，此时我们该如何处理呢？在正常情况下，我们难以满足这三个式子相等。但是在灰度图下可以做到<code>r = g = b</code>，此时灰度图的亮度信息由像素亮度决定。同样的我们也需要注意到<code>0 &lt;= a &lt;= 1</code>的大小关系，因此我们必须有<code>r1 &gt;= r2</code>。为了避免像素值也超出值域，我们在实现时，需要将<code>r1</code>压缩到[128,255]，将<code>r2</code>压缩到[0,127]</p>
<p>现在我们可以计算出我们想要的幻影坦克图片的每一个像素信息了。</p>
<h2 id="代码实现">代码实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mix</span>(<span class="params">img1, img2, output</span>):</span><br><span class="line">    <span class="comment"># 灰度图转化RGB图</span></span><br><span class="line">    img1 = Image.<span class="built_in">open</span>(img1).convert(<span class="string">&quot;L&quot;</span>).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">    img2 = Image.<span class="built_in">open</span>(img2).convert(<span class="string">&quot;L&quot;</span>).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">    <span class="comment"># 调整图像大小</span></span><br><span class="line">    img2 = img2.resize(img1.size)</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    imgarr1 = np.array(img1,dtype=np.float32) / <span class="number">255.0</span></span><br><span class="line">    imgarr2 = np.array(img2,dtype=np.float32) / <span class="number">255.0</span></span><br><span class="line">    <span class="comment"># 压制像素 r1&gt;= r2</span></span><br><span class="line">    imgarr1 = <span class="number">0.5</span> + <span class="number">0.5</span> * imgarr1</span><br><span class="line">    imgarr2 = <span class="number">0.5</span> * imgarr2</span><br><span class="line">    <span class="comment"># 透明度计算</span></span><br><span class="line">    alpha = <span class="number">1</span> - imgarr1 + imgarr2</span><br><span class="line">    alpha = np.clip(alpha,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算幻影坦克图颜色值</span></span><br><span class="line">    rgb = imgarr2 / (alpha + <span class="number">1e-6</span>)</span><br><span class="line">    rgb = np.clip(rgb,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 合并RGB与Alpha</span></span><br><span class="line">    rgba = np.dstack((rgb,alpha.mean(axis=<span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 输出图片</span></span><br><span class="line">    img = Image.fromarray((rgba * <span class="number">255</span>).astype(np.uint8),<span class="string">&quot;RGBA&quot;</span>)</span><br><span class="line">    img.save(output)</span><br><span class="line"></span><br><span class="line">mix(<span class="string">&quot;D:/Photo/111.png&quot;</span>,<span class="string">&quot;D:/Photo/123.png&quot;</span>,<span class="string">&quot;D:/Micro/test.png&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们可以看到以下效果：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/DBt4flajgAI7EJp.jpg"
alt="表图" />
<figcaption aria-hidden="true">表图</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/14/fPk1qbGjeRNF6Ah.jpg"
alt="里图" />
<figcaption aria-hidden="true">里图</figcaption>
</figure>
<h2 id="优化">优化</h2>
<p>我们通过将图片转换成灰度图的形式从而实现了幻影坦克图，但是我们却丢失了RGB通道的颜色。有没有什么方式能够尽可能的保留原来的色彩信息呢？</p>
<p>我们回到这个公式： <span class="math display">$$
\begin{cases}
\alpha = 1-r_1+r_2 \\
\alpha = 1-g_1+g_2 \\
\alpha = 1-b_1+b_2 \\
\\
r = \frac{r_2}{\alpha} = \frac{r_2}{1-r_1+r_2} \\
g = \frac{g_2}{1-g_1+g_2} \\
b = \frac{b_2}{1-b_1+b_2}
\end{cases}
$$</span>
由于在<code>RGBA</code>模式下，一个像素只能有一个<code>alpha</code>通道，也就是说三个色彩通道只能使用一个<code>alpha</code>。所以我们怎么才能让$1-r_1+r_2-g_1+g_2-b_1+b_2rg
b $呢？</p>
<p>我们可以通过两种方式来实现：</p>
<ul>
<li><p><strong>调整图片整体亮度</strong></p>
<p>我们知道，图像的亮度调整通常是通过线性放缩实现的： <span
class="math display">$$
\begin{align*}
RGB_{new} = RGB_{original} * k\space(0&lt;k&lt;1)
\end{align*}
$$</span>
所以RGB值之间的差值也是随着线性放缩变化的。当我们的亮度趋近于0时，<span
class="math inline"><em>r</em> ≈ <em>g</em> ≈ <em>b</em></span>是成立的，我们可以通过减少亮度，从而提升幻影坦克图的视觉效果。</p></li>
<li><p><strong>设置插值平衡色彩与灰度的混合比例</strong></p>
<p>我们可以通过设置差值，来平衡色彩和灰度的混合比例，使得透明度的计算结果在<code>RGB</code>三个通道上趋于一致，，我们基于这个公式可以求出合适的插值：
<span class="math display">$$
\begin{align*}
r_{new} &amp;= r*lerp + gray*(1-lerp) \\
gray &amp;= 0.299*r + 0.587*g + 0.114*b
\end{align*}
$$</span>
通过这种方式我们可以强制对齐<code>alpha</code>通道，并减少通道差异。</p></li>
</ul>
<p>所以接下来，我们需要考虑合适的合适的亮度与色彩插值，以实现更好的效果。</p>
<p>对于亮度，我们有</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/rwftb384S76H2uI.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>由于人的眼睛对光的线性变化的感受是非线性的，所以即使我们把黑底图的亮度系数调暗到0.22，视觉上也只会认为颜色变暗了0.5。所以<code>[0.18,0.22]</code>是我们选取亮度最合适的区间。</p>
<p>对于色彩插值参数，参数越大，灰度平衡的比例就越小，图片能更好的保留色彩。我们希望白底图能够保留较多的色彩，且黑底图能够更加隐蔽，所以我们设置：</p>
<ul>
<li>表图 <code>lerp = [0.6,0.8]</code></li>
<li>里图 <code>lerp = [0.2,0.4]</code></li>
</ul>
<p>综上所述我们可以实现我们最终的效果了</p>
<h2 id="最终实现">最终实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image,ImageEnhance</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Phantom</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.default = &#123;</span><br><span class="line">            <span class="string">&#x27;brightnessW&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">&#x27;brightnessB&#x27;</span>: <span class="number">0.8</span>,</span><br><span class="line">            <span class="string">&#x27;lerpW&#x27;</span>:<span class="number">0.8</span>,</span><br><span class="line">            <span class="string">&#x27;lerpB&#x27;</span>:<span class="number">0.8</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loadImage</span>(<span class="params">self,img1,img2</span>):	<span class="comment"># 加载图片</span></span><br><span class="line">        img1 = Image.<span class="built_in">open</span>(img1).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        img2 = Image.<span class="built_in">open</span>(img2).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">        img2 = img2.resize(img1.size)</span><br><span class="line">        <span class="keyword">return</span> img1,img2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">adjustBrightnedd</span>(<span class="params">self,img,brightness</span>):	<span class="comment"># 亮度调节</span></span><br><span class="line">        <span class="keyword">if</span> brightness == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> img</span><br><span class="line">        <span class="keyword">return</span> ImageEnhance.Brightness(img).enhance(brightness)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">blendColor</span>(<span class="params">self,arr,lerp</span>):	<span class="comment"># 色彩插值平衡</span></span><br><span class="line">        <span class="keyword">if</span> lerp == <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">return</span> arr</span><br><span class="line">        gray = np.dot(arr,[<span class="number">0.299</span>,<span class="number">0.587</span>,<span class="number">0.114</span>])</span><br><span class="line">        <span class="keyword">return</span> arr * lerp + gray[...,<span class="literal">None</span>] * (<span class="number">1</span>-lerp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self,img1,img2,outpath,**kwargs</span>):	</span><br><span class="line">        params = &#123;**<span class="variable language_">self</span>.default,**kwargs&#125;</span><br><span class="line">        <span class="comment"># 加载图片</span></span><br><span class="line">        img1, img2 = <span class="variable language_">self</span>.loadImage(img1,img2)</span><br><span class="line">        <span class="comment"># 亮度调整</span></span><br><span class="line">        img1 = <span class="variable language_">self</span>.adjustBrightnedd(img1,params[<span class="string">&#x27;brightnessW&#x27;</span>])</span><br><span class="line">        img2 = <span class="variable language_">self</span>.adjustBrightnedd(img2,params[<span class="string">&#x27;brightnessB&#x27;</span>])</span><br><span class="line">        <span class="comment"># 归一化</span></span><br><span class="line">        arr1 = np.array(img1, dtype=np.float32) / <span class="number">255.0</span></span><br><span class="line">        arr2 = np.array(img2, dtype=np.float32) / <span class="number">255.0</span></span><br><span class="line">        <span class="comment"># 压制像素 值域区分</span></span><br><span class="line">        arr1 = <span class="number">0.5</span> + <span class="number">0.5</span> * arr1</span><br><span class="line">        arr2 = <span class="number">0.5</span> * arr2</span><br><span class="line">        <span class="comment"># 色彩差值平衡</span></span><br><span class="line">        arr1 = <span class="variable language_">self</span>.blendColor(arr1,params[<span class="string">&#x27;lerpW&#x27;</span>])</span><br><span class="line">        arr2 = <span class="variable language_">self</span>.blendColor(arr2,params[<span class="string">&#x27;lerpB&#x27;</span>])</span><br><span class="line">        <span class="comment"># 透明度计算</span></span><br><span class="line">        alpha = np.clip(<span class="number">1</span>-arr1+arr2,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 幻影坦克图像素计算</span></span><br><span class="line">        rgb = np.clip(arr2/(alpha + <span class="number">1e-6</span>),<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        rgba = np.dstack((rgb, alpha.mean(axis=<span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># 转化图片</span></span><br><span class="line">        img = Image.fromarray((rgba * <span class="number">255</span>).astype(np.uint8), <span class="string">&quot;RGBA&quot;</span>)</span><br><span class="line">        img.save(outpath)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    ph = Phantom()</span><br><span class="line">    ph.generate(<span class="string">&quot;D:/Photo/q.jpg&quot;</span>,<span class="string">&quot;D:/Photo/w.jpg&quot;</span>,<span class="string">&quot;D:/Micro/test.png&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>效果挺好滴，展示一下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/gexcpnUyqYSomBW.jpg"
alt="表图" />
<figcaption aria-hidden="true">表图</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/14/5DZ3NUtTFJbuqRd.jpg"
alt="里图" />
<figcaption aria-hidden="true">里图</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/13/73-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/13/73-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81-3/" class="post-title-link" itemprop="url">73:异常控制流(3)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-08-13 13:08:39 / 修改时间：17:28:38" itemprop="dateCreated datePublished" datetime="2025-08-13T13:08:39+08:00">2025-08-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">计算机科学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81/" itemprop="url" rel="index"><span itemprop="name">异常控制流</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="信号">信号</h1>
<p>我们已经认识到了操作系统怎么通过异常使得进程上下文切换，以实现异常控制流的实现。现在我们将尝试另一种实现——<strong>Linux信号</strong>，来允许进程和内核中断其他的进程。</p>
<p>我们可以将信号理解成一条消息，它通知进程系统中发生了某一个事件。每种信号类型都会对应于某种系统事件。然后由不同的处理程序去处理这个事件。我们可以通过<code>man 7 signal</code>来进一步的认识这些信号：</p>
<figure>
<img src="https://s2.loli.net/2025/08/13/iSXhvnLw5c41CY9.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="信号术语">信号术语</h2>
<p>传送一个信号到目的进程可以分作两个步骤：</p>
<ul>
<li><strong>发送信号：</strong>内核通过更新目的进程的上下文中的某个状态，从而实现发送一个信号给目的进程。发送信号一般有以下两种原因：1)内核检测到了一个系统事件，2)一个进程调用了kill函数，显式的要求内核发送一个信号给目的进程</li>
<li><strong>接收信号：</strong>当目的进程被内核强迫对信号的发送做出反应时，我们就说它接受了信号。目的进程可以忽略这个信号，终止，或者通过执行信号处理程序的用户层来捕获这个信号。</li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/08/13/tQ2OVL8gGhT9mKq.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>一个发出但没有被接收的信号我们称之为<strong>待处理信号</strong>。在任何时刻一个类型只会有一个待处理信号。如果一个进程中有一个类型为k的待处理信号。接下来任何发送到这个进程的类型为k的信号都不会再排队等候，而是被丢弃。同时一个进程可以选择阻塞接受某种信号。如果一个信号被阻塞，它可以发送，但是不会再被接收，直到取消对它的阻塞。</p>
<p>这个实现通过内核为每个进程维护这一个信号处理集合实现。在<code>pending</code>向量中维护着一个待处理信号的集合，在<code>blocked</code>向量中维护着一个阻塞信号的集合。当一个信号类型为<code>k</code>的信号被发送时，目的进程会检查其<code>pending</code>位是否已被设置，若是则丢弃；不是则设置。然后检查其<code>block</code>位是否被阻塞，若是则丢弃；若不是则接受信号，并清除<code>pending</code>位。</p>
<h2 id="发送信号">发送信号</h2>
<p>发送信号的机制，是基于进程组实现的。我们接下来进一步的理解信号的发送：</p>
<h3 id="进程组">进程组</h3>
<p>每个进程都只属于一个进程组。进程组是由一个正整数进程组ID来标识的。我们有以下函数可以认识并改变进程组：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">getpgrp</span><span class="params">(<span class="type">void</span>)</span>;	<span class="comment">//返回调用进程的进程组ID</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">setpgid</span><span class="params">(<span class="type">pid_t</span> pid,<span class="type">pid_t</span> pgid)</span>;	<span class="comment">//成功则返回0，失败则返回-1</span></span><br></pre></td></tr></table></figure>
<p>默认情况下，子进程和它的父进程是同属于一个进程组的。一个进程可以通过使用<code>setpgid</code>函数来改变自己或者其他进程的进程组。<code>setpgid</code>函数将<code>pid(目标进程PID)</code>进程加入到进程组<code>pgid(目标进程组ID)</code>。如果<code>pid</code>是0，就表示当前进程。如果<code>pgid</code>是0，就用<code>pid</code>的值作为新的<code>pgid</code>。</p>
<h3 id="kill程序发送信号">kill程序发送信号</h3>
<p><code>/bin/kill</code>程序可以向其他程序发送任意的信号：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -&lt;signalNumber&gt; &lt;pid&gt;</span><br></pre></td></tr></table></figure>
<p>也可以向指定的进程组中的所有进程发送信号：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -&lt;signalNumber&gt; -&lt;pgid&gt;</span><br></pre></td></tr></table></figure>
<p>我们可以尝试一下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/13/E8UBMIxsinXbcgG.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>此外我们也可以通过键盘发送信号</p>
<h3 id="从键盘发送信号">从键盘发送信号</h3>
<p>Uinx shell
中通常使用<code>job</code>（作业）来表示对一条命令行求值而创建的进程。在任何时刻，只有一个前台作业和0或多个后台作业。其中每个作业的都属于一个独立的进程组。</p>
<p>在键盘上<code>Ctrl+C</code>会导致内核发送一个<code>SIGINT</code>信号到前台进程组中的每个进程，导致作业终止。在键盘上<code>Ctrl+Z</code>会导致内核发送一个<code>SIGTSTP</code>信号到前台进程组中的每个进程，导致作业被停止（挂起）。</p>
<h3 id="用kill函数发送信号">用kill函数发送信号</h3>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">kill</span><span class="params">(<span class="type">pid_t</span> pid,<span class="type">int</span> sig)</span>	<span class="comment">//成功则返回0，失败则返回-1</span></span><br></pre></td></tr></table></figure>
<p>通过调用kill函数发送信号到其他进程。</p>
<ul>
<li>如果<code>pid&gt;0</code>，那么发送信号<code>sig</code>给进程<code>pid</code>。</li>
<li><code>pid=0</code>，那么发送信号<code>sig</code>给调用进程所在进程组中的每个进程，包括自己。</li>
<li><code>pid&lt;0</code>，则发送信号<code>sig</code>给进程组<code>|pid|</code>中的每个进程</li>
</ul>
<p>我们可以尝试编写一个程序来使用kill函数：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;csapp.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">pid_t</span> pid;</span><br><span class="line">    <span class="keyword">if</span>((pid = Fork())==<span class="number">0</span>)&#123;</span><br><span class="line">        Pause();		<span class="comment">//将进程挂起，直到接收到一个信号</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;never get it\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    kill(pid,SIGKILL);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样就实现了父进程击杀自己的子进程。</p>
<h3 id="用alarm函数发送信号">用alarm函数发送信号</h3>
<p>进程可以通过调用<code>alarm</code>函数向自己发送<code>SIGALRM</code>信号</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> <span class="title function_">alarm</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span> secs)</span>;	<span class="comment">//返回前一次闹钟剩余的秒数，若之前没有设置闹钟，返回0</span></span><br></pre></td></tr></table></figure>
<p><code>alarm</code>函数安排内核在<code>secs</code>秒之后发送一个<code>SIGALRM</code>信号给调用进程。如果<code>secs==0</code>，那么不会安排调度新的闹钟。在任何情况下，<code>alarm</code>的调用都会取消之前的待处理的闹钟，并返回前一个闹钟的剩余的秒数。</p>
<h2 id="接收信号">接收信号</h2>
<p>当内核把进程<code>p</code>从内核态切换到用户态时，它会检查进程<code>p</code>的未被阻塞的待处理信号的集合<code>pending &amp; ~blocked</code>。如果这个集合为空，那么内核将控制传递到<code>p</code>的逻辑控制流中的下一条指令。如果集合使非空的，那么内核选择集合中的某个信号<code>k</code>(通常是最小的<code>k</code>)，并强制<code>p</code>接收信号<code>k</code>。收到这个信号会触发某种行为。一旦进程完成这个行为，就会将控制传递回<code>p</code>的逻辑控制流中的下一条指令。</p>
<p>在上面展示信号类型的图中，也有每个信号类型相关联的默认行为。我们也可以通过<code>signal</code>函数修改和信号相关联的默认行为。唯一例外的是<code>SIGSTOP</code>和<code>SIGKILL</code>。它们的默认行为是不能修改的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="title function_">void</span> <span class="params">(*<span class="type">sighandler_t</span>)</span><span class="params">(<span class="type">int</span>)</span>;</span><br><span class="line"><span class="type">sighandler_t</span> <span class="title function_">signal</span><span class="params">(<span class="type">int</span> signum,<span class="type">sighandler_t</span> handler)</span>;</span><br><span class="line"><span class="comment">//如果成功则返回指向前次处理程序的指针；否则返回SIG_ERR，不设置errno</span></span><br></pre></td></tr></table></figure>
<p><code>signal</code>函数通过以下三种方式之一来改变和信号<code>signum</code>相关联的行为：</p>
<ul>
<li>如果<code>handler</code>是SIG_IGN，那么忽略这个类型的信号</li>
<li>如果<code>handler</code>是SIG_DFL，那么恢复这个类型的信号的默认行为</li>
<li>否则，<code>handler</code>就是用户定义的函数的地址，这个函数被称为<strong>信号处理程序</strong>。当接收到指定的信号类型时就会调用信号处理程序，我们称之为<strong>捕获信号</strong>。一个处理程序可以捕获不同的信号。</li>
</ul>
<p>比如我们可以写一个程序用来改变<code>SIGINT</code>信号的默认行为（终止进程）：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;csapp.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">handler</span><span class="params">(<span class="type">int</span> sig)</span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\nOVER!\n&quot;</span>);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    Signal(SIGINT,handler);</span><br><span class="line">    Pause();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然信号处理程序也可以被其他信号处理程序中断。例如在下图中演示了这个过程：</p>
<figure>
<img src="https://s2.loli.net/2025/08/13/5JV6lqbaiWARLEO.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="阻塞和解除阻塞信号">阻塞和解除阻塞信号</h2>
<p>Linux提供了两种阻塞信号的机制：</p>
<ul>
<li><strong>隐式阻塞机制</strong>
内核默认阻塞任何当前处理程序正在处理的信号类型的待处理信号。</li>
<li><strong>显式阻塞机制</strong>
使用<code>sigprocmask</code>函数和它的辅助函数，明确阻塞/解除指定的信号。</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">sigprocmask</span><span class="params">(<span class="type">int</span> how, <span class="type">const</span> <span class="type">sigset_t</span>* <span class="built_in">set</span>, <span class="type">sigset_t</span>* oldset)</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">sigemptyset</span><span class="params">(<span class="type">sigset_t</span>* <span class="built_in">set</span>)</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">sigfillset</span><span class="params">(<span class="type">sigset_t</span>* <span class="built_in">set</span>)</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">sigaddset</span><span class="params">(<span class="type">sigset_t</span>* <span class="built_in">set</span>, <span class="type">int</span> signum)</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">sigdelset</span><span class="params">(<span class="type">sigset_t</span>* <span class="built_in">set</span>, <span class="type">int</span> signum)</span>;	<span class="comment">//成功则返回0，否则返回-1</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">sigismember</span><span class="params">(<span class="type">const</span> <span class="type">sigset_t</span>* <span class="built_in">set</span>, <span class="type">int</span> signum)</span>;	<span class="comment">//若是则返回1，不是则返回0，出错返回-1</span></span><br></pre></td></tr></table></figure>
<p><code>sigprocmask</code>函数改变当前阻塞的信号集合（blocked位向量）。其具体的行为依赖于<code>how</code>值：</p>
<ul>
<li><strong>SIG_BLOCK</strong> 把set中的信号添加到blocked中
<code>blocked = set|blocked</code></li>
<li><strong>SIG_UNBLOCK</strong> 从blocked中删除set中的信号
<code>blocked = blocked &amp; ~set</code></li>
<li><strong>SIG_SETMASK</strong> 令<code>blocked = set</code></li>
</ul>
<p>如果<code>oldset</code>非空，则将之前的<code>blocked</code>保存在其中。对于其他的几个辅助函数：</p>
<ul>
<li><code>sigemptyset</code>初始化set为空集合</li>
<li><code>sigfillset</code>将每个信号都添加到set中</li>
<li><code>sigaddset</code>将signum加入到set中</li>
<li><code>sigdelset</code>从set中删除signum，如果signum是set的成员，返回1。不是则返回0</li>
</ul>
<h2 id="编写信号处理程序">编写信号处理程序</h2>
<p>这一部分太难了，我难以理解并接受。之后再来看看吧</p>
<h2 id="显式地等待信号">显式地等待信号</h2>
<p>和上面的关联度较高，涉及到竞争并发等内容，我暂时无法理解</p>
<h1 id="非本地跳转">非本地跳转</h1>
<p>C语言提供了一种用户级的异常控制流形式，即非本地跳转（本地跳转是<code>goto</code>），它将控制从一个函数转移到另一个当前正在执行的函数，而不需要经过正常的调用-返回序列。其中非本地跳转是通过<code>setjmp</code>和<code>longjmp</code>实现的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;setjump.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">setjump</span><span class="params">(jmp_buf env)</span>;	<span class="comment">//setjmp返回0，longjmp返回非0</span></span><br></pre></td></tr></table></figure>
<p><code>setjmp</code>函数会在env缓冲区中保存当前的调用环境（相当于设置一个锚点，保存当前状态），以供后面的<code>longjmp</code>使用，并返回0。注意<code>setjump</code>由于其特殊的返回机制，不能被存储在变量之中，但是可以被switch使用。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;setjump.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">longjmp</span><span class="params">(jmp_buf env, <span class="type">int</span> retval)</span>;	<span class="comment">//不返回</span></span><br></pre></td></tr></table></figure>
<p><code>longjmp</code>函数从env缓冲区中恢复调用环境，然后触发一个从最近一次初始化env的<code>setjmp</code>调用的返回。然后<code>setjmp</code>返回，并带有非零的返回值<code>retval</code></p>
<p>注意到，setjmp只被执行一次，但是会返回多次：一次是第一次调用<code>setjmp</code>时，调用环境保存在缓冲区<code>env</code>中。一次时为每个相应的<code>longjmp</code>调用。另一方面，<code>longjmp</code>被调用一次，但是不返回。</p>
<p>通过非本地条状我们可以实现从一个深层嵌套的函数调用中立即返回，从而实现对错误的分析，而不用多次退出复杂的调用栈。我们以下面的程序为例，可以感受到非本地跳转的用途：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;setjmp.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;csapp.h&quot;</span></span></span><br><span class="line">jmp_buf buf;</span><br><span class="line"><span class="type">int</span> error1 = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> error2 = <span class="number">1</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">foo</span><span class="params">()</span>,<span class="title function_">bar</span><span class="params">()</span>;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">switch</span>(setjmp(buf))&#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">            foo();</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Detected error1 in foo\n&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Detected error2 in foo\n&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Unknown error in foo\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">foo</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(error1)</span><br><span class="line">        longjmp(buf,<span class="number">1</span>);</span><br><span class="line">    bar();</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">bar</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(error2)</span><br><span class="line">        longjmp(buf,<span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>虽然C中并没有异常的捕获函数，但是我们可以通过这种方式去实现。当遇到一个错误是，从setjmp返回，并解析它的错误类型。</p>
<p>同时，也要注意。<code>longjmp</code>允许跳过中间调用机制的过程可能回导致许多意外的后果。比如没有释放一些数据结构，导致内存泄露….</p>
<h1 id="写在最后">写在最后</h1>
<p>关于异常控制流我感觉还是比较抽象的。涉及到的函数很多，尤其是信号部分，牵连到许多并发相关的内容。对于现在的我而言还是太过超前，日后再来巩固。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/07/72-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/07/72-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81-2/" class="post-title-link" itemprop="url">72:异常控制流(2)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-08-07 11:53:06 / 修改时间：17:14:57" itemprop="dateCreated datePublished" datetime="2025-08-07T11:53:06+08:00">2025-08-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">计算机科学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81/" itemprop="url" rel="index"><span itemprop="name">异常控制流</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="系统调用错误处理">系统调用错误处理</h1>
<p>由于之后会用到大量的系统调用函数，我们需要做好错误处理，以便于查找问题。Uinx系统中的系统级函数遇到错误时会返回-1，并设置全局整数变量<code>errno</code>来表示错误类型。这个时候我们可以通过<code>strerror()</code>函数来返回和errno值相关联的错误。我们以处理<code>fork()</code>的错误为例：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>((pid = fork()) &lt; <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;fork error: %s\n&quot;</span>,strerror(errno));</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以进一步的封装这个错误：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">unix_error</span><span class="params">(<span class="type">char</span> *msg)</span>&#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;%s: %s\n&quot;</span>,msg,strerror(errno));</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>((pid = fork()) &lt; <span class="number">0</span>)&#123;</span><br><span class="line">    unix_error(<span class="string">&quot;fork error&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过错误处理包装函数，我们可以进一步的优化代码。对于错误处理包装函数，有一个约定成俗的规矩，对于基本函数<code>foo</code>，我们定义一个具有相同参数的包装函数<code>Foo</code>。包装函数调用基本函数，检查错误，如果有问题就终止。比如下面对<code>fork</code>的包装：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">pid_t</span> <span class="title function_">Fork</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">pid_t</span> pid;</span><br><span class="line">    <span class="keyword">if</span>((pid = fork()) &lt; <span class="number">0</span>)</span><br><span class="line">        unix_error(<span class="string">&quot;fork error&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> pid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们之后的包装函数也会按照相同的处理模式，来进行编写。</p>
<h1 id="进程调用">进程调用</h1>
<p>Unix提供了大量从C程序中操作进程的系统调用，我们来详细了解他们：</p>
<h2 id="获取进程id">获取进程ID</h2>
<p>每个进程都有一个唯一的正数非零进程<code>PID</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">getpid</span><span class="params">()</span>;		<span class="comment">//返回调用进程的PID</span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">getppid</span><span class="params">()</span>;	<span class="comment">//返回调用进程的父进程的PID</span></span><br></pre></td></tr></table></figure>
<p>这两个函数返回的类型为<code>pid_t</code>，在Linux中它们被<code>types.h</code>定义为<code>int</code></p>
<h2 id="创建和终止进程">创建和终止进程</h2>
<p>我们可以认为进程总是处于下面三种状态：</p>
<ul>
<li><strong>运行</strong>
进程要么在CPU上执行，要么在等待被执行且最终会被调度</li>
<li><strong>停止</strong>
进程的执行被挂起，且不会被调度。当收到<code>SIGSTOP SIGTSTP SIGTTIN SIGTTOU</code>信号时，进程就保持停止，直到它收到一个<code>SIGCONT</code>信号，这个时候，进程在次开始运行</li>
<li><strong>终止</strong>
进程永远地停止了。三种原因:1)收到终止进程信号，2)从主程序返回，3)调用exit()函数</li>
</ul>
<p>下面我们了解进程的创建和终止过程：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="type">void</span> <span class="title function_">exit</span><span class="params">(<span class="type">int</span> status)</span>;	<span class="comment">//exit函数以status退出状态来终止进程</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">fork</span><span class="params">()</span>;	<span class="comment">//子进程返回0,父进程返回子进程的PID,如果出错返回-1</span></span><br></pre></td></tr></table></figure>
<p>新创建的子进程会得到父进程用户级虚拟地址空间相同的一份副本，包括代码、数据、用户栈、堆、共享库。子进程也会得到与父进程任何打开文件描述符相同的副本，这意味着当父进程调用<code>fork</code>时，子进程可以读取父进程中打开的所有文件。父子进程最大的区别就在于他们的PID不同</p>
<p>fork函数被调用一次，却会返回两次，这是因为调用之后创建了一个新的进程。然而，在两个进程中的返回值会有所不同，因此我们根据<code>fork()</code>的返回值来判断父子进程</p>
<p>我们可以用下面这个程序来展示一个进程的创建：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">	<span class="type">pid_t</span> pid;</span><br><span class="line">	<span class="type">int</span> x = <span class="number">1</span>;</span><br><span class="line">	pid = Fork();</span><br><span class="line">	<span class="keyword">if</span>(pid == <span class="number">0</span>)&#123;	<span class="comment">//子进程</span></span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;child: x = %d\n&quot;</span>,x+<span class="number">1</span>);</span><br><span class="line">		<span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;parent: x = %d\n&quot;</span>,x<span class="number">-1</span>);	<span class="comment">//父进程</span></span><br><span class="line">	<span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到执行的结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ylin@Ylin:~/Program/test$ ./a.out</span><br><span class="line">parent: x = 0</span><br><span class="line">child: x = 2</span><br></pre></td></tr></table></figure>
<p>实际的运行过程我们可以简化成流程图：</p>
<figure>
<img src="https://s2.loli.net/2025/08/07/r8OAl2RgnLdvPNw.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>对于整个过程我们可以从中注意到一些关键点：</p>
<ul>
<li><strong>调用一次，返回两次</strong>
fork函数被父进程调用一次，但是却返回两次——一次返回到父进程，一次返回到子进程。对于多个fork函数的情况我们之后会涉及</li>
<li><strong>并发执行</strong>
父子进程都是并发运行的独立进程。内核可能以任意方式交替执行它们的逻辑控制流的指令。因此我们不能对不同进程中指令的交替执行做出假设。不存在执行的先后关系</li>
<li><strong>相同但是独立的空间</strong>
通过观察局部变量x，我们可以看出，父子进程对x所作的改变都是独立的。说明它们之间的空间是独立的。根据数值可以判断，x的值是相同的。因此我们说父子进程的空间相同且独立。</li>
<li><strong>共享文件</strong>
printf将输出输入到stdout文件中，结果表明在子进程中的stdout也是打开的。子进程继承了这个文件，所以说父子进程中的文件描述符也是相同的</li>
</ul>
<p>理解了这些我们就可以理解更复杂的情况，我们可以通过流程图来更好的分析复杂的嵌套情况：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    Fork();</span><br><span class="line">    Fork();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/08/07/ZQseSj79UoVKr8v.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="回收子进程">回收子进程</h2>
<p>当一个进程被终止时，内核不会立即将其清除。而是进程会处于一个终止的状态下，直到它的父进程将其回收。当父进程将终止的子进程回收时，内核将子进程的推出状态传递给了父进程，然后抛弃终止的进程。直到现在，这个进程才不存在了。对于处于终止状态，但没有被回收的进程，我们称之为僵死进程。</p>
<p>如果一个父进程终止了，内核会安排init进程作为它们的孤儿进程的养父。init进程的PID为1，是在系统启动时就由内核创建的，它不会终止，是所有内核的祖先，如果父进程没有回收它的僵死子进程就终止了。内核会安排init进程去回收它们。因为即使僵死子进程没有运行，也会消耗系统的内存资源</p>
<p>进程可以通过调用<code>waitpid</code>函数来等待它的子进程终止或停止。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">waitpid</span><span class="params">(<span class="type">pid_t</span> pid, <span class="type">int</span>* statusp, <span class="type">int</span> options)</span>;</span><br><span class="line"><span class="comment">// 如果成功终止就返回子进程的PID;如果WNOHANG,则为0;其他错误,则为-1</span></span><br></pre></td></tr></table></figure>
<p>waitpid函数比较复杂，我们需要仔细讨论一下。</p>
<p>默认情况下(options=0),waitpid挂起调用进程的执行，直到它的等待集合中的一个子进程终止。如果等待集合中的一个进程在刚调用的时候就已经终止了，那么waitpid就立即返回。在这两种情况中，waitpid会返回导致waitpid返回的已终止进程的PID。此时，已终止的子进程会被回收，内核会清理它的痕迹。</p>
<p>这个过程非常的抽象，我们需要深入去理解waitpid:</p>
<h3 id="判定等待集合的成员">判定等待集合的成员</h3>
<p>等待集合的成员由参数pid确定：</p>
<ul>
<li>如果pid&gt;0，那么等待集合就是一个单独的子进程，它的进程ID等于pid</li>
<li>如果pid=-1，那么等待集合就是由父进程所有的子进程组成的</li>
</ul>
<h3 id="修改默认行为">修改默认行为</h3>
<p>可以通过修改<code>options</code>为各个常量从而实现修改默认行为<strong>：</strong></p>
<ul>
<li><p><strong>WNOHANG</strong></p>
<p>如果等待集合中的任何子进程都没有终止，那么就立即返回。而默认的行为是挂起调用进程，直到有子进程终止。默认行为是等待的，会阻塞之后的操作。如果想要在等待子进程终止的同时，想要进行别的工作，我们就可以启用这个选项</p></li>
<li><p><strong>WUNTRACED</strong></p>
<p>挂起调用进程的执行，直到等待集合中的一个进程变成已终止或者被停止。返回的PID为导致返回的已终止或者被停止的子进程的PID。默认是返回导致返回的已终止的子进程。如果想要检查已终止和被停止的子进程时，可以启用这个选项。</p></li>
<li><p><strong>WCONTINUED</strong></p>
<p>挂起调用进程的执行，直到等待集合中一个正在运行的进程变成已终止或等待集合中一个被停止的进程收到<code>SIGCONT</code>信号重新开始执行</p></li>
</ul>
<p>这些常量可以通过”|“来连接，从而更改行为</p>
<h3 id="检查已回收子进程的退出状态">检查已回收子进程的退出状态</h3>
<p>status是statusp指向的值，如果这个值不为NULL。waitpid就会在status中放上关于导致返回的子进程的状态信息。我们可以通过&lt;wait.h&gt;中定义的宏来解释status参数：</p>
<ul>
<li><strong>WIFEXITED()</strong>
如果子进程通过exit或return正常终止则返回真</li>
<li><strong>WEXITSTATUS()</strong>
返回一个正常终止的子进程的退出状态。只有WIFEXITED()返回为真时，才有这个状态</li>
<li><strong>WIFSIGNALED()</strong>
如果子进程是因为一个未被捕获的信号终止的，那么返回真</li>
<li><strong>WTERMSIG()</strong>
返回导致子进程终止的信号的编号。只有WIFSIGNALED()返回为真时，才有这个状态</li>
<li><strong>WIFSTOPPED()</strong> 如果子进程是停止的，就返回真</li>
<li><strong>WSTOPSIG()</strong>
返回引起子进程停止的信号的编号。只有WITSTOPPED()返回为真时，才有这个状态</li>
<li><strong>WIFCONTINUED()</strong>
如果子进程收到SIGCONT信号重新启动，则返回真</li>
</ul>
<h3 id="错误条件">错误条件</h3>
<p>如果调用进程没有子进程，则返回-1，设置errno=ECHILD</p>
<p>如果waitpid被信号中断，返回-1，设置errno=EINTR</p>
<h3 id="wait函数">wait函数</h3>
<p><code>wait()</code>是<code>waitpid()</code>的简化版</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">wait</span><span class="params">(<span class="type">int</span>* statusp)</span>;</span><br><span class="line"><span class="comment">//如果成功，返回子进程PID;否则返回-1</span></span><br></pre></td></tr></table></figure>
<p><code>wait(&amp;status)</code>等价于<code>waitpid(-1,&amp;status,0)</code></p>
<h2 id="让进程休眠">让进程休眠</h2>
<p>sleep函数可以将一个进程挂起指定的时间</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> <span class="title function_">sleep</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span> secs)</span>;	<span class="comment">//返回还要休眠的秒数</span></span><br></pre></td></tr></table></figure>
<p>如果请求的时间到了，就返回0；否则返回还要休眠的秒数，这种情况是因为sleep可能会被信号中断而过早返回。</p>
<p>另一个函数是puase，该函数让进程休眠，直到收到信号。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">pause</span><span class="params">()</span>;	<span class="comment">//总是返回-1</span></span><br></pre></td></tr></table></figure>
<h2 id="加载并运行程序">加载并运行程序</h2>
<p>execve函数用于在当前进程的上下文中加载并运行一个新的程序。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">execve</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* filename,</span></span><br><span class="line"><span class="params">           <span class="type">const</span> <span class="type">char</span>* argv[],</span></span><br><span class="line"><span class="params">           <span class="type">const</span> <span class="type">char</span>* envp[])</span>;</span><br><span class="line"><span class="comment">//如果成功，则不返回；否则返回-1</span></span><br></pre></td></tr></table></figure>
<p>execve函数加载并运行可执行目标文件filename，且待参数列表argv和环境变量列表envp。只有出现错误时，execve才会返回到调用程序。正常情况下调用一次不返回。</p>
<p>在execve加载了filename之哦胡。它会调用启动代码<code>__libc_start_main</code>。启动代码设置栈，并将控制传递给新程序的主函数：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span>* argv[],<span class="type">char</span>*envp[])</span>;</span><br></pre></td></tr></table></figure>
<p>当main开始执行时，用户栈的组织结构如下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/07/CmR8H3ILYJb6aoD.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们从高地址往下看，先是存放了参数和环境的字符串。然后是以NULL结尾的指针数组，其中每个指针都指向栈中的一个字符串。其中全局变量environ指向这些指针中的第一个envp[0]。在栈的顶部是系统启动函数<code>__libc_start_main</code>的栈帧。</p>
<p>main的三个参数：</p>
<ul>
<li>argc 给出argv[]数组中非空指针的数量</li>
<li>argv 指向argv[]数组中的第一个条目</li>
<li>envp 指向envp[]数组中的第一个条目</li>
</ul>
<p>同时LInux还提供了几个函数用来操作环境数组：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="type">char</span>* <span class="title function_">getenv</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name)</span>;	<span class="comment">//若存在则返回指向name的指针；否则返回NULL</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">setenv</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name,<span class="type">const</span> <span class="type">char</span>* newvalue,<span class="type">int</span> overwrite)</span>; <span class="comment">//成功则返回0；否则返回-1</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">unsetenv</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name)</span>; <span class="comment">//不返回</span></span><br></pre></td></tr></table></figure>
<ul>
<li>getenv函数在环境数组中搜索字符串“name=value”。如果找到了，就返回指向value的指针。</li>
<li>unsetenv函数查找字符串”name=value”，并删除</li>
<li>setenv函数找到环境变量”name=value”后，会用新的value替换；否则则创建一个”name=new_value”的环境变量。overwrite用来控制是否覆盖已存在的同名环境变量，0则不覆盖。</li>
</ul>
<h2 id="使用fork和execve">使用fork和execve</h2>
<p>我们写一个shell。shell打印一个命令行提示符，我们在stdin上输入命令行，然后对这个命令执行。于是我们可以搭建出一个简单的框架:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;shell.h&quot;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="type">char</span> cmdline[MAXLINE];</span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;&gt;&gt;&gt;&quot;</span>);</span><br><span class="line">                Fgets(cmdline,MAXLINE,<span class="built_in">stdin</span>);</span><br><span class="line">                <span class="keyword">if</span>(feof(<span class="built_in">stdin</span>))</span><br><span class="line">                        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">                eval(cmdline);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们首先需要解析命令行参数，我们以空格作为分隔符，同时返回一个参数列表<code>argv</code>。如果命令的参数以<code>&amp;</code>结尾，我们就把这个程序放在后台运行。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">parseline</span><span class="params">(<span class="type">char</span>* buf,<span class="type">char</span>** argv)</span>&#123;</span><br><span class="line">        <span class="type">char</span>* delim;    <span class="comment">// 指向分隔符的指针</span></span><br><span class="line">        <span class="type">int</span> argc;               <span class="comment">// 参数数量</span></span><br><span class="line">        <span class="type">int</span> bg;                 <span class="comment">// 是否为后台程序</span></span><br><span class="line"></span><br><span class="line">        buf[<span class="built_in">strlen</span>(buf)<span class="number">-1</span>]=<span class="string">&#x27; &#x27;</span>; <span class="comment">// \0替换为空格</span></span><br><span class="line">        <span class="keyword">while</span>(*buf &amp;&amp; (*buf==<span class="string">&#x27; &#x27;</span>)) <span class="comment">// 忽略多余的空格</span></span><br><span class="line">                buf++;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//解析参数</span></span><br><span class="line">        argc=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>((delim = <span class="built_in">strchr</span>(buf,<span class="string">&#x27; &#x27;</span>)))&#123;</span><br><span class="line">                argv[argc++] = buf;</span><br><span class="line">                *delim = <span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line">                buf = delim+<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">while</span>(*buf &amp;&amp; (*buf==<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">                        buf++;</span><br><span class="line">        &#125;</span><br><span class="line">        argv[argc] = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(argc==<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//是否应该在后台运行</span></span><br><span class="line">        <span class="keyword">if</span>((bg = (*argv[argc<span class="number">-1</span>] == <span class="string">&#x27;&amp;&#x27;</span>)) != <span class="number">0</span>)</span><br><span class="line">                argv[argc<span class="number">-1</span>] = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> bg;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>解析好命令参数后，我们也需要判断第一个参数是否为程序名，或者是shell的内置函数。如果是内置函数我们就执行该函数，并返回1；如果不是就返回0。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">builtin_command</span><span class="params">(<span class="type">char</span> **argv)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>],<span class="string">&quot;quit&quot;</span>))</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>],<span class="string">&quot;cd&quot;</span>))&#123;</span><br><span class="line">                <span class="keyword">if</span>(argv[<span class="number">1</span>]==<span class="literal">NULL</span>)</span><br><span class="line">                        chdir(<span class="string">&quot;~&quot;</span>);</span><br><span class="line">                chdir(argv[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后我们就可以写出我们的执行函数了，如果<code>builtin_comand</code>返回0，我们就需要创建一个新的子进程并加载程序运行。然后根据是否后台运行的需求，使用waitpid进行对前台程序的等待。当作业结束后，再进行一次迭代。我们的Shell就粗略的完成了。但是现在有一个问题，我们的shell不能回收已经结束的子进程，我们会在之后加以改进。程序的完整代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// csapp.h	</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;errno.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/wait.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">unix_error</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *msg)</span>&#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>,<span class="string">&quot;%s: %s\n&quot;</span>,msg,strerror(errno));</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> <span class="title function_">Fork</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">pid_t</span> pid;</span><br><span class="line">    <span class="keyword">if</span>((pid = fork()) &lt; <span class="number">0</span>)</span><br><span class="line">        unix_error(<span class="string">&quot;fork error&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> pid;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">char</span>* <span class="title function_">Fgets</span><span class="params">(<span class="type">char</span>* str,<span class="type">int</span> n,FILE* stream)</span>&#123;</span><br><span class="line">        <span class="type">char</span>* p = fgets(str,n,stream);</span><br><span class="line">        <span class="keyword">if</span>(p == <span class="literal">NULL</span>)</span><br><span class="line">                unix_error(<span class="string">&quot;fgets error&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// shell.h</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;csapp.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXARGS 32</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXLINE 256</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//执行命令行任务</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">eval</span><span class="params">(<span class="type">char</span>* cmdline)</span>;</span><br><span class="line"><span class="comment">//解析参数</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">parseline</span><span class="params">(<span class="type">char</span>* buf, <span class="type">char</span>** argv)</span>;</span><br><span class="line"><span class="comment">//判断Shell内联函数</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">builtin_command</span><span class="params">(<span class="type">char</span>** argv)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">parseline</span><span class="params">(<span class="type">char</span>* buf,<span class="type">char</span>** argv)</span>&#123;</span><br><span class="line">        <span class="type">char</span>* delim;    <span class="comment">// 指向分隔符的指针</span></span><br><span class="line">        <span class="type">int</span> argc;               <span class="comment">// 参数数量</span></span><br><span class="line">        <span class="type">int</span> bg;                 <span class="comment">// 是否为后台程序</span></span><br><span class="line"></span><br><span class="line">        buf[<span class="built_in">strlen</span>(buf)<span class="number">-1</span>]=<span class="string">&#x27; &#x27;</span>; <span class="comment">// \0替换为空格</span></span><br><span class="line">        <span class="keyword">while</span>(*buf &amp;&amp; (*buf==<span class="string">&#x27; &#x27;</span>)) <span class="comment">// 忽略多余的空格</span></span><br><span class="line">                buf++;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//解析参数</span></span><br><span class="line">        argc=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>((delim = <span class="built_in">strchr</span>(buf,<span class="string">&#x27; &#x27;</span>)))&#123;</span><br><span class="line">                argv[argc++] = buf;</span><br><span class="line">                *delim = <span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line">                buf = delim+<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">while</span>(*buf &amp;&amp; (*buf==<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">                        buf++;</span><br><span class="line">        &#125;</span><br><span class="line">        argv[argc] = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(argc==<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//是否应该在后台运行</span></span><br><span class="line">        <span class="keyword">if</span>((bg = (*argv[argc<span class="number">-1</span>] == <span class="string">&#x27;&amp;&#x27;</span>)) != <span class="number">0</span>)</span><br><span class="line">                argv[argc<span class="number">-1</span>] = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> bg;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">builtin_command</span><span class="params">(<span class="type">char</span> **argv)</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>],<span class="string">&quot;quit&quot;</span>))</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span>(!<span class="built_in">strcmp</span>(argv[<span class="number">0</span>],<span class="string">&quot;cd&quot;</span>))&#123;</span><br><span class="line">                <span class="keyword">if</span>(argv[<span class="number">1</span>]==<span class="literal">NULL</span>)</span><br><span class="line">                        chdir(<span class="string">&quot;~&quot;</span>);</span><br><span class="line">                chdir(argv[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">eval</span><span class="params">(<span class="type">char</span>* cmdline)</span>&#123;</span><br><span class="line">        <span class="type">char</span>* argv[MAXARGS];    <span class="comment">//参数列表</span></span><br><span class="line">        <span class="type">char</span> buf[MAXLINE];      <span class="comment">//命令存储区</span></span><br><span class="line">        <span class="type">int</span> bg;                 <span class="comment">//是否后台调用</span></span><br><span class="line">        <span class="type">pid_t</span> pid;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">strcpy</span>(buf,cmdline);</span><br><span class="line">        bg = parseline(buf,argv);</span><br><span class="line">        <span class="keyword">if</span>(argv[<span class="number">0</span>]==<span class="literal">NULL</span>)</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(!builtin_command(argv))&#123;</span><br><span class="line">                <span class="keyword">if</span>((pid = Fork()) == <span class="number">0</span>)&#123;</span><br><span class="line">                        <span class="comment">// printf(&quot;%s&quot;,argv[0]);</span></span><br><span class="line">                        <span class="keyword">if</span>(execvp(argv[<span class="number">0</span>],argv)&lt;<span class="number">0</span>)&#123;</span><br><span class="line">                                <span class="built_in">printf</span>(<span class="string">&quot;Command not found\n&quot;</span>);</span><br><span class="line">                                <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(!bg)&#123;</span><br><span class="line">                        <span class="type">int</span> status;</span><br><span class="line">                        <span class="keyword">if</span>(waitpid(pid,&amp;status,<span class="number">0</span>) &lt; <span class="number">0</span>)</span><br><span class="line">                                unix_error(<span class="string">&quot;waitpid error&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;%d %s\n&quot;</span>,pid,cmdline);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// main.c</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;shell.h&quot;</span></span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">char</span> cmdline[MAXLINE];</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;&gt;&gt;&gt;&quot;</span>);</span><br><span class="line">        Fgets(cmdline,MAXLINE,<span class="built_in">stdin</span>);</span><br><span class="line">        <span class="keyword">if</span>(feof(<span class="built_in">stdin</span>))</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        eval(cmdline);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/06/71-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/06/71-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81-1/" class="post-title-link" itemprop="url">71:异常控制流(1)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-08-06 10:49:30 / 修改时间：18:26:11" itemprop="dateCreated datePublished" datetime="2025-08-06T10:49:30+08:00">2025-08-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">计算机科学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81/" itemprop="url" rel="index"><span itemprop="name">异常控制流</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="异常">异常</h1>
<p>从处理器加电一直到断电，程序计数器始终执行着一个序列的指令。每次从一个指令到下一条指令的过渡被称为控制转移。而这个控制转移序列则被称为处理器的<strong>控制流</strong>。最简单的控制流是一个平滑的序列，由诸如跳转，调用，返回一类的指令造成的。这些指令都是必要的，使程序能根据程序内部状态做出反映。</p>
<p>但是系统也应该能对系统状态的变化做出反应，这些系统状态不会被内部程序变量捕获，也不一定和程序的执行相关。可能是某个硬件向系统发出的信号或是请求。这个时候原本的控制流是难以处理这些情况的，所以现代的系统通过使控制流发生突变，从而对这些情况做出反应。一般而言，我们将这些突变称作<strong>异常控制流(ECF)</strong></p>
<p>异常则是异常控制流的一种形式，指的是控制流中的突变，用来响应处理器的某些变化，下图就反映了这个过程：</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/K1RtFgpsSHPenb5.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>当处理器状态发生一个重要的变化时，这个状态的变化我们称之为<strong>事件</strong>。事件和当前执行的指令相关，比如以0作为除数，算数溢出……</p>
<p>当处理器检测到事件发生时，它就会通过一张叫<strong>异常表</strong>的跳转表，进行一个间接过程的调用（异常），到一个专门设计用来处理这些事件的操作系统子程序（异常处理程序）。事件经过处理后，根据事件类型，程序会进入其中一种状态：</p>
<ul>
<li>控制返回给当前指令<code>I_curr</code>，即事件发生时的指令</li>
<li>控制返回给<code>I_next</code>，如果没有发生异常将会执行的下一条指令</li>
<li>终止该程序</li>
</ul>
<h2 id="异常处理">异常处理</h2>
<p>我们进一步的了解一下，异常处理的过程中都发生了什么。</p>
<p>系统为每种类型的异常都分配了一个唯一的非负整数的<strong>异常号</strong>。号码的分配也有所区别，处理器设计者分配的异常号码通常是零除，内存访问违例，算数溢出一类的。另一部分是，操作系统的内核的设计者分配的，如系统调用和来自外部I/O设备的信号.</p>
<p>在系统启动时，操作系统会分配和初始化一张称为异常表的跳转表，使得表目k包含异常k的处理程序的地址：</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/nm6rfDzxywHFeiN.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>当运行时，处理器检测到发生了一个事件，且确定了其异常号k时。处理器会触发异常，执行间接过程调用，通过异常表的表目k，跳转到相应的处理程序，其过程如下
：</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/waGlhBZuQ9CeAgq.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>异常表基址寄存器是用来存放异常表地址的特殊寄存器，在异常表中，异常号是到异常表的索引。</p>
<p>异常类似于过程调用，但是有些区别：</p>
<ul>
<li>过程调用时，会把返回地址压入栈中（确定的）。但是，根据异常类型，返回的地址会有所不同，返回地址要么是当前指令（事件发生时的执行的指令），要么是下一条指令</li>
<li>由于要切换到异常处理程序，所以我们需要保存额外的处理器状态（通用寄存器，PC，条件寄存器…）以保存上下文。</li>
<li>如果控制从用户程序转移到内核，所有这些项目都被压到内核栈中，而不是用户栈。</li>
<li>异常处理程序运行在内核模式下，它们对所有系统资源都有访问权</li>
</ul>
<p>当硬件触发了异常，剩下的工作就是由异常处理程序在软件中完成。在处理程序处理完毕之后，通过“从终端返回”指令，可选的返回到被中断的程序，该指令将保存的状态弹回寄存器中。并恢复用户模式，将控制返回给呗中断的程序。</p>
<h2 id="异常的类别">异常的类别</h2>
<p>异常可以分为四类：中断(interrupt)、陷阱(trap)、故障(fault)、终止(abort)</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/i2FmTvkAgyI4xrB.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="中断">中断</h3>
<p>中断时异步发生的，时来自处理器外部的I/O设备的信号的结果。硬件中断不是由指令造成的，且不可预测，所以我们说它是异步的。硬件中断的异常处理程序称之为中断处理程序</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/vQmHLj5XZ9cNuab.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>设备会将异常号放到系统总线上，并且向处理器的中断引脚发送信号。当处理器发现中断引脚电压升高，就会读取异常号，调用中断处理程序。当处理程序返回时，就将控制返回给下一条指令。这样从外界看，就好像没有发生过中断一样。</p>
<p>剩下的异常类型都是同步发生的，他们是执行当前指令的结果。我们把这类指令叫做故障指令。</p>
<h3 id="陷阱和系统调用">陷阱和系统调用</h3>
<p>陷阱是有意的异常，是一条指令执行的结果。它可以在用户程序和内核之间提供一个像过程一样的接口，即<strong>系统调用</strong>。</p>
<p>用户程序经常需要像内核请求服务。如读取文件(read)、创建一个新的进程(fork)、加载一个新的程序(exec)、终止当前进程(exit)。为了支持对这些内核服务的访问，处理器支持<code>syscall n</code>指令，当用户想要请求服务<code>n</code>时，可以执行这个指令。执行<code>syscall</code>会导致一个到异常处理程序的陷阱，这个处理程序会解析参数，调用合适的内核程序。</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/VtKIB6imG1yMTox.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>看起来系统调用和函数调用是一样的，但是实际上函数调用是在用户模式下进行，用户模式限制了函数可执行的指令的类型，而且他们只能访问于调用函数相同的栈。系统调用则是运行在内核模式中，内核模式允许指令调用特权指令，并访问内核中的栈。</p>
<h3 id="故障">故障</h3>
<p>故障是由错误情况导致的，它可能会被故障处理程序修正。当故障发生，处理器会将控制传递给故障处理程序。如果故障被修读，就将控制传递会引起故障的程序，重新执行。否则，船里程序会返回<code>abort</code>历程例程，从而终止引起故障的应用程序。</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/rFLMOPeKX2n5YJb.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="终止">终止</h3>
<p>终止是不可恢复的错误造成的结果
。终止处理程序不会将控制返回给应用程序，而是但会给一个abort例程，从而终止这个应用。</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/GbrpKvjw8Nls5df.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="linuxx86-64系统中的异常">Linux/x86-64系统中的异常</h2>
<p>为了认识的更加具体，我们可以看看x86_64系统定义的一些异常。其中<code>0~31</code>的号码对应Intel架构定义的异常。<code>32~255</code>的号码对应的是操作系统定义的中断和陷阱。</p>
<p>这是一些比较常见的：</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/y5lTFS4qr3jDmku.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="故障和终止">故障和终止</h3>
<ul>
<li><strong>除法错误：</strong>当试图除0时，或者一个除法指令的结果对于目标操作数而言太大的时候，就会导致除法错误。在LinuxShell里面，执行一个有除法错误的程序会报告<code>Floating point exception</code></li>
<li><strong>一般保护故障：</strong>这个故障比较容易触发，通常是因为程序引用了一个未定义的虚拟内存区域，或者是尝试写一个只读文本段。Linux不会恢复这类故障，会报告为段故障<code>Segmentation fault</code></li>
<li><strong>缺页：</strong>这是一个会重新执行产生故障的指令的一个异常示例。处理程序会将适当的磁盘上的虚拟内存的一个页面映射到物理内存的一个页面，然后重新执行这个产生故障的指令。</li>
<li><strong>机器检查：</strong>机器检查是在导致故障的指令执行中检测到致命的硬件错误时发生的。</li>
</ul>
<h3 id="系统调用">系统调用</h3>
<p>下面展示一些常用的系统调用</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/rUEbyfLgIjKPdHn.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>C语言中可以用syscall来进行系统调用。不过没必要，因为在&lt;unistd.h&gt;头文件中，封装了许多对操作系统底层服务的访问接口。我们将这些系统调用和包装函数称为系统级函数。</p>
<p>在Linux系统中，我们使用<code>syscall</code>陷阱指令来实现系统调用。它的调用过程如下：</p>
<p>使用寄存器<code>%rax</code>包含系统调用号，使用寄存器<code>%rdi %rsi %rdx %r10 %r8 %r9</code>来依次传递参数。从系统调用返回时，<code>%r11 %rcx</code>会被破坏（因为rcx用来存放返回地址，r11存放标志寄存器），<code>%rax</code>存放返回值。如果返回值是负数则说明发生错误。</p>
<p>可以通过查看系统级函数的编译来看到这个参数传递的过程:</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/9DruvC53JEP8AnI.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h1 id="进程">进程</h1>
<p>在系统上运行一个程序时，我们会得到一个假象，我们的程序似乎是系统中的唯一一个程序，独占着内存和处理器的使用。但事实并非如此。</p>
<p>系统中的每个程序都运行在某个进程的上下文中。上下文由程序正确运行所需的状态组成。这个状态包括许多，如存放在内存中的数据和代码，它的栈、通用寄存器的内容、程序计数器、环境变量、和打开文件描述符的集合。</p>
<p>每次运行一个新的程序时，shell就会创建一个新的进程，然后再这个新进程的上下文中运行这个程序。应用程序也是如此，创建新进程，并且再新进程的上下文中运行自己的代码和其他应用程序。不过我们只需要关注进程提供给应用程序的关键抽象：</p>
<ul>
<li>一个独立的逻辑控制流：提供一个假象，让我们认为程序独占处理器</li>
<li>一个私有的地址空间：提供一个假象，让我们以为程序独占内存系统</li>
</ul>
<h2 id="逻辑控制流">逻辑控制流</h2>
<p>通常系统中同时有很多程序在进行，进程可以向程序提供一个假象，自以为独占处理器与内存。但实际上并非如此。</p>
<p>我们将一个程序的顺序执行的PC值的序列称为<strong>逻辑控制流</strong>。将处理器执行的PC值的序列称为物理控制流。那么，在处理器的视角中控制流的转移实际上是这样的：</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/tfipN5LPgykZrzO.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>进程实际上是轮流使用处理器的。每个进程执行它的流的一部分，然后被抢占（暂时挂起），然后轮到其他进程。再次运行这个进程时，由于进程的上下文信息不变，所以运行在这些进程之一的上下文中的程序，它自认为是始终独占处理器的。</p>
<h2 id="并发流">并发流</h2>
<p>如果一个逻辑流得执行在时间上和另一个流重叠，称为<strong>并发流</strong>，这两个流称为并发的运行。多个流并发的执行的一般现象称为并发。一个进程和其他进程轮流运行的概念称为<strong>多任务</strong>。一个进程执行它的控制流的一部分的每一时间段就叫做<strong>时间片</strong>。因此多任务也叫时间分片。例如上图的进程A就是由两个时间片组成的。</p>
<p>这里我们还要提到一下并行和并发的区别。并行是并发的一个真子集，只不过并行是并发的运行在不同的处理器核或计算机上的。现代计算机的并行能力，是基于计算机数或处理器核数上的，单一的处理器核无法实现并行。这一点要加以区分。</p>
<h2 id="私有地址空间">私有地址空间</h2>
<p>进程也为每个程序提供一个假象，好像它独占了系统地址空间。这是因为进程为每个程序提供了自己的私有地址空间（进程地址空间）。一般而言，和这个空间中某个地址相关联的内存字节，是不能被其他进程读或写的。这个意义上来说，这个地址空间是私有的。</p>
<p>尽管和每个私有地址空间相关联的内存的内容一般是不同的，但是每个这样的空间都有相同的通用结构：</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/mxfuyZFswWo1kbQ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>地址空间的底部是留给用户程序的，地址空间的顶部总是留给内核。这里需要注意，代码段总是从地址<code>0x0040000</code>开始的。这个进程的地址空间是进程上下文的一部分。</p>
<h2 id="用户模式和内核模式">用户模式和内核模式</h2>
<p>为了进一步提供进程的抽象能力，操作系统需要一种机制，限制一个应用可以执行的指令以及它可以访问的地址空间范围。</p>
<p>处理器通过控制某个控制寄存器中的一个模式位来提供这种功能，这个寄存器会描述当前的进程所享有的特权。当设置了模式位时，进程就运行在内核模式下。在内核模式下的进程可以执行指令集中的所有指令，访问系统中的任何内存地址。</p>
<p>没有设置模式位时，在用户模式下的进程，不允许执行特权指令，比如停止处理器，改变模式位，发起IO操作…….。也不允许进程直接引用地址空间中内核区的代码和数据。否则会引起故障保护，用户程序只能通过系统调用接口间接的访问内核的代码和数据。</p>
<p>初始时，应用程序代码的进程是在用户模式中的，当发生异常时。控制传递到异常处理程序，处理器从用户模式切换到内核模式。处理程序在内核模式中运行，当它返回到应用程序时，处理器将内核模式切换回用户模式</p>
<p>当然除此之外，Linux提供了一系列的机制可以让用户进程访问内核的数据结构的内容。在<code>/proc</code>下我们可以访问进程的属性还有一般的系统属性。<code>/sys</code>中则可以查看关于系统总线和设备的底层信息…….</p>
<h2 id="上下文切换">上下文切换</h2>
<p>操作系统内核通过<strong>上下文切换</strong>的机制来实现多任务。这个机制是基于底层的异常机制之上的。</p>
<p>内核为每个进程维护一个上下文。上下文就是内核重新启动一个进程所需要的状态。它由一系列的对象的值组成。这些对象有通用目的寄存器、浮点寄存器、程序计数器、用户栈、状态寄存器、内核栈和一系列内核数据结构（例如描述地址空间的页表、包含有关当前进程信息的进程表，以及包含进程已打开文件的信息的文件表……）组成的。</p>
<p>在进程执行的某些时刻，内核可以挂起当前进程转而执行恢复执行其他被挂起的进程。这个决策被称为<strong>调度</strong>，由内核中的调度器处理。当内核选择一个新的进程时，我们就称内核调度了这个进程。内核调度了一个新的进程后，就抢占当前进程。使用上下文切换的机制来控制转移新的进程。</p>
<p>上下文切换主要分为三个过程：</p>
<ul>
<li>保存当前进程的上下文</li>
<li>恢复某个先前被挂起的进程的上下文</li>
<li>将控制传递给新恢复的进程</li>
</ul>
<p>了解了上下文切换，我们再来看看上下文切换的场景：</p>
<ul>
<li>执行系统调用sleep</li>
<li>系统调用因为等待某个事件而阻塞时</li>
<li>中断发生（有的系统会有周期性的定时中断器，以免处理器在单个进程运行太长时间）</li>
<li>……..</li>
</ul>
<p>总而言之就是尽可能安排任务，不要让处理器空转。下面这个图片就很好的体现了这个过程：</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/fE85YFu9ib1C2hK.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/05/70-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/05/70-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1/" class="post-title-link" itemprop="url">70:初窥深度学习(1)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-05 16:06:58" itemprop="dateCreated datePublished" datetime="2025-08-05T16:06:58+08:00">2025-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-29 13:56:35" itemprop="dateModified" datetime="2025-11-29T13:56:35+08:00">2025-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近很流行这些，出于好奇，我也想知道这些技术背后的原理是什么。而且我感觉很多知识可能之后会用到，所以我打算浅浅的了解一下。最终的目标是实现一个手写数字识别的神经网络吧。试试看吧。</p>
<h1 id="神经网络简介">神经网络简介</h1>
<h2 id="基础构件神经元">基础构件：神经元</h2>
<p>神经元是神经网络的基本单元。它接受输入，对数据进行计算从而产生一个输出。比如下面的一个二元输入神经元样例：</p>
<figure>
<img src="https://s2.loli.net/2025/08/05/ryetOK7vjB2D3Y9.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这个神经元进行了以下操作：</p>
<ul>
<li><p>输入乘以权重w：</p>
<p>x<sub>1</sub> –&gt; x<sub>1</sub> * w<sub>1</sub> x<sub>2</sub> –&gt;
x<sub>2</sub> * w<sub>2</sub></p></li>
<li><p>加权输入与偏置b相加：</p>
<p>( x<sub>1</sub> * w<sub>1</sub>) + (x<sub>2</sub> * w<sub>2</sub>) +
b</p></li>
<li><p>最后将总和传递给激活函数：(其中f是激活函数)</p>
<p>y = f(x<sub>1</sub> * w<sub>1</sub> + x<sub>2</sub> * w<sub>2</sub> +
b)</p></li>
</ul>
<p>对于任意输入的神经元，我们的输出是：</p>
<p><span class="math display">$$
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
$$</span></p>
<p>对于激活函数<code>f</code>我们需要额外了解到，在不引入激活函数的情况下，我们的输出和下一个输入的结果之间总是线性的关系。我们使用激活函数则可以将无界的输入转换成良好的、可以预测形式的输出。这里我们使用的激活函数是<code>sigmoid</code>函数：</p>
<p><span class="math display">$$
\begin{aligned}
f(x)=\frac{1}{1+e^{-x}}
\end{aligned}
$$</span></p>
<figure>
<img src="https://s2.loli.net/2025/08/05/kHw1mWV6jEPCUze.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><code>sigmoid</code>函数只输出（0，1）范围内的数值，它将<span
class="math inline">(−∞, +∞)</span>的数值压缩到了（0，1）.</p>
<h3 id="简单的举例">简单的举例</h3>
<p>假设我们现在有一个使用sigmoid激活函数的二元输入神经元，其<code>w=[0,1] b=4</code></p>
<p>若我们想神经元输入<code>x = [2,3]</code>，我们可以得到</p>
<figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(w * x) + b = 0*2 + 1*3 + 4</span><br><span class="line">		   = 7</span><br><span class="line">y = f(w*x+b) = f(7) = 0.999</span><br></pre></td></tr></table></figure>
<p>我们向前传递输入以获取输出，这个过程我们称之为前馈(<code>feedforward</code>)</p>
<h3 id="神经元的代码实现">神经元的代码实现</h3>
<p>我们使用Python中的numpy来实现这个功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x: <span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weights, bias</span>):</span><br><span class="line">        <span class="variable language_">self</span>.weights = weights</span><br><span class="line">        <span class="variable language_">self</span>.bias = bias</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        total = np.dot(<span class="variable language_">self</span>.weights, inputs) + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="keyword">return</span> sigmoid(total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">weights = np.array([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">bias = <span class="number">4</span></span><br><span class="line">n = Neuron(weights,bias)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(n.feedforward(x))</span><br><span class="line"><span class="comment"># 0.9990889488055994</span></span><br></pre></td></tr></table></figure>
<p>可以看到我们的输出和我们的计算是吻合的</p>
<h2 id="将神经元组合成神经网络">将神经元组合成神经网络</h2>
<p>神经网络实际上是许多相互连接的神经元，一个简单的神经元长这样：</p>
<figure>
<img src="https://s2.loli.net/2025/08/05/1mdrhxJyzN6cCep.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这个网络有两个输入组成的输入层，还有两个神经元(h<sub>1</sub>，h<sub>2</sub>)组成的隐藏层，以及一个神经元(o<sub>1</sub>)组成的输出层。其中隐藏层指的是位于输入层和输出层之间的任何层，可以有多个隐藏层。</p>
<h3 id="简单的举例前馈计算">简单的举例：前馈计算</h3>
<p>我们使用上述的网络，令每个神经元都是使用sigmoid激活函数且<code>w=[0,1] b=0</code>，用<code>h1 h2 o1</code>来表示神经元的输出，则有：</p>
<figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">h1 = h2 = f(w*x+b)</span><br><span class="line">		=f((0*2)+(1*3)+0)</span><br><span class="line">		=f(3)</span><br><span class="line">		=0.9526</span><br><span class="line">o1 = f(w*x+b)</span><br><span class="line">	= f(0.9526)</span><br><span class="line">	= 0.7216</span><br></pre></td></tr></table></figure>
<p>此时我们的神经网络的前馈就是<code>0.7216</code>，整个过程简而言之就是，将输入信息通过网络中的神经元向前传递，最终得到输出信息，作为整个神经网络的前馈</p>
<h3 id="神经网络的代码实现">神经网络的代码实现</h3>
<p>现在我们为这个简单的神经网络实现前向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        weights = np.array([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">        bias = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.h1 = Neuron(weights,bias)</span><br><span class="line">        <span class="variable language_">self</span>.h2 = Neuron(weights,bias)</span><br><span class="line">        <span class="variable language_">self</span>.o1 = Neuron(weights,bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforword</span>(<span class="params">self,x</span>):</span><br><span class="line">        out_h1 = <span class="variable language_">self</span>.h1.feedforward(x)</span><br><span class="line">        out_h2 = <span class="variable language_">self</span>.h2.feedforward(x)</span><br><span class="line">        out_o1 = <span class="variable language_">self</span>.o1.feedforward(np.array([out_h1,out_h2]))</span><br><span class="line">        <span class="keyword">return</span> out_o1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">network = NeuralNetwork()</span><br><span class="line">x = np.array([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(network.feedforword(x))</span><br><span class="line"><span class="comment"># 0.7216325609518421</span></span><br></pre></td></tr></table></figure>
<p>和我预期的答案是吻合的</p>
<h2 id="训练神经网络">训练神经网络</h2>
<h3 id="损失">损失</h3>
<p>训练神经网络意味着，有预测的答案和实际的答案。训练的过程就是让网络预测的结果贴合真实的答案。那么首先我们就需要知道，预测的答案和真实的答案差距有多大，我们需要将它量化。</p>
<p>假设有以下测量值：</p>
<figure>
<img src="https://s2.loli.net/2025/08/05/3vRlEQd6C1OYrfb.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们用0表示男性，用1表示女性。我们要训练我们的网络，根据体重和身高预测某人的性别。我们通过对数据设置偏移，让它更容易被处理：</p>
<figure>
<img src="https://s2.loli.net/2025/08/05/riXCgwjOT9LQPsu.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>现在我们需要找到一个方法量化它的”好坏”，以训练它做的更好。这里我们使用均方误差损失(MSE)来衡量它的好坏：
<span class="math display">$$
MSE = \frac{1}{n}\sum_{i=1}^{n}(y_{true} - y_{pred})^2
$$</span> 其中：</p>
<ul>
<li>n是样本数量，这里是4</li>
<li>y代表被预测的变量，这里是性别</li>
<li>y<sub>true</sub>是变量的真实值（“正确答案”）</li>
<li>y<sub>pred</sub>是网络输出的结果，即预测值</li>
</ul>
<p>我们可以用代码实现MSE的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_true,y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> ((y_true - y_pred)**<span class="number">2</span>).mean()</span><br></pre></td></tr></table></figure>
<h3 id="反向传播">反向传播</h3>
<p>我们现在已经量化了我们的损失，我们现在需要通过调整网络的权重和偏差从而使得预测更加准确。我们该怎么做呢?</p>
<p>我们从下面这个最简单的情况开始，一点一点反推整个训练的过程</p>
<figure>
<img src="https://s2.loli.net/2025/08/05/rNY86vJlCkgSeuw.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>在这次训练中，正确答案为1，预测结果为y<sub>pred</sub>。此时有： <span
class="math display">$$
\begin{align*}
MSE = \frac{1}{1}\sum_{i=1}^{n}(1-y_{pread})^2
= (1-y_{pred})^2
\end{align*}
$$</span>
有了量化的偏差，接下来我们给网络中的每个权重和偏差都标记出来，此时我们可以写出一个多变量函数：
<span
class="math display"><em>L</em>(<em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, <em>w</em><sub>3</sub>, <em>w</em><sub>4</sub>, <em>w</em><sub>5</sub>, <em>w</em><sub>6</sub>, <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, <em>b</em><sub>3</sub>)</span>
<img src="https://s2.loli.net/2025/08/05/igJTIyfzBGuY2jm.png"
alt="image.png" /></p>
<p>假如我们调整w1，那么损失L会怎么变化呢？也就是说我们需要求出<span
class="math inline">$\frac{\partial L}{\partial
w_1}$</span>，从而进一步调整w1以减少L</p>
<p>我们可以用下列过程来求出它： <span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial w_1} &amp;= \frac{\partial L}{\partial
y_{pred}}*\frac{\partial y_{pred}}{\partial w_1}
\\
\frac{\partial L}{\partial y_{pred}} &amp;= \frac{\partial
(1-y_{pred})^2}{\partial y_{pred}} = -2(1-y_{pred})
\end{align*}
$$</span> 我们想处理<span class="math inline">$\frac{\partial
y_{pred}}{\partial w_1}$</span>，需要用h1 h2 o1
来代表神经元的输出，然后得到： <span class="math display">$$
\begin{align*}
\frac{\partial y_{pred}}{\partial w_1} &amp;= \frac{\partial
y_{pred}}{\partial h_1}*\frac{\partial h_1}{\partial w_1}
\\
\\
y_{pred} &amp;= o_1 = f(w_5h_1 + w_6h_2 + b_3)
\\
\frac{\partial y_{pred}}{\partial h_1} &amp;= w_5*f'(w_5h_1 + w_6h_2 +
b_3)
\\
\\
h_1 &amp;= f(w_1x_1+w_2x_2+b_1)
\\
\frac{\partial h_1}{\partial w_1} &amp;= x_1*f'(w_1x_1+w_2x_2+b_1)
\end{align*}
$$</span> 这里我们要用到激活函数的导数，所以对其进行求导： <span
class="math display">$$
\begin{align*}
f(x)&amp;=\frac{1}{1+e^{-x}}
\\
f'(x)&amp;=\frac{e^{-x}}{(1+e^{-x})^2}=f(x)*(1-f(x))
\end{align*}
$$</span> 现在我们可以合并计算出 <span class="math display">$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial
y_{pred}}*\frac{\partial y_{pred}}{\partial h_1}*\frac{\partial
h_1}{\partial w_1}
$$</span>
这个反向计算偏导数的系统被称之为<strong>反向传播</strong>。现在我们可以带入数值计算出<span
class="math inline">$\frac{\partial L}{\partial
w_1}=0.0214$</span>，我们可以根据这个值来训练我们的权重。</p>
<h3 id="训练">训练</h3>
<p>于是我们可以制定我们的训练过程了。在这里我们使用一种名为随机梯度下降的算法，它将告诉我们如何调整权重和偏差以最小化损失。它实际上就是这么个更新公式：
<span class="math display">$$
w_1 \gets w_1 - \eta*\frac{\partial L}{\partial w_1}
$$</span> 这里的<span
class="math inline"><em>η</em></span>指的是学习率，用来控制我们训练的速度和精度。我们对网络中的每个权重和偏差都这么做，我们的损失将慢慢减少，我们的网络将越来越准确。</p>
<p>我们的徐连过程将如下：</p>
<ul>
<li>从数据集中选择一个样本（随机梯度下降的原理就是一次只操作一个样本）</li>
<li>计算损失相对于权重或偏差的所有偏导数</li>
<li>使用更新方程来更新每个权重和偏差</li>
<li>重复</li>
</ul>
<h3 id="实现一个完整的神经网络">实现一个完整的神经网络</h3>
<p>现在我们可以实现一个完整的神经网络来实现这个训练过程了</p>
<p>这是我们的数据集和网络结构：</p>
<figure>
<img src="https://s2.loli.net/2025/08/05/riXCgwjOT9LQPsu.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/05/igJTIyfzBGuY2jm.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">deriv_sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> sigmoid(x)*(<span class="number">1</span>-sigmoid(x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_loss</span>(<span class="params">y_true,y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> ((y_true - y_pred)**<span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neuron</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weights, bias</span>):</span><br><span class="line">        <span class="variable language_">self</span>.weights = weights</span><br><span class="line">        <span class="variable language_">self</span>.bias = bias</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        total = np.dot(<span class="variable language_">self</span>.weights, inputs) + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="keyword">return</span> sigmoid(total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.w1 = np.random.normal()</span><br><span class="line">        <span class="variable language_">self</span>.w2 = np.random.normal()</span><br><span class="line">        <span class="variable language_">self</span>.w3 = np.random.normal()</span><br><span class="line">        <span class="variable language_">self</span>.w4 = np.random.normal()</span><br><span class="line">        <span class="variable language_">self</span>.w5 = np.random.normal()</span><br><span class="line">        <span class="variable language_">self</span>.w6 = np.random.normal()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.b1 = np.random.normal()</span><br><span class="line">        <span class="variable language_">self</span>.b2 = np.random.normal()</span><br><span class="line">        <span class="variable language_">self</span>.b3 = np.random.normal()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self,x</span>):</span><br><span class="line">        h1 = sigmoid(<span class="variable language_">self</span>.w1 * x[<span class="number">0</span>] + <span class="variable language_">self</span>.w2 * x[<span class="number">1</span>] + <span class="variable language_">self</span>.b1)</span><br><span class="line">        h2 = sigmoid(<span class="variable language_">self</span>.w3 * x[<span class="number">0</span>] + <span class="variable language_">self</span>.w4 * x[<span class="number">1</span>] + <span class="variable language_">self</span>.b2)</span><br><span class="line">        o1 = sigmoid(<span class="variable language_">self</span>.w5 * h1 + <span class="variable language_">self</span>.w6 * h2 + <span class="variable language_">self</span>.b3)</span><br><span class="line">        <span class="keyword">return</span> o1</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self,data,all_y_trues</span>):</span><br><span class="line">        learn_rate = <span class="number">0.05</span></span><br><span class="line">        epochs = <span class="number">5000</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            <span class="keyword">for</span> x,y_true <span class="keyword">in</span> <span class="built_in">zip</span>(data,all_y_trues):</span><br><span class="line">                sum_h1 = <span class="variable language_">self</span>.w1 * x[<span class="number">0</span>] + <span class="variable language_">self</span>.w2 * x[<span class="number">1</span>] + <span class="variable language_">self</span>.b1</span><br><span class="line">                h1 = sigmoid(sum_h1)</span><br><span class="line">                sum_h2 = <span class="variable language_">self</span>.w3 * x[<span class="number">0</span>] + <span class="variable language_">self</span>.w4 * x[<span class="number">1</span>] + <span class="variable language_">self</span>.b2</span><br><span class="line">                h2 = sigmoid(sum_h2)</span><br><span class="line">                sum_o1 = <span class="variable language_">self</span>.w5 * h1 + <span class="variable language_">self</span>.w6 * h2 + <span class="variable language_">self</span>.b3</span><br><span class="line">                o1 = sigmoid(sum_o1)</span><br><span class="line">                y_pred = o1</span><br><span class="line"></span><br><span class="line">                d_L_d_ypred = -<span class="number">2</span>*(y_true-y_pred)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># o1</span></span><br><span class="line">                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)</span><br><span class="line">                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)</span><br><span class="line">                d_ypred_d_b3 = deriv_sigmoid(sum_o1)</span><br><span class="line">                d_ypred_d_h1 = <span class="variable language_">self</span>.w5 * deriv_sigmoid(sum_o1)</span><br><span class="line">                d_ypred_d_h2 = <span class="variable language_">self</span>.w6 * deriv_sigmoid(sum_o1)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># h1</span></span><br><span class="line">                d_h1_d_w1 = x[<span class="number">0</span>] * deriv_sigmoid(sum_h1)</span><br><span class="line">                d_h1_d_w2 = x[<span class="number">1</span>] * deriv_sigmoid(sum_h1)</span><br><span class="line">                d_h1_d_b1 = deriv_sigmoid(sum_h1)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># h2</span></span><br><span class="line">                d_h2_d_w3 = x[<span class="number">0</span>] * deriv_sigmoid(sum_h2)</span><br><span class="line">                d_h2_d_w4 = x[<span class="number">1</span>] * deriv_sigmoid(sum_h2)</span><br><span class="line">                d_h2_d_b2 = deriv_sigmoid(sum_h2)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># h1 train</span></span><br><span class="line">                <span class="variable language_">self</span>.w1 -= d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1 * learn_rate</span><br><span class="line">                <span class="variable language_">self</span>.w2 -= d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2 * learn_rate</span><br><span class="line">                <span class="variable language_">self</span>.b1 -= d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1 * learn_rate</span><br><span class="line"></span><br><span class="line">                <span class="comment"># h2 train</span></span><br><span class="line">                <span class="variable language_">self</span>.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3</span><br><span class="line">                <span class="variable language_">self</span>.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4</span><br><span class="line">                <span class="variable language_">self</span>.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2</span><br><span class="line"></span><br><span class="line">                <span class="comment"># o1 train</span></span><br><span class="line">                <span class="variable language_">self</span>.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5</span><br><span class="line">                <span class="variable language_">self</span>.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6</span><br><span class="line">                <span class="variable language_">self</span>.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                y_preds =np.apply_along_axis(<span class="variable language_">self</span>.feedforward,<span class="number">1</span>,data)</span><br><span class="line">                loss = mse_loss(all_y_trues,y_preds)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Epoch %d loss: %.3f&quot;</span> % (epoch,loss))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 来源：国家体育总局《第五次国民体质监测公报》2022[^44^]</span></span><br><span class="line"><span class="comment"># cm-150 kg-50</span></span><br><span class="line">data = np.array([</span><br><span class="line">    [<span class="number">10.6</span>, <span class="number">5.7</span>],   <span class="comment"># 女 20-24 岁</span></span><br><span class="line">    [<span class="number">9.8</span>, <span class="number">6.7</span>],   <span class="comment"># 女 25-29 岁</span></span><br><span class="line">    [<span class="number">9.1</span>, <span class="number">8.0</span>],   <span class="comment"># 女 30-34 岁</span></span><br><span class="line">    [<span class="number">8.6</span>, <span class="number">9.1</span>],   <span class="comment"># 女 35-39 岁</span></span><br><span class="line">    [<span class="number">8.0</span>, <span class="number">9.7</span>],   <span class="comment"># 女 40-44 岁</span></span><br><span class="line">    [<span class="number">7.5</span>, <span class="number">10.1</span>],   <span class="comment"># 女 45-49 岁</span></span><br><span class="line">    [<span class="number">7.2</span>, <span class="number">10.8</span>],   <span class="comment"># 女 50-54 岁</span></span><br><span class="line">    [<span class="number">7.0</span>, <span class="number">10.7</span>],   <span class="comment"># 女 55-59 岁</span></span><br><span class="line">    [<span class="number">22.6</span>, <span class="number">20.4</span>],   <span class="comment"># 男 20-24 岁</span></span><br><span class="line">    [<span class="number">22.1</span>, <span class="number">22.8</span>],   <span class="comment"># 男 25-29 岁</span></span><br><span class="line">    [<span class="number">21.4</span>, <span class="number">24.3</span>],   <span class="comment"># 男 30-34 岁</span></span><br><span class="line">    [<span class="number">20.4</span>, <span class="number">24.0</span>],   <span class="comment"># 男 35-39 岁</span></span><br><span class="line">    [<span class="number">19.4</span>, <span class="number">23.2</span>],   <span class="comment"># 男 40-44 岁</span></span><br><span class="line">    [<span class="number">18.7</span>, <span class="number">22.5</span>],   <span class="comment"># 男 45-49 岁</span></span><br><span class="line">    [<span class="number">17.9</span>, <span class="number">21.6</span>],   <span class="comment"># 男 50-54 岁</span></span><br><span class="line">    [<span class="number">17.5</span>, <span class="number">21.0</span>]    <span class="comment"># 男 55-59 岁</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">all_y_trues = np.array([</span><br><span class="line">    <span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,   <span class="comment"># 8 位女性</span></span><br><span class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>    <span class="comment"># 8 位男性</span></span><br><span class="line">])</span><br><span class="line">network = NeuralNetwork()</span><br><span class="line">network.train(data,all_y_trues)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(network.feedforward([<span class="number">161</span>-<span class="number">150</span>,<span class="number">65</span>-<span class="number">50</span>]))</span><br></pre></td></tr></table></figure>
<p>找了下几个热心嘉宾试了一下还是很准确滴</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/04/69-%E9%9A%8F%E4%BE%BF%E7%9A%84%E6%83%B3%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/04/69-%E9%9A%8F%E4%BE%BF%E7%9A%84%E6%83%B3%E6%B3%95/" class="post-title-link" itemprop="url">69:随便的想法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-04 23:08:48" itemprop="dateCreated datePublished" datetime="2025-08-04T23:08:48+08:00">2025-08-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-29 13:46:54" itemprop="dateModified" datetime="2025-11-29T13:46:54+08:00">2025-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>和好兄弟出去玩，谈到上大学。我觉得很遗憾，遗憾自己没能去想去的学校。他说，感觉人生就是因为有遗憾的事情所以才值得去回忆。我想想也是，我看历史书也是这样的，就是因为有遗憾的部分我才总是记得很清楚。就像是三国演义的诸葛之死，我每次想到都很难过，王者荣耀这个赛季也是三国主题。我看别人都是选的魏国吴国的阵营，我就选蜀国，我就是单纯的比较喜欢。</p>
<p>感觉人生也是小小的历史，从小到大也有很多遗憾的瞬间。某场惜败，某个街头匆匆错过的音乐，或者是未曾意识到的错误。我总是会想，要是……就好了，我每次这么想着都觉得很有趣，人生有好多可能，像是好多部电影。我有时候就是喜欢什么也不干躺在床上想这些，很多偶然记下的细节在脑子里循环播放，我想从每件事情里面都总结出什么，吸取经验教训。像历史那样，避免前人的错误，但我就是做不到。或者说是不太喜欢各种道理，我比较喜欢想到什么做什么，但是即使是这样我也有一直想做的事情。</p>
<p>我想做点有意义的事情，就是那种别人一听就能想起我的事情。不是违法犯罪那种，也不是今日头条那种。就是比较有意思的事情有价值的东西，比如牛顿定律那种，一听就能想起牛顿。或者是相对论，想起爱因斯坦。不过我就打个比方，总之就是想留下点有个人价值的事情吧。比如做个好玩的游戏，有个好玩的发明发现啥的，但我感觉还是挺难的。</p>
<p>我学东西感觉还是太慢了，技术也比较落后吧。我总是喜欢刨根问底，我现在在学的计算机在这一点上就让我好痛苦。计算机里我最讨厌的就是封装，它屏蔽了我对原理的认识；最喜欢的东西也是封装，因为把自己的归纳和设计封装起来很有成就感。导致我每次看到一个技术或者一个功能，我总是喜欢自己动手实现一下。我感觉这是一个好的品质，我看网上书上都说这样好，但我又感觉这样不好。我是不是在做没有意义的事，我这样是不是不适合当下的快节奏的社会。短短的大学四年我应该将时间和精力去浪费在这些事情上嘛。</p>
<p>什么是浪费，什么是有价值的。我的做法是正确的嘛。我只是想试试想看看，就是感觉很神奇。我平时看课外书也是这样，总喜欢看些没用的东西，好多人多我说，这些没用的知识早晚都会帮到你，我也想这样想，但我更清楚，我可能这辈子也不会用到他们。但我就是想知道，我也有时候会突然想到一些内容想要对别人说，一些好玩的科普，一些好玩的典故。但是感觉大多数人都不太感兴趣，或者有的人觉得是在炫耀。所以我就不想说了。</p>
<p>感觉这么一想都是从好功利的角度出发，因为换个角度将，这些想法都是可以被反驳的。我感觉打出来的字都是好意识流的哦，这么一看感觉人的思绪也是好凌乱的，也是很矛盾的。今天突然想随便写一些东西，因为我想暑假把我的博客数量争取破80，所以随便水一水。今天和同学聊天，我说想试试不用库写一个深度学习的模型，emm用C++吧。但是他说不太可能，我感觉还是挺可能的，我打算接下来试一试。刚好找点事情做。之后再是学下图像加密什么的，最近听学长聊天，我感觉科研好重要哦。不过我对这个也挺感兴趣的。不过我想先学掉链接之后再说。不知道怎么下手哦好烦。也不知道怎么跟导师联系，我好怕问些好蠢的问题。</p>
<p>我感觉打游戏还是挺好玩的，玩我的世界，总有要干的事情，不过最好玩的还是向懂行的展示自己的成果，很好玩。就是下矿不太好玩，火把总是不够，怪物总是到处出来。我现在还开了困难模式，所以更难玩了。我想之后把怪物猎人和艾尔登法环通关。暑假好短呀。</p>
<p>太晚了，我先不写了。躺在床上玩一会儿手机就可睡觉了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ylin"
      src="/images/1.jpg">
  <p class="site-author-name" itemprop="name">Ylin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">108</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Ylin07" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ylin07" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/2504_90550008?spm=1010.2135.3001.5343" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;2504_90550008?spm&#x3D;1010.2135.3001.5343" rel="noopener" target="_blank">Ylin's CSDN</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/ylin07" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;ylin07" rel="noopener" target="_blank">Ylin's 博客园</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://bluestar-34.github.io/" title="https:&#x2F;&#x2F;bluestar-34.github.io&#x2F;" rel="noopener" target="_blank">Neroblue's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://haoine.github.io/" title="https:&#x2F;&#x2F;haoine.github.io&#x2F;" rel="noopener" target="_blank">Haoine's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://admintor889.github.io/" title="https:&#x2F;&#x2F;admintor889.github.io&#x2F;" rel="noopener" target="_blank">Cnext's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://auberginewly.site/" title="https:&#x2F;&#x2F;auberginewly.site&#x2F;" rel="noopener" target="_blank">auberginewly's Blog</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ylin</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">248k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:46</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
