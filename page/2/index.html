<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>


  <meta property="og:type" content="website">
<meta property="og:title" content="Ylin&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Ylin&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Ylin">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Ylin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ylin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/27/84-%E4%B9%9D%E6%9C%88%E7%9A%84%E5%8F%8D%E6%80%9D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/84-%E4%B9%9D%E6%9C%88%E7%9A%84%E5%8F%8D%E6%80%9D/" class="post-title-link" itemprop="url">84:九月的反思</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-27 19:56:50 / 修改时间：20:19:59" itemprop="dateCreated datePublished" datetime="2025-09-27T19:56:50+08:00">2025-09-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">日常总结</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>793</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这个月发生了各种各样的事情，我特别累也特别疲惫。对于上次以后的总结，我几乎记不得中间都做了哪些事情。我不想对这一个月的经历过多谈论，我心里是十分难过的。</p>
<p>接下来一段时间会很忙，忙在哪些地方呢？</p>
<ul>
<li>NJU PA实验，我要坚持写下去</li>
<li>操作系统真象还原，我也要坚持看下去，我希望能通过这本书的指引，实现一个自己的操作系统。之后我想仿写xv6的源代码，实现一个基本的操作系统内核。</li>
<li>最近新接的科研训练项目，主题是<code>基于动态欺骗的主动网络防护技术</code>，现在还在写文档阶段，非常无聊和痛苦</li>
<li>这个学期冗杂的课程，还有各种活动</li>
</ul>
<p>直白的说，我这个月是感情失利了。我和在一起四年的一个女生分开了。感觉就是很难过，不知道怎么用语言描述，我一开始天天躺着，天天打游戏来麻痹自己，但是感觉都没什么用。我不知道怎么办，我想各种办法去转移自己的注意力，但是无论做什么都没办法集中自己，这几天睡得也很晚，总之就是精神状态很差。</p>
<p>但是我也不该，也不愿意一直这么下去，我找各种各样得事情给自己，不知不觉就这么多事情了。现在已经过去了大半个月了，不想刚开始一样一想到就会好难过，但是偶尔还是会难过。我觉得生活还得继续，不能一直自暴自弃，继续走下去，也许我会遇到更好的女生，也许我会以更好的状态遇见她。我这么想着，慢慢开始忙碌起来，身体还是很疲惫，依然需要时间去习惯与适应。</p>
<p>可能十月份会非常忙吧，各种各样的事情，所以国庆也不能闲着，要抓紧时间。可能这个学期能玩游戏的时间会越来越少，但还是要坚持住。这也是我第一次参加科研训练，我对科研学习的过程也一直很感兴趣，我想好好体验一下这个过程。</p>
<p>还有PA实验和操作系统，本来操作系统是想跟着JYY老师的课程学习的，现在看来是没什么时间了。还要概率论、离散数学、密码学应用基础，这些课基本还没听过，也不知道怎么办。这个学期的408课程也是牢的没边，也不知道怎么办。</p>
<p>这么看了这个学期还真是坎坷，和我暑假时预想的一点也不一样。当然我也没想到会和她分手QAQ，不过现在她还是愿意陪我聊天的，说不定以后还有机会。事到如今只能，继续坚持下去。希望以后会越来越好，也希望我能在这段时间里慢慢获得成长。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/27/83-NJU-PA-STUDY-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/83-NJU-PA-STUDY-1/" class="post-title-link" itemprop="url">83:NJU_PA_STUDY(1)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-27 07:37:21 / 修改时间：10:54:09" itemprop="dateCreated datePublished" datetime="2025-09-27T07:37:21+08:00">2025-09-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">项目练习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0/NJUPA/" itemprop="url" rel="index"><span itemprop="name">NJUPA</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>18 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="pa1-rtfsc">PA1 RTFSC</h1>
<h2 id="框架代码">框架代码</h2>
<p>由于NEMU-PA是一个很庞大的框架系统，所以要在其基础之上开发需要对框架代码进行熟悉。所以最重要的一步应该是阅读程序的源代码。在课件中，已经给出了相关代码的简要结构说明，按照标题简单理解即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">nemu</span><br><span class="line">├── configs                    # 预先提供的一些配置文件</span><br><span class="line">├── include                    # 存放全局使用的头文件</span><br><span class="line">│   ├── common.h               # 公用的头文件</span><br><span class="line">│   ├── config                 # 配置系统生成的头文件, 用于维护配置选项更新的时间戳</span><br><span class="line">│   ├── cpu</span><br><span class="line">│   │   ├── cpu.h</span><br><span class="line">│   │   ├── decode.h           # 译码相关</span><br><span class="line">│   │   ├── difftest.h</span><br><span class="line">│   │   └── ifetch.h           # 取指相关</span><br><span class="line">│   ├── debug.h                # 一些方便调试用的宏</span><br><span class="line">│   ├── device                 # 设备相关</span><br><span class="line">│   ├── difftest-def.h</span><br><span class="line">│   ├── generated</span><br><span class="line">│   │   └── autoconf.h         # 配置系统生成的头文件, 用于根据配置信息定义相关的宏</span><br><span class="line">│   ├── isa.h                  # ISA相关</span><br><span class="line">│   ├── macro.h                # 一些方便的宏定义</span><br><span class="line">│   ├── memory                 # 访问内存相关</span><br><span class="line">│   └── utils.h</span><br><span class="line">├── Kconfig                    # 配置信息管理的规则</span><br><span class="line">├── Makefile                   # Makefile构建脚本</span><br><span class="line">├── README.md</span><br><span class="line">├── resource                   # 一些辅助资源</span><br><span class="line">├── scripts                    # Makefile构建脚本</span><br><span class="line">│   ├── build.mk</span><br><span class="line">│   ├── config.mk</span><br><span class="line">│   ├── git.mk                 # git版本控制相关</span><br><span class="line">│   └── native.mk</span><br><span class="line">├── src                        # 源文件</span><br><span class="line">│   ├── cpu</span><br><span class="line">│   │   └── cpu-exec.c         # 指令执行的主循环</span><br><span class="line">│   ├── device                 # 设备相关</span><br><span class="line">│   ├── engine</span><br><span class="line">│   │   └── interpreter        # 解释器的实现</span><br><span class="line">│   ├── filelist.mk</span><br><span class="line">│   ├── isa                    # ISA相关的实现</span><br><span class="line">│   │   ├── mips32</span><br><span class="line">│   │   ├── riscv32</span><br><span class="line">│   │   ├── riscv64</span><br><span class="line">│   │   └── x86</span><br><span class="line">│   ├── memory                 # 内存访问的实现</span><br><span class="line">│   ├── monitor</span><br><span class="line">│   │   ├── monitor.c</span><br><span class="line">│   │   └── sdb                # 简易调试器</span><br><span class="line">│   │       ├── expr.c         # 表达式求值的实现</span><br><span class="line">│   │       ├── sdb.c          # 简易调试器的命令处理</span><br><span class="line">│   │       └── watchpoint.c   # 监视点的实现</span><br><span class="line">│   ├── nemu-main.c            # 你知道的...</span><br><span class="line">│   └── utils                  # 一些公共的功能</span><br><span class="line">│       ├── log.c              # 日志文件相关</span><br><span class="line">│       ├── rand.c</span><br><span class="line">│       ├── state.c</span><br><span class="line">│       └── timer.c</span><br><span class="line">└── tools                      # 一些工具</span><br><span class="line">    ├── fixdep                 # 依赖修复, 配合配置系统进行使用</span><br><span class="line">    ├── gen-expr</span><br><span class="line">    ├── kconfig                # 配置系统</span><br><span class="line">    ├── kvm-diff</span><br><span class="line">    ├── qemu-diff</span><br><span class="line">    └── spike-diff</span><br></pre></td></tr></table></figure>
<p>为了支持不同的ISA形式。框架代码将NEMU分成两部分：ISA的相关实现和ISA无关的框架代码。其中不同的ISA被存放在<code>src/isa</code>目录下，用于提供接口，其余部分框架则是相同的实现。这里我们选择RISCV作为我们的ISA，现在我们就可以对整个框架代码进行分析了。</p>
<h2 id="配置系统和项目构建">配置系统和项目构建</h2>
<p>系统的主要配置文件存放在主目录下的<code>Kconfig</code>文件中，当我们运行<code>make memuconfig</code>时，会弹出一个可视化的编辑界面，程序会将我们的选择对应的添加到<code>include\generate\autoconf.h</code>中，用于编译时设置。从而实现对框架代码的简易配置。</p>
<p>对于更复杂的过程，涉及到<code>makefile</code>的编写，这里暂时忽略。</p>
<h2 id="准备第一个客户应用">准备第一个客户应用</h2>
<p>NEMU作为一个模拟的计算机系统，主要的功能就是运行客户程序。我们可以从头观察NEMU的项目框架，来查看，NEMU是怎么进行初始化，并且将客户应用加载到内存中运行的。</p>
<p>首先是进入<code>nemu-main.c</code>中，可以看到形如<code>CONFIG_XX</code>的宏定义字样，我们可以在<code>autoconf.h</code>中找到相关的宏定义，根据部分宏的配置，可能会编译时忽略或是开启部分功能。</p>
<p>NEMU的框架代码主要通过函数进行包装，进入<code>nemu-main.c</code>中，首先执行的是<code>init_moniter()</code>,步进程序可以看到monitor的初始化过程，对于<code>mem</code>和<code>seed</code>都是简单的设置，可以之间看源代码</p>
<p>其中<code>init_isa()</code>的代码比较特殊，也比较关键：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">void</span> <span class="title function_">restart</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="comment">/* Set the initial program counter. */</span></span><br><span class="line">  cpu.pc = RESET_VECTOR;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* The zero register is always 0. */</span></span><br><span class="line">  cpu.gpr[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">init_isa</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="comment">/* Load built-in image. */</span></span><br><span class="line">  <span class="built_in">memcpy</span>(guest_to_host(RESET_VECTOR), img, <span class="keyword">sizeof</span>(img));</span><br><span class="line">  <span class="comment">/* Initialize this virtual computer system. */</span></span><br><span class="line">  restart();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>程序首先将<code>img</code>（这里是初始程序，加载到主机的起始地址），具体的内容可以在<code>isa\risc32\init.c</code>中查看看，<code>restart()</code>的作用是将CPU复位成初始状态，这里主要是将pc置0，并将riscv的第一个寄存器设置为0，作为零寄存器。</p>
<p>通过查看<code>memory</code>目录我们可以知道，NEMU为客户计算机提供了<code>128MB</code>的物理内存，同时我们将客户程序读入到内存的固定内存位置<code>RESET_VECTOR</code>。</p>
<p>在这里我们需要分清楚主机和客户机的区别，主机就是运行NEMU的物理计算机，客户机就是在NEMU上运行的计算机程序。我们使用<code>guest_to_host()</code>和<code>host_to_guest()</code>进行主机和客户机地址的相互转换。<code>guest_to_host()</code>将我们在客户机的物理地址转换成在NEMU内存中的数组地址，<code>host_to_guest()</code>则将内存中的数组地址转换成客户机的物理地址。</p>
<p>我们可以在<code>include\memory\paddr.h</code>中找到对<code>RESET_VECTOR</code>的定义，由于这里我们没有设置<code>CONFIG_PC_RESET_OFFSET</code>所以内存的加载从<code>pmem[0]</code>开始。</p>
<p>接着程序调用<code>load_image()</code>用于向内存中加载程序，如果没有给出img参数，则NEMU使用内置的初始化程序，我们可以在<code>isa/risc32/init.c</code>中看到。</p>
<p>然后程序调用<code>welcome()</code>,我们编译运行时看到的信息就是来自这里。</p>
<h2 id="运行第一个客户运用">运行第一个客户运用</h2>
<p>在monitor完成初始化之后，<code>nemu-main.c</code>会进入下一个程序<code>engine_start</code>中的<code>sdb_mainloop()</code>，并输出提示符指示输入：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(nemu)</span><br></pre></td></tr></table></figure>
<p>在<code>src\monitor\sdb\sdb.c</code>中，程序预设了一个<code>cnd_table</code>，设置在sdb中支持的指令：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cmd_table [] = &#123;</span><br><span class="line">  &#123; <span class="string">&quot;help&quot;</span>, <span class="string">&quot;Display information about all supported commands&quot;</span>, cmd_help &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;c&quot;</span>, <span class="string">&quot;Continue the execution of the program&quot;</span>, cmd_c &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;q&quot;</span>, <span class="string">&quot;Exit NEMU&quot;</span>, cmd_q &#125;,</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* <span class="doctag">TODO:</span> Add more commands */</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>对于参数的处理和选择执行可以通过阅读<code>sdb_mainloop</code>理解，这里我们主要将注意力放到<code>cmd_c()</code>的调用函数<code>cpu_exec()</code>上，它是我们模拟器运行程序的cpu执行的核心，这里传入了一个参数<code>-1</code>但由于是<code>uint64_t</code>表示，所以实际上的数值是<code>0xFFFFFFFFFFFFFFFF</code>，即持续执行，这里我们进一步的步入追踪，最终查看到<code>exec_once()</code>，他负责将pc设置成下一条指令执行的位置。</p>
<p>现在NEMU会不断的进行执行，首先它执行的便是我们的内置程序：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">uint32_t</span> img [] = &#123;</span><br><span class="line">  <span class="number">0x00000297</span>,  <span class="comment">// auipc t0,0</span></span><br><span class="line">  <span class="number">0x00028823</span>,  <span class="comment">// sb  zero,16(t0)</span></span><br><span class="line">  <span class="number">0x0102c503</span>,  <span class="comment">// lbu a0,16(t0)</span></span><br><span class="line">  <span class="number">0x00100073</span>,  <span class="comment">// ebreak (used as nemu_trap)</span></span><br><span class="line">  <span class="number">0xdeadbeef</span>,  <span class="comment">// some data</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>在NEMU中我们将<code>ebreak</code>的语义设置成，接受a0的数据作为退出状态。同时为了检测客户程序的退出，设置了以下三种状态：</p>
<ul>
<li><code>HIT GOOD TRAP</code> - 客户程序正确地结束执行</li>
<li><code>HIT BAD TRAP</code> - 客户程序错误地结束执行</li>
<li><code>ABORT</code> - 客户程序意外终止, 并未结束执行</li>
</ul>
<p>我们在nemu中使用<code>c</code>就可以获得以下输出：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nemu: HIT GOOD TRAP at pc = 0x8000000c</span><br></pre></td></tr></table></figure>
<p>即nemu的客户程序在<code>pc = 0x8000000c</code>处成功退出。退出<code>cpu_exec()</code>之后，我们再使用<code>q</code>退出nemu程序。</p>
<h2 id="优美的退出">优美的退出</h2>
<p>我们运行NEMU后直接使用<code>q</code>会产生报错：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ylin@Ylin:~/ics2025/nemu$ make run</span><br><span class="line">/home/ylin/ics2025/nemu/build/riscv32-nemu-interpreter --log=/home/ylin/ics2025/nemu/build/nemu-log.txt</span><br><span class="line">[src/utils/log.c:30 init_log] Log is written to /home/ylin/ics2025/nemu/build/nemu-log.txt</span><br><span class="line">[src/memory/paddr.c:50 init_mem] physical memory area [0x80000000, 0x87ffffff]</span><br><span class="line">[src/monitor/monitor.c:51 load_img] No image is given. Use the default build-in image.</span><br><span class="line">[src/monitor/monitor.c:28 welcome] Trace: ON</span><br><span class="line">[src/monitor/monitor.c:29 welcome] If trace is enabled, a log file will be generated to record the trace. This may lead to a large log file. If it is not necessary, you can disable it in menuconfig</span><br><span class="line">[src/monitor/monitor.c:32 welcome] Build time: 20:11:21, Sep 26 2025</span><br><span class="line">Welcome to riscv32-NEMU!</span><br><span class="line">For help, type &quot;help&quot;</span><br><span class="line">(nemu) q</span><br><span class="line">make: *** [/home/ylin/ics2025/nemu/scripts/native.mk:38: run] Error 1</span><br></pre></td></tr></table></figure>
<p>我们需要找出原因并解决这个问题。</p>
<p>这是<code>cmd_q</code>的源代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">cmd_q</span><span class="params">(<span class="type">char</span> *args)</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们输入q后会因为<code>sdb_mainloop</code>的判断逻辑退出到<code>nemu_main</code>执行<code>is_exit_status_bad()</code>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">is_exit_status_bad</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">int</span> good = (nemu_state.state == NEMU_END &amp;&amp; nemu_state.halt_ret == <span class="number">0</span>) ||</span><br><span class="line">    (nemu_state.state == NEMU_QUIT);</span><br><span class="line">  <span class="keyword">return</span> !good;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>程序会检测nemu的状态而决定以什么情况退出，我们之前的报错则是因为，我们没有为NEMU设置任何状态，NEMU以默认状态退出，因此返回错误。想要优雅的退出，我们只需要再退出前设置好NEMU的状态。因此我们对<code>cmd_q()</code>函数进行重写</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">cmd_q</span><span class="params">(<span class="type">char</span> *args)</span> &#123;</span><br><span class="line">  nemu_state.state = NEMU_QUIT;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/26/82-NJU-PA-STUDY-0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/26/82-NJU-PA-STUDY-0/" class="post-title-link" itemprop="url">83:NJU_PA_STUDY(0)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-26 19:33:39 / 修改时间：20:21:55" itemprop="dateCreated datePublished" datetime="2025-09-26T19:33:39+08:00">2025-09-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">项目练习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%A1%B9%E7%9B%AE%E7%BB%83%E4%B9%A0/NJUPA/" itemprop="url" rel="index"><span itemprop="name">NJUPA</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这个学期抽取一部分课余时间用来完成NJU的PA实验，争取能做多少做多少。</p>
<p>第一篇先从环境配置开始，这里我使用的程序环境是WSL+Vscode。</p>
<h1 id="pa0-getting-source-code-for-pas">PA0 Getting Source Code For
PAs</h1>
<h2 id="拉取源码">拉取源码</h2>
<p>由于我的git环境已经配置好了，所以直接在我的主目录下面<code>clone</code>文件内容就行了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone -b 2025 git@github.com:NJU-ProjectN/ics-pa.git ics2025</span><br></pre></td></tr></table></figure>
<p>然后<code>cd ics2025</code>，以后这个目录就是项目的工程目录了。由于我不需要追踪进度，所以就不用提交信息啥的，直接进行初始化就行了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git branch -m master</span><br><span class="line">bash init.sh nemu</span><br></pre></td></tr></table></figure>
<p>为了方便子项目的编译，<code>init.sh</code>会向环境变量中添加部分环境变量，这里可以通过</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo $NEMU_HOME</span><br><span class="line">cd $NEMU_HOME</span><br></pre></td></tr></table></figure>
<p>来验证环境变量是否正确设置，如果没有的话，使用<code>source /.bashrc</code>来激活cd</p>
<h2 id="分支创建">分支创建</h2>
<p>现在我们需要创建一个新的分支，作为<code>pa0</code>的工作分支，之后每个阶段的PA都会单独设置一个分支，再进行合并，我们可以使用<code>git branch</code>来查看现有分支，然后我们：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">❯ git checkout -b pa0</span><br><span class="line">Switched to a new branch &#x27;pa0&#x27;</span><br><span class="line">❯ git branch</span><br><span class="line">  master</span><br><span class="line">* pa0</span><br></pre></td></tr></table></figure>
<p>从而创建并跳转到一个新的分支上，然后我们修改一下makefile（因为我们不需要跟踪进度）</p>
<h2 id="编译运行nemu">编译运行NEMU</h2>
<p>通过<code>make menuconfig</code>进行文件的编译：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">ylin@Ylin:~/ics2025/nemu$ make menuconfig</span><br><span class="line">/home/ylin/ics2025/nemu/scripts/config.mk:20: Warning: .config does not exist!</span><br><span class="line">/home/ylin/ics2025/nemu/scripts/config.mk:21: To build the project, first run &#x27;make menuconfig&#x27;.</span><br><span class="line">+ CC confdata.c</span><br><span class="line">+ CC expr.c</span><br><span class="line">+ CC preprocess.c</span><br><span class="line">+ CC symbol.c</span><br><span class="line">+ CC util.c</span><br><span class="line">+ YACC build/parser.tab.h</span><br><span class="line">+ LEX build/lexer.lex.c</span><br><span class="line">+ CC build/lexer.lex.c</span><br><span class="line">+ CC build/parser.tab.c</span><br><span class="line">+ CC mconf.c</span><br><span class="line">+ CC lxdialog/inputbox.c</span><br><span class="line">+ CC lxdialog/yesno.c</span><br><span class="line">+ CC lxdialog/textbox.c</span><br><span class="line">+ CC lxdialog/checklist.c</span><br><span class="line">+ CC lxdialog/util.c</span><br><span class="line">+ CC lxdialog/menubox.c</span><br><span class="line">+ LD /home/ylin/ics2025/nemu/tools/kconfig/build/mconf</span><br><span class="line">+ CC confdata.c</span><br><span class="line">+ CC expr.c</span><br><span class="line">+ CC preprocess.c</span><br><span class="line">+ CC symbol.c</span><br><span class="line">+ CC util.c</span><br><span class="line">+ CC build/lexer.lex.c</span><br><span class="line">+ CC build/parser.tab.c</span><br><span class="line">+ CC conf.c</span><br><span class="line">+ LD /home/ylin/ics2025/nemu/tools/kconfig/build/conf</span><br><span class="line">+ CC fixdep.c</span><br><span class="line">+ LD /home/ylin/ics2025/nemu/tools/fixdep/build/fixdep</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">*** End of the configuration.</span><br><span class="line">*** Execute &#x27;make&#x27; to start the build or try &#x27;make help&#x27;.</span><br></pre></td></tr></table></figure>
<p>实际上这是再设置<code>make</code>的配置文件，会跳出一个可视化界面，按照自己的需求选择即可，这里我选择了DEBUG的信息。第一次编译的时候有报错，缺少了几个程序，安装即可正常运行。然后<code>make</code>即可。如果编译有误，可以通过<code>make clean</code>删除内容再进行编译。</p>
<p>现在可以通过<code>make run</code>来运行这个项目，可以得到：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ylin@Ylin:~/ics2025/nemu$ make run</span><br><span class="line">/home/ylin/ics2025/nemu/build/riscv32-nemu-interpreter --log=/home/ylin/ics2025/nemu/build/nemu-log.txt</span><br><span class="line">[src/utils/log.c:30 init_log] Log is written to /home/ylin/ics2025/nemu/build/nemu-log.txt</span><br><span class="line">[src/memory/paddr.c:50 init_mem] physical memory area [0x80000000, 0x87ffffff]</span><br><span class="line">[src/monitor/monitor.c:51 load_img] No image is given. Use the default build-in image.</span><br><span class="line">[src/monitor/monitor.c:28 welcome] Trace: ON</span><br><span class="line">[src/monitor/monitor.c:29 welcome] If trace is enabled, a log file will be generated to record the trace. This may lead to a large log file. If it is not necessary, you can disable it in menuconfig</span><br><span class="line">[src/monitor/monitor.c:32 welcome] Build time: 20:05:51, Sep 26 2025</span><br><span class="line">Welcome to riscv32-NEMU!</span><br><span class="line">For help, type &quot;help&quot;</span><br><span class="line">[src/monitor/monitor.c:35 welcome] Exercise: Please remove me in the source code and compile NEMU again.</span><br><span class="line">riscv32-nemu-interpreter: src/monitor/monitor.c:36: welcome: Assertion `0&#x27; failed.</span><br><span class="line">make: *** [/home/ylin/ics2025/nemu/scripts/native.mk:38: run] Aborted (core dumped)</span><br></pre></td></tr></table></figure>
<p>这里可以看到一段报错<code>riscv32-nemu-interpreter: src/monitor/monitor.c:36: welcome: Assertion</code>0’
failed.`，这个是实验的第一个简单测试，我们到源码处，注释掉指定的错误语句即可：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">void</span> <span class="title function_">welcome</span><span class="params">()</span> &#123;</span><br><span class="line">  Log(<span class="string">&quot;Trace: %s&quot;</span>, MUXDEF(CONFIG_TRACE, ANSI_FMT(<span class="string">&quot;ON&quot;</span>, ANSI_FG_GREEN), ANSI_FMT(<span class="string">&quot;OFF&quot;</span>, ANSI_FG_RED)));</span><br><span class="line">  IFDEF(CONFIG_TRACE, Log(<span class="string">&quot;If trace is enabled, a log file will be generated &quot;</span></span><br><span class="line">        <span class="string">&quot;to record the trace. This may lead to a large log file. &quot;</span></span><br><span class="line">        <span class="string">&quot;If it is not necessary, you can disable it in menuconfig&quot;</span>));</span><br><span class="line">  Log(<span class="string">&quot;Build time: %s, %s&quot;</span>, __TIME__, __DATE__);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Welcome to %s-NEMU!\n&quot;</span>, ANSI_FMT(str(__GUEST_ISA__), ANSI_FG_YELLOW ANSI_BG_RED));</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;For help, type \&quot;help\&quot;\n&quot;</span>);</span><br><span class="line"> <span class="comment">// Log(&quot;Exercise: Please remove me in the source code and compile NEMU again.&quot;);</span></span><br><span class="line"> <span class="comment">// assert(0);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里设置了一个断言，让我们再运行时必定停止在这里。</p>
<p>现在我们可以正常的使用我们的程序了，同时我们也可以使用<code>make gdb</code>来进行对NEMU的调试。</p>
<h1 id="pa1-开天辟地的篇章">PA1 开天辟地的篇章</h1>
<p>计算机的本质就是状态机，从CPU出发，我们可以简单的将CPU的行为总结为</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">  从PC指示的存储器位置取出指令;</span><br><span class="line">  执行指令;</span><br><span class="line">  更新PC;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们的计算机实际上是从一个初始状态开始（就是硬件所指定的通电后的复位状态），计算机有一个程序计数器，始终指向下一段要执行的指令地址，当我们从地址取出要执行的指令，当我们对指令进行执行。程序的状态就发生了改变，我们称之为状态的迁移。</p>
<p>所以计算机的一切行为对我们而言都是可溯源的，我们只需要理解状态机的规则，与进行的动作，我们就可以还原任意时刻计算机内存的状态。我们就可以知道每一个动作背后的原理和现象。</p>
<p>PA实验则是通过对NEMU的设计，还原和理解真实计算机的各种功能和实现。所以对于计算机系统的学习是十分有帮助的。希望自己能坚持做下去吧。最近还挺忙的，但还是要加油。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/30/81-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/30/81-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98-3/" class="post-title-link" itemprop="url">81:虚拟内存(3)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-30 09:11:03" itemprop="dateCreated datePublished" datetime="2025-08-30T09:11:03+08:00">2025-08-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-18 00:09:53" itemprop="dateModified" datetime="2025-09-18T00:09:53+08:00">2025-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">计算机科学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/" itemprop="url" rel="index"><span itemprop="name">虚拟内存</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一篇中我们研究了虚拟内存在理想状态下的工作模式，现在我们要结合实际案例，来进一步了解虚拟内存在具体环境下是怎么工作的：</p>
<h2 id="intel-core7linux内存系统">Intel Core7/Linux内存系统</h2>
<h3 id="core-i7地址翻译">Core i7地址翻译</h3>
<p>我们的系统是一个运行Linux的Intel Core
i7。下图是其处理器的一个封装结构：</p>
<figure>
<img src="https://s2.loli.net/2025/09/01/rs3UFV2QJAC6Nxc.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>包括四个核，一个大的所有核共享的L3高速缓存，以及一个DDR3内存控制器。每个核中都有L1L2的高速缓存，还有TLB条目缓存。其中L1、L2、L3高速缓存都是物理寻址的，块大小为64字节。L1和L2是8路组相联，L3是16路组相联。页大小可以在启动时配置为4KB或4MB大小。这里Linux使用的是4KB的页。</p>
<p>下图则展示了Core i7地址翻译的概况：</p>
<figure>
<img src="https://s2.loli.net/2025/09/01/twyMrV1jNqslhf4.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>由于不同层级的页表中存储的条目不同，所以对于第一级、第二级、第三级的条目格式和第四级的条目格式。其地址字段结构略有不同：</p>
<p>对于第一级、第二级、第三级的条目格式。当P=1时（Linux中总是P=1），地址字段包含一个40位的物理页号（PPN），它指向下一级页表的开始处。（注：由于物理页面是4KB对齐的，所以起始地址应该是4096的倍数）：</p>
<figure>
<img src="https://s2.loli.net/2025/09/01/sBxqL1nA3oyClkv.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>对于第四级页表中条目的格式。当P=1，地址字段包括一个40位的PPN，指向物理内存中某一页的基地址。同样的，需要4KB对齐：</p>
<figure>
<img src="https://s2.loli.net/2025/09/01/j98GbwsZrY7km1J.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们可以看到，PTE有三个权限位，用来控制对页的访问：</p>
<ul>
<li><code>R/W</code>位确定页的内容是可读的还是只读的</li>
<li><code>U/S</code>位确定是否能在用户模式中访问该页，从而保护内核中的代码和数据不被用户程序访问</li>
<li><code>XD</code>位用来禁止从某些内存页取指令。通过限制只能执行只读代码段，从而避免溢出攻击</li>
</ul>
<p>下图则反映了通过四级页表将虚拟地址翻译成物理地址的过程：</p>
<figure>
<img src="https://s2.loli.net/2025/09/01/E6mqlPLi8I2FT4j.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>通常情况下，我们将地址翻译的分为两步：</p>
<ol type="1">
<li>MMU将虚拟地址翻译成物理地址</li>
<li>将物理地址传送到L1高速缓存</li>
</ol>
<p>然而，实际的硬件实现优化了地址翻译的过程，允许这两个步骤一定程度上的重叠进行。例如Core
i7系统上的一个虚拟地址有12位的VPO，这些位和相应的物理地址的PPO相同。且有八路相联的、64个组和大小为63字节的缓存块的物理寻址的L1高速缓存。</p>
<p>因此每个物理地址有6个缓存偏移位和6个索引位。这12个位刚好和VPO相对应。当CPU翻译一个虚拟地址的时候，它发送VPN到MMU，发送VPO到L1。当MMU查找PTE的时候，L1高速缓存正在利用VPO位查找相应的组合块偏移，读出组中的8个标记的数据字。当MMU得到PPN时，缓存可以直接对这8个标记进行匹配。这样就极大程度的优化了地址翻译的过程。</p>
<h3 id="linux虚拟内存系统">Linux虚拟内存系统</h3>
<p>我们现在需要对LInux的虚拟内存系统做一个简单的描述，以能够大致的了解一个实际的操作系统是怎么组织虚拟内存，并处理缺页的。</p>
<p>我们知道linux为每个进程维护了一个单独的虚拟地址空间，如下图所示：</p>
<figure>
<img src="https://s2.loli.net/2025/09/01/4M9ksL18UH5himO.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>在此之前我们从来没有讨论过，内核部分的虚拟内存，这一部分位于用户栈之上，现在我们需要进一步的去了解它。</p>
<p>内核虚拟内存包含内核中的代码和数据结构。内核虚拟内存中的某些区域被映射到所有进程共享的物理页面。例如，每个进程都共享内核的代码和数据结构。同时，内核将一片连续的虚拟地址空间映射到一片相同大小的物理地址空间，从而实现对虚拟内存的线性直接映射。这样对指定虚拟地址空间的访问，就可以通过固定的偏移映射来进行访问，而无需页表查找的模式。</p>
<p>内核虚拟内存的的其他区域则存储着每个进程都不相同的数据。如页表、内核在进程的上下文中执行代码时所用的栈，以及记录虚拟地址空间当前组织的各种数据结构。</p>
<h4 id="虚拟内存区域">虚拟内存区域</h4>
<p>Linux将虚拟内存组织成一些区域（也叫做段）。区域实际上就是一片连续的已分配的虚拟内存页，往往不同的区域负责不同的内容。例如代码段、数据段、堆、共享库段、用户栈都是不同的区域。大大小小的区域有不同的意义，所以每个存在的虚拟页面一定是属于某个区域的。区域的概念使得虚拟地址空间之间可以有间隙，且不用记录（不用分配）那些不被使用的虚拟内核空间。</p>
<p>下图就是一个用于记录进程中虚拟内存区域的内核数据结构：</p>
<figure>
<img src="https://s2.loli.net/2025/09/01/DcdnCMRAbKQrma2.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>内核为系统中的每个进程维护了一个独立的任务结构(task_struct)，任务结构中的元素包含或者指向内核运行这个进程所需要的所有信息（PID，指向用户栈的指针，可执行目标文件的名字，程序计数器…）</p>
<p><code>task_struct</code>中的一个条目指向mm_struct，它描述了虚拟内存的当前状态。其中有两个我们感兴趣的字段：</p>
<ul>
<li>pgd：指向第一级页表的基址，运行时该值被存放在CR3控制寄存器中</li>
<li>mmap指向一个<code>vm_area_structs</code>（区域结构）的链表，其中每个<code>vm_area_structs</code>都描述了当前虚拟地址空间的一个区域。一个具体的区域结构包含以下字段：
<ul>
<li>vm_start：指向这个区域的起始处</li>
<li>vm_end：指向这个区域的结束处</li>
<li>vm_prot：描述这个区域内包含的所有页的读写许可权限</li>
<li>vm_flags：描述这个区域内的页面是与其他进程共享的，还是私有的（以及一些其他信息）</li>
<li>vm_next：指向链表中的下一个区域结构</li>
</ul></li>
</ul>
<h4 id="linux缺页异常处理">Linux缺页异常处理</h4>
<p>假设MMU在翻译某个虚拟地址A的时候，触发了一个缺页。这个异常会触发缺页处理程序，处理程序会执行下面的步骤：</p>
<ul>
<li><p><strong>虚拟地址A是否合法？</strong></p>
<p>换句话说就是A是否在某个区域结构定义的区域内吗？缺页处理程序会搜索区域结构的链表，把和每个区域结构中的<code>vm_start</code>和<code>vm_end</code>作比较。如果该指令不合法，就会触发一个段错误，从而终止这个进程。既图中”1”</p></li>
<li><p><strong>试图进行的内存访问是否合法？</strong></p>
<p>换句话说就是进程是否有读、写或者执行这个区域内页面的权限？如果试图进行的操作是违法的，那么缺页处理程序就会触发一个保护异常，从而终止这个进程。既图中”2”</p></li>
<li><p><strong>正常缺页</strong></p>
<p>排除以上的可能，那么这个缺页就是对合法的虚拟地址进行合法的操作造成的。处理程序会选择有一个牺牲页面，如果这个这个牺牲页面被修改过，就将其交换出去，换入新的页面并更新页表。当缺页处理程序返回时，重新启动引起缺页的指令，这次便可以正常的进行。既图中”3”</p></li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/09/01/PWfUsgJ1Gbwu395.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="内存映射">内存映射</h2>
<p>Linux通过将一个虚拟内存区域和一个磁盘上的对象关联起来，以初始化这个虚拟内存区域的内容，这个过程，我们就称之为<strong>内存映射</strong>。虚拟内存区域可以映射到两种类型的对象中的一种：</p>
<ul>
<li><p><strong>Linux文件系统中的普通文件</strong></p>
<p>一个区域可以映射到一个普通的磁盘文件的连续部分。文件区被分成页大小的片，每一片的包含一个虚拟页面的初始内容。由于系统按需进行页面调度，所以这些虚拟页面美亚由实际交换进入物理内存，直到CPU第一次引用这个页面时，才会调入。对于区域大小大于文件的部分，用0填充余下部分。</p></li>
<li><p><strong>匿名文件</strong></p>
<p>一个区域也可以映射到一个匿名文件，匿名文件是由内核创建的，内容全部为二进制0填充。CPU第一次引用这样一个区域内的虚拟页面时，内核在物理内存中查找，如果有空闲的物理页框，就用二进制0填充初始化，再建立虚拟页到物理页的映射；如果没有，就挑选一个合适的牺牲页，如果这个页面被修改过，就将其内容写回交换空间，并用二进制0覆盖这个物理页框，建立新的映射关系</p></li>
</ul>
<p>这里我们还要知道，一旦一个虚拟页面被初始化了，它就在一个由内核维护的专门的交换空间(swap
file)之间换来换去。它相当于一个物理内存的页面的一个临时存储处，被替换的牺牲页被暂时的保存在这里。因此，交换空间限制着当前运行的进程能够分配的虚拟页面的总数。</p>
<h3 id="共享对象">共享对象</h3>
<p>本章暂时结束。之后再回头搞一下，接下来要做一些实验和一些项目了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/28/80-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/28/80-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98-2/" class="post-title-link" itemprop="url">80:虚拟内存(2)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-28 12:30:27" itemprop="dateCreated datePublished" datetime="2025-08-28T12:30:27+08:00">2025-08-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-29 13:44:39" itemprop="dateModified" datetime="2025-08-29T13:44:39+08:00">2025-08-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">计算机科学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/" itemprop="url" rel="index"><span itemprop="name">虚拟内存</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我们已经学习了虚拟内存的作用和虚拟内存的基本使用过程，为了进一步的深入的理解虚拟内存的机制，我们需要要深入理解虚拟内存的基本原理。</p>
<h2 id="地址翻译">地址翻译</h2>
<p>现在我们将从底层除出发，理解硬件在虚拟内存中的角色。为了简化之后的说明，这里提前展示我们所要用到的符号以参考：</p>
<figure>
<img src="https://s2.loli.net/2025/08/28/w1YmzSBy2MpVWer.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>实际上，地址翻译就是一个N元素的虚拟地址空间(VAS)中的元素和一个M元素的物理地址空间(PAS)中元素的映射：
<span class="math display">$$
\begin{align*}
MAP&amp;:VAS \to PAS \cup \varnothing \\
MAP(A) &amp;= \begin{cases}
A' \text{ 如果虚拟地址A处的数据在PAS的物理地址A'处} \\
\varnothing \text{ 如果虚拟地址A处的数据不在物理内存中}
\end{cases}
\end{align*}
$$</span> 下图演示了MMU如何利用页表来实现这种映射：</p>
<figure>
<img src="https://s2.loli.net/2025/08/28/tpHq1JOUC3rojW6.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>控制寄存器——页表基址寄存器（PTBR）指向当前页表。对于n位的虚拟地址，包含两个部分：</p>
<ul>
<li>一个p位的虚拟页面偏移(VPO)</li>
<li>一个n-p位的虚拟页号(VPN)，作为页表的索引</li>
</ul>
<p>MMU通过VPN选择对应的PTE，同时PTE中的内容实际上就是物理页号(PPN)，这样就可以通过虚拟页号访问到对应的物理页。至于偏移量，由于虚拟页和物理页的大小相同，所以虚拟页偏移量和物理页偏移量(PPO)是是相同的。</p>
<p>现在我们可以分析CPU硬件的执行步骤了：</p>
<p>当页面命中时：</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/WqiRlNCVASMmg6J.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol type="1">
<li>处理器生成一个虚拟地址VA并发送到MMU中</li>
<li>MMU生成PTE地址(将VA拆分成VPN和VPO，VPN作为PTE地址)，从内存中请求它</li>
<li>内存向MMU返回PTE</li>
<li>MMU构造物理地址(返回的PTE就是PPN，物理地址=PPN+(PPO=VPO))发送给内存</li>
<li>内存通过物理地址找到数据字节返回给CPU</li>
</ol>
<p>当缺页时：</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/vKj3YZi5JsBTfwM.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol type="1">
<li>前三步同上</li>
<li>返回的PTE有效位为0，MMU触发异常，CPU控制传递到内核中的缺页异常处理程序</li>
<li>缺页处理程序调入新的页面，并更新内存中的PTE</li>
<li>缺页处理程序返回原来的进程，再次执行导致缺页的指令。此时页面命中</li>
</ol>
<h3 id="结合高速缓存和虚拟内存">结合高速缓存和虚拟内存</h3>
<p>我们的系统一般既有虚拟内存的机制又有SRAM高速缓存，那么对于SRAM高速缓存，我们是应该使用物理地址访问还是虚拟地址访问呢？下图是一个将其结合起来的方案：</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/2tZcq38gRUAPN9E.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们使用物理寻址的方案，先将VA转换为PTEA然后在L1中检索，如果命中就直接返回PTE，未命中则再取一次。拿到PTE后构造出PA再对L1进行检索，如果命中就返回PA，未命中就再取一次。L1的存在可以帮我们节省数据传送的时间。页表条目本质上也是数据字，所以也可以被缓存在L1中。</p>
<h3 id="利用tlb加速地址翻译">利用TLB加速地址翻译</h3>
<p>CPU每次产生一个虚拟地址都需要查阅一个PTE，以便于将虚拟地址进行翻译。每次从内存取数据都需要花费几十到几百的周期，为了解决这个问题，我们引入了高速缓存，在命中的情况下，将开销降到了1到2周期。为了尽可能的减少开销，我们在MMU中引入了一个关于PTE的小缓存，即翻译后备缓冲器(TLB)</p>
<p>TLB是一个小的缓存，其每一行都保存有一个由单个PTE组成的块。其结构如下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/ldcVmZgx9f53LaA.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>用于组选择和行匹配的索引和标记字段都是从VPN提取出来的。如果TLB有<code>T=2^t</code>个组，那么TLB索引(TLBI)是由VPN的t个最低位组成的，而TLB标记(TLBT)是由VPN中剩余的位组成的（用来行匹配）。</p>
<p>下图展示了TLB的工作过程：</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/hkUIDiX5fcmr1tG.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>当TLB命中时：</p>
<ul>
<li>CPU产生VA</li>
<li>MMU从TLB中取出相应的PTE</li>
<li>MMU构造物理地址，发送到缓存</li>
<li>缓存返回数据字到CPU</li>
</ul>
<p>当不命中时，MMU必须从L1中取出相应的PTE。新取出的PTE会覆盖TLB中的一个条目。</p>
<p>由于所有的地址翻译步骤都是在MMU中完成的，所以速度很快。</p>
<h3 id="多级页表">多级页表</h3>
<p>到此为止，我们一直假设系统只使用一个单独的页表来进行地址翻译。如果我们有一个32位的地址空间、4KB的页面和一个4字节的PTE，那么即使我们只使用虚拟空间中很小的一部分，我们也需要一个4MB的页表驻留在内存中。对于64位的地址空间，这个问题更加明显，我们甚至需要4PB的页表常驻内存，这荒谬。为了解决这个问题，我们引入多级页表，用过层次结构来压缩。</p>
<p>我们以下图为例：</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/B1f5t2nGwiJ9a6D.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>对于32位的地址空间，我们有4GB的地址空间，空间被分为4KB的页，每个页有一个4字节的页表条目。假设此时，虚拟空间有以下形式：内存的前2K个页面被分配给了代码和数据，接下来的6K个页面没有被分配，再接下里的1K个页面中有1023个未分配的页和一个分配作为栈的页面。</p>
<p>我们用一级页表中的每个PTE负责映射虚拟空间中一个4MB的片，这里的每一片都是由1024个连续的页面组成的。所以我们只需要1024个一级页表条目就可以指向4G大小的片。</p>
<p>如果片i中的每个页面都没有被分配，那么对应的一级页表条目i就是空的。反之，如果片i中至少有一个页是分配了的，那么一级PTEi就需要指向有一个二级页表的基址。每个二级页表的结构和一级页表都是一样的。</p>
<p>这种方法减少了内存的需求：</p>
<ul>
<li>如果一级页表中的一个PTE是空的，那么对应的二级页表都不需要要存在，这样极大的节省了内存空间，因为虚拟地址空间大部分时候都是未分配的</li>
<li>只有一级页表才需要总是在主存中，VM系统在需要时创建、页面调入或调出二级表，进一步减少了主存的压力，只有常用的二级表才需要缓存在主存中</li>
</ul>
<p>对于一个k级的页表层次结构的地址翻译。虚拟地址被分隔成k个VPN和一个VPO:</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/eEwhlUGs7bfCap9.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>每个VPNi都是一个到第i个级页表的索引，第j级的每个页表中都指向第j+1级的某个页表的基址。第k级页表中的每个PTE包含某个物理页面的PPN，或一个磁盘块的地址。最终构造物理地址。</p>
<p>为了构造物理地址，MMU必须访问k个PTE。看上去开销很大，实际上TLB在这里会起到重要的作用，通过将不同层次上页表的PTE缓存起来，效率很不错。</p>
<h3 id="端到端的地址翻译">端到端的地址翻译</h3>
<p>讲了很多原理和过程，只有自己动手实践才是最真实的，我们用下面的的环境——在一个有TLB和L1
d-cache的系统上：</p>
<ul>
<li>内存是按字节寻址的</li>
<li>内存访问是针对1字节的字</li>
<li>虚拟地址是14为长的(n=14)</li>
<li>物理地址是12位长的(m=12)</li>
<li>页面大小为64字节(P=64)</li>
<li>TLB是四路组相联的，共有16个条目</li>
<li>L1 d-cache是物理寻址、直接映射的，行大小4个字节16个组</li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/08/29/gFIJBoKmOy6QkHt.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>然后是对于TLB和高速缓存，访问这些设备时，我们通过以下方法对位进行划分：</p>
<ul>
<li><strong>TLB</strong>
因为TLB有四个组，所以VPN的低2位用来做TLBI，高6位作为TLBT</li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/08/29/DspQodRxmIbCegv.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ul>
<li><strong>页表</strong>
这是一个单级设计，一共有256个页面，这里我们只关注前16个。为了方便，我们直接令VPN来标识PTE</li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/08/29/VwlgKTdEzW3M4GX.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ul>
<li><strong>高速缓存</strong>
每个块都是4字节，所以用低2位作为块偏移(CO)。因为有16组，所以接下来的4位做组索引(CI)。剩下的6位做标记(CT)</li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/08/29/CLomsbBcd8OekDW.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>现在，假设CPU执行一条读地址<code>0x3d4</code>处字节的加载指令会发生什么？</p>
<p>首先MMU会对VA进行解析，并在TLB中查找是否有缓存的PTE:</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/Bt3SX2Yd567NpIx.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们根据索引找到，获取了TLB中缓存的PPN<code>0x0D</code>，从而和VPO构造出物理地址<code>0x0354</code>。现在MMU将物理地址发送到缓存。我们对地址进行解析，查看L1是否缓存了我们需要的数据：</p>
<figure>
<img src="https://s2.loli.net/2025/08/29/4uHY1ncCbmp8LZP.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们根据标记，行索引，块偏移，得到了数据<code>0x36</code>，并将其返回到MMU，随后MMU又将数据返回到CPU。至此，我们就完成了一次虚拟内存的使用。</p>
<p>当然以上的演示，都是理想状态下简化的情况。实际上我们可能会遇到不命中的问题。如果TLB不命中，那么MMU必须从页表PTE中取出PPN。如果得到的PTE是无效的，那么就产生缺页，那么就调用缺页异常处理程序；如果是有效的，但是缓存不命中。那么则又需要进行取用…</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/27/79-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/27/79-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98-1/" class="post-title-link" itemprop="url">79:虚拟内存(1)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-27 16:50:33" itemprop="dateCreated datePublished" datetime="2025-08-27T16:50:33+08:00">2025-08-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-28 11:14:28" itemprop="dateModified" datetime="2025-08-28T11:14:28+08:00">2025-08-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">计算机科学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/" itemprop="url" rel="index"><span itemprop="name">虚拟内存</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>总是听到诸如页表，分页机制一类的词汇，听起来让人感到十分的复杂。实际上这些都是涉及到虚拟内存相关的只是。一直以来对这一部分的知识都是望而生畏，现在好好来理解一下它：</p>
<h2 id="物理和虚拟寻址">物理和虚拟寻址</h2>
<p>计算机系统的主存被组织成一个由M个连续的字节大小的单元组成的数组。每个字节都有一个唯一的<strong>物理地址</strong>，物理地址从0开始依次设置。这是最简单自然的结构，我们把CPU以这个结构用来访问地址的方式称为<strong>物理寻址</strong>。早期的计算机和一些数字处理器和嵌入式设备仍然使用这种方式，</p>
<p>现代处理器则是使用一种被称为<strong>虚拟寻址</strong>的方式来进行寻址。使用虚拟寻址，CPU会生成一个虚拟地址(VA)来访问主存，VA被送到内存之前会先转换为适当的物理地址，这个过程叫做<strong>地址翻译</strong>。有一个专门的硬件单元来完成这个任务——<strong>内存管理单元</strong>(MMU)。原理是利用存放在主存中的查询表来动态翻译虚拟地址，这个表中的内容由操作系统管理。</p>
<h2 id="地址空间">地址空间</h2>
<p>地址空间是非负整数地址的有序集合。如果地址空间中的整数时连续的，我们说它是一个<strong>线性地址空间</strong>，我们假定之后用到的所有地址空间都是线性的。在一个带虚拟内存的系统中，CPU从有一个有N=2^n个地址的地址空间中生成虚拟地址，则这个地址口空间称为<strong>虚拟地址空间</strong>。</p>
<p>一个地址空间的大小是由表示最大地址所需要的位数来描述的。对于一个包含N=2^n个地址的虚拟地址空间，我们可以将其叫做为一个n位的地址空间。一个系统还带有一个物理地址空间，对应于系统中的物理内存的M个字节。</p>
<p>地址空间的概念实际上区分了两个概念：</p>
<ul>
<li>数据对象(字节)</li>
<li>属性(地址)</li>
</ul>
<p>所以我们应该意识到数据对象实际上可以有多个地址，只不过每一个地址都选自一个不同的地址空间，这就是我们虚拟空间所用到的概念。例如主存中的每一个字节都有有一个选自虚拟地址空间的虚拟地址和一个选自物理地址空间的物理地址。</p>
<h2 id="虚拟内存作为缓存工具">虚拟内存作为缓存工具</h2>
<p>虚拟地址实际上就是一个由存放在磁盘上的N个连续的字节大小的单元组成数组，每一个字节都有着一个对应的虚拟地址，作为对这个数组的索引。磁盘上数组的内容被缓存在主存中。和其他的缓存一样，磁盘上的数据被分隔成块，这些快作为磁盘和主存之间的传输单元。</p>
<p>VM系统将虚拟内存分割为<strong>虚拟页</strong>的大小固定的块，每个虚拟页的大小为<code>P=2^p</code>字节。物理内存也被分隔为同样大小的<strong>物理页</strong>，也称页帧。</p>
<p>在任意时刻，虚拟页处于以下中的一种状态：</p>
<ul>
<li><strong>未分配：</strong>VM系统还没有创建的页。未分配的块不会有任何数据关联，不占用磁盘空间</li>
<li><strong>已缓存：</strong>当前已缓存在物理内存中的已分配页</li>
<li><strong>已分配：</strong>未缓存在物理内存中的已分配页</li>
</ul>
<h3 id="页表">页表</h3>
<p>在这里需要用到DRAM和SRAM的关系，可以查看存储器层次架构进行回顾。</p>
<p>和任何缓存一样，VM系统需要要一种方法来判定一个虚拟页是否被缓存在物理内存中的某个地方。如果命中，怎么确定这个虚拟页被存放在那个物理页中。如果不命中，系统需要判断虚拟页存放在磁盘的哪个位置，并在物理内存中选择有一个牺牲页，将虚拟页复制到这里，替换这个牺牲页。</p>
<p>通过操作系统软件、MMU和存放在物理内存中的页表，软硬联合，从而将虚拟页映射到物理页。每次地址翻译硬件将一个虚拟地址转换为一个物理地址的时候，都会读取页表。操作系统则负责维护页表中的内容，在磁盘和主存间来回传送页。页表的结构大致如下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/28/nIDLxybfH6TXhq4.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们认识一下页表的基本数据结构，页表就是一个页表条目(PTE)的数组。虚拟地址空间中的每个页在页表中一个固定的偏移量处都有一个PTE（也就是说PTE的大小是固定的）。根据上面的这个简化模型，每个PTE实际上是由一个有效位和一个n位地址字段组成的：</p>
<ul>
<li>有效位表明该虚拟也当前是否被缓存在主存中。</li>
<li>n位地址字段，在有效位被设置的情况下，表示主存中相应的物理页的起始位置，这个物理页中缓存了该虚拟页。如果没有设置有效位，那么这个地址指向该虚拟页在磁盘中的起始位置。</li>
</ul>
<p>在上图中我们就可以看到虚拟页的三种状态：未分配、未缓存、已缓存。</p>
<h3 id="页命中">页命中</h3>
<p>当CPU想要读取包含在VP2中的虚拟内存的一个字时，地址翻译硬件会将虚拟地址作为一个索引来定位PTE2，然后再页表（内存）中读取它。因为设置了有效位，地址翻译硬件就会知道VP2被缓存在内存中，然后就会使用PTE中存储的物理内存地址，构造出这个字的物理地址。</p>
<figure>
<img src="https://s2.loli.net/2025/08/28/LNcFv8UT5wEyRVn.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="缺页">缺页</h3>
<p>缺页实际上就是缓存不命中，同上图。CPU引用了VP3中的一个字，地址翻译硬件根据有效位发现VP3并没有被缓存在内存中，于是触发一个缺页异常。这个异常调用内核中的缺页异常处理程序，该程序会选择一个牺牲页。程序将牺牲页复制回硬盘中，并将VP3覆盖牺牲页。并修改页表中它们的状态。然后返回，并将导致缺页的虚拟地址重新发送给地址翻译硬件，此时页命中，可以被正确处理：</p>
<figure>
<img src="https://s2.loli.net/2025/08/28/E3GubmVq1HlvFWY.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这个在磁盘和内存之间传送页的活动叫做<strong>页面调度</strong>，仅在不命中的情况下才进行调度的策略是<strong>按需页面调度</strong>，我们之后都会使用这个策略。</p>
<h3 id="分配页面">分配页面</h3>
<figure>
<img src="https://s2.loli.net/2025/08/28/eXsFdkTt5cBQ68D.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这个过程展示了当操作系统分配一个新的虚拟内存页时，对我们的示例页表产生的影响。在这个过程中，系统在磁盘上创建了一个空间并更新PTE5，使它指向磁盘上这个新创建的页面。</p>
<h3 id="局部性分析">局部性分析</h3>
<p>对于虚拟内存的策略，我们可能会认为这是一个效率极低的方案，因为它的不命中惩罚很大。但是实际上，它有着良好的<strong>局部性</strong>。局部性保证了，在任意时刻中，程序将趋于一个较小的活动页面上工作，例如空间局部性，较大的页空间确保了很好的空间局部性，因为对于数据结构，程序是按序访问的；对于时间局部性，一段内存往往会被反复利用，所以有着良好的时间局部性。</p>
<p>当然如果出现了工作集大小超出内存大小的情况时，程序可能会发生<strong>抖动</strong>，页面会不停的换进换出，带来严重的不命中开销。</p>
<h2 id="虚拟内存作为内存管理的工具">虚拟内存作为内存管理的工具</h2>
<p>虚拟内存不仅有着很好的缓存性能，同时它也很好的简化了内存管理，为我们提供了一个很好的内存保护机制。</p>
<p>实际上，操作系统为每个进程提供了一个独立的页表，也就是一个独立的虚拟空间，下图很好的展示了这一点：</p>
<figure>
<img src="https://s2.loli.net/2025/08/28/l57NgTqRbkWKtz3.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>注意，这里可以看到多个虚拟页面实际上是可以映射到同一个共享物理页面上。</p>
<p>通过按需页面调度和独立的虚拟地址空间的结合，系统对内存的使用和管理被极大的简化，VM系统简化了链接和加载、代码和数据共享、以及应用程序的内存分配…</p>
<h3 id="简化链接">简化链接</h3>
<p>独立的地址空间也允许每个进程的内存映像使用相同的基本格式，而不用考虑代码和数据实际上被存储在哪里。这样的一致性简化了链接器的设计和实现，允许链接器生成完全链接的可执行文件，这些可执行文件是独立于物理内存中代码和数据的最终位置的。</p>
<figure>
<img src="https://s2.loli.net/2025/08/06/mxfuyZFswWo1kbQ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="简化加载">简化加载</h3>
<p>虚拟内存简化了向内存中加载可执行文件和共享对象文件的过程。要把目标文件中<code>.text</code>和<code>.data</code>节加载到一个新创建的进程中，Linux加载器会为代码段和数据段分配虚拟页，然后将其标为无效的（即未缓存）。而不是将其进行缓存，只有当页被引用到时，虚拟内存会按需调度这些页面。</p>
<p>将一组连续的虚拟页映射到任意一个文件的任意位置的表示法叫做<strong>内存映射</strong>我们会在之后涉及这些内容。</p>
<h3 id="简化共享">简化共享</h3>
<p>一般而言，每个进程都有自己私有的代码，数据，堆栈等区域，这个其他进程是不共享的。操作系统为每个进程提供页表，将相应的虚拟页映射到不同的物理页面。也就是说，对于不同进程来说，尽管是同一个虚拟地址，但是实际上映射的是不同的物理地址。极大程度上简化了进程间私有的问题。</p>
<p>当然有时候进程间有也需要共享代码和数据，例如每个进程都调用相同的操作系统内核代码，操作系统会将不同进程中适当的虚拟页面映射到相同的物理页面，从而安排多个进程共享这部分代码的一个副本，而不是为每个进程都分配一个副本。</p>
<h3 id="简化内存分配">简化内存分配</h3>
<p>虚拟内存为用户进程提供了一个简单的分配额外内存的机制。当一个运行在用户进程的程序要求有一个额外的堆空间时，操作系统只需要分配适当的连续的虚拟内存页面，并将其映射到物理内存中的物理页面就行了。通过页表，操作系统也不用分配连续的物理页。使得页面可以随机的分布在物理内存中，提高了碎片空间的可用性。</p>
<h2 id="虚拟内存作为内存保护的工具">虚拟内存作为内存保护的工具</h2>
<p>操作系统需要有手段来控制对内存系统的访问，不应该允许用户进程对其只读代码段进行修改，也不应该允许它修改内核中的代码和数据结构，不应该允许它读写其他进程的私有内存或是修改和其他进程共享的虚拟原页面。而虚拟内存能够很好的实现这个机制：</p>
<p>当每次CPU生成一个地址时，地址翻译硬件都会读一个PTE，我们可以通过有效位来判断这个页面的状态。我们也可以通过添加额外的许可页来控制对一个虚拟页面内容的访问。</p>
<figure>
<img src="https://s2.loli.net/2025/08/28/MYLDidmhXo6Kgtc.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>例如图中的，SUP位表示进程是否必须在内核模式下才能访问此页。READ和WRITE位则控制对原页面的读写访问。不同进程的页表中对同一个页的访问权是不同的，以此可以实现对进程内存访问的控制。</p>
<p>如果一条指令违反了这些许可条件，那么CPU就会触发保护故障，将控制传递给异常处理程序。LInux
Shell一般将这个异常报告为<code>Segmentation fault</code></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/26/78-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/26/78-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4/" class="post-title-link" itemprop="url">78:初窥深度学习(4)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-26 13:33:07" itemprop="dateCreated datePublished" datetime="2025-08-26T13:33:07+08:00">2025-08-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-27 16:13:36" itemprop="dateModified" datetime="2025-08-27T16:13:36+08:00">2025-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我们先前实现了卷积神经网络的各层，以及基本的前向传播，现在我们要进一步的完善整个神经网络，通过反向传播实现对权重的更新，从而提高神经网络的准确性。</p>
<h2 id="反向传播">反向传播</h2>
<p>现在我们已经完成了神经网络的前向传播，现在我们需要对每个层进行反向传播以更新权重，来寻来你神经网络。进行反向传播，我们需要注意两点：</p>
<ul>
<li>在前向传播的阶段，我们需要在每一层换从它需要用于反向传播的数据（如中间值等）。这也反映了，任意反向传播的阶段，都需要有着相应的前向阶段。</li>
<li>在反向传播阶段，每一层都会接受一个梯度，并返回一个梯度。其接受其输出（<span
class="math inline">$\frac{\partial L}{\partial
out}$</span>）的损失梯度，并返回其输入（<span
class="math inline">$\frac{\partial L}{\partial
in}$</span>）的损失梯度</li>
</ul>
<p>我们的训练过程应该是这样的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">out = conv.forward((image / <span class="number">255</span>) - <span class="number">0.5</span>)</span><br><span class="line">out = pool.forward(out)</span><br><span class="line">out = softmax.forward(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化梯度</span></span><br><span class="line">gradient = np.zeros(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">gradient = softmax.backprop(gradient)</span><br><span class="line">gradient = pool.backprop(gradient)</span><br><span class="line">gradient = conv.backprop(gradient)</span><br></pre></td></tr></table></figure>
<p>现在我们将逐步构建我们的反向传播函数：</p>
<h3 id="softmax层反向传播">Softmax层反向传播</h3>
<p>我们的损失函数是： <span class="math display">$$
\begin{align*}
L = -ln(p_c)
\end{align*}
$$</span>
所以我们首先要计算的就是对于<code>softmax</code>层反向传播阶段的输入，其中<code>out_s</code>就是<code>softmax</code>层的输出。一个包含了10个概率的向量，我们只在乎正确类别的损失，所以我们的第一个梯度为：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial out_s(i)} =
\begin{cases}
0 \space\space\space\space\space \text{ if i!=c} \\
-\frac{1}{p_i} \text{ if i=c}
\end{cases}
\end{align*}
$$</span> 所以我们正确的初始梯度应该是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gradient = np.zeros(<span class="number">10</span>)</span><br><span class="line">gradient[label] = -<span class="number">1</span> / out[label]</span><br></pre></td></tr></table></figure>
<p>然后我们对softmax层的前向传播阶段进行一个缓存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">    <span class="comment"># 输入体积的形状</span></span><br><span class="line">    <span class="variable language_">self</span>.last_input_shape = <span class="built_in">input</span>.shape</span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.flatten()</span><br><span class="line">    <span class="comment"># 展平后的输入向量</span></span><br><span class="line">    <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line">    </span><br><span class="line">    totals = np.dot(<span class="built_in">input</span>,<span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">    <span class="comment"># 输出结果（提供给激活函数）</span></span><br><span class="line">    <span class="variable language_">self</span>.last_totals = totals</span><br><span class="line">    exp = np.exp(totals)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> exp/np.<span class="built_in">sum</span>(exp,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们可以开始准备softmax层的反向传播了：</p>
<h4 id="计算">计算</h4>
<p>我们已经计算出，损失对于激活函数值的梯度，我们现在需要进一步的推导，最终我们希望得到<span
class="math inline">$\frac{\partial L}{\partial input} \frac{\partial
L}{\partial w} \frac{\partial L}{\partial b}$</span></p>
<p>的梯度，以用于对权重的梯度训练。根据链式法则，我们应该有： <span
class="math display">$$
\begin{align*}
\frac{\partial L}{\partial w} &amp;= \frac{\partial L}{\partial out} *
\frac{\partial out}{\partial t} * \frac{\partial t}{\partial w} \\
\frac{\partial L}{\partial b} &amp;= \frac{\partial L}{\partial out} *
\frac{\partial out}{\partial t} * \frac{\partial t}{\partial b} \\
\frac{\partial L}{\partial input} &amp;= \frac{\partial L}{\partial out}
*
\frac{\partial out}{\partial t} * \frac{\partial t}{\partial input}
\end{align*}
$$</span> 其中
<code>t = w * input + b</code>，<code>out</code>则是softmax函数的输出值，我们可以依次求出。对于<span
class="math inline">$\frac{\partial L}{\partial out}$</span>我们有：
<span class="math display">$$
\begin{align*}
out_s(c) &amp;= \frac{e^{t_c}}{\sum_{i}e^{t_i}} = \frac{e^{t_c}}{S} \\
S &amp;= \sum_{i}e^{t_i} \\
\to out_s(c) &amp;= e^{t_c}S^{-1}
\end{align*}
$$</span> 现在我们求<span class="math inline">$\frac{\partial
out_s(c)}{\partial
t_k}$</span>，需要分别考虑<code>k=c</code>和<code>k!=c</code>的情况，我们依次进行求导：
<span class="math display">$$
\begin{align*}
\frac{\partial out_s(c)}{\partial t_k} &amp;= \frac{\partial
out_s(c)}{\partial S}
*\frac{\partial S}{\partial t_k} \\
&amp;= -e^{t_c}S^{-2}\frac{\partial S}{\partial t_k} \\
&amp;= -e^{t_c}S^{-2}(e^{t_k}) \\
&amp;= \frac{-e^{t_c}e^{t_k}}{S^2} \\
\\
\frac{\partial out_s(c)}{\partial t_c} &amp;=
\frac{Se^{t_c}-e^{t_c}\frac{\partial S}{\partial t_c}}{S^2} \\
&amp;= \frac{Se^{t_c}-e^{t_c}e^{t_c}}{S^2} \\
&amp;= \frac{e^{t_c}(S-e^{t_c})}{S^2} \\
\to
\frac{\partial out_s(k)}{\partial t} &amp;=
\begin{cases}
\frac{-e^{t_c}e^{t_k}}{S^2} \space\space\space\space \text{ if k!=c}  \\
\frac{e^{t_c}(S-e^{t_c})}{S^2} \text{ if k=c}
\end{cases}
\end{align*}
$$</span> 最后我们根据公式<code>t = w * input + b</code>得到: <span
class="math display">$$
\begin{align*}
\frac{\partial t}{\partial w}&amp;=input \\
\frac{\partial t}{\partial b}&amp;=1 \\
\frac{\partial t}{\partial input}&amp;=w
\end{align*}
$$</span> 现在我们可以用代码实现这个过程了</p>
<h4 id="实现">实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self,d_L_d_out</span>):</span><br><span class="line">    <span class="comment"># d_L_d_out是这一层的输出梯度,作为参数</span></span><br><span class="line">    <span class="comment"># 返回d_L_d_in作为下一层的参数</span></span><br><span class="line">    <span class="keyword">for</span> i,gradient <span class="keyword">in</span> <span class="built_in">enumerate</span>(d_L_d_out):</span><br><span class="line">        <span class="keyword">if</span> gradient == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># e^totals</span></span><br><span class="line">        t_exp = np.exp(<span class="variable language_">self</span>.last_totals)</span><br><span class="line">        <span class="comment"># S = sum(e^totals)</span></span><br><span class="line">        S = np.<span class="built_in">sum</span>(t_exp)</span><br><span class="line">        <span class="comment"># total对out[i]的梯度关系</span></span><br><span class="line">        <span class="comment"># 第一次是对所有的梯度进行更新</span></span><br><span class="line">        d_out_d_t = -t_exp[i]*t_exp / (S**<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 第二次是只对 =i 的梯度进行更新 从而使第一次的更新只针对 !=i 的梯度</span></span><br><span class="line">        d_out_d_t[i] = t_exp[i]*(S-t_exp[i]) / (S**<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 权重对total的梯度关系</span></span><br><span class="line">        d_t_d_w = <span class="variable language_">self</span>.last_input</span><br><span class="line">        d_t_d_b = <span class="number">1</span></span><br><span class="line">        d_t_d_input = <span class="variable language_">self</span>.weights</span><br><span class="line">        <span class="comment"># total对Loss的梯度关系</span></span><br><span class="line">        d_L_d_t = gradient * d_out_d_t</span><br><span class="line">        <span class="comment"># 权重对Loss的梯度关系</span></span><br><span class="line">        d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]</span><br><span class="line">        d_L_d_b = d_L_d_t * d_t_d_b</span><br><span class="line">        d_L_d_input = d_t_d_input @ d_L_d_t</span><br><span class="line">        <span class="comment"># 梯度训练</span></span><br><span class="line">    <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.learn_rate * d_L_d_w</span><br><span class="line">    <span class="variable language_">self</span>.biases -= <span class="variable language_">self</span>.learn_rate * d_L_d_b</span><br><span class="line">    <span class="comment"># 返回梯度</span></span><br><span class="line">    <span class="keyword">return</span> d_L_d_input.reshape(<span class="variable language_">self</span>.last_input_shape)</span><br></pre></td></tr></table></figure>
<p>由于softmax层的输入是一个输入体积，在一开始被我们展平处理了，但是我们返回的梯度也应该是一个同样大小的输入体积，所以我们需要通过<code>reshape</code>确保这层的返回的梯度和原始的输入格式相同。</p>
<p>我们可以测试一下softmax反向传播后的训练效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">from</span> maxpool <span class="keyword">import</span> MaxPool2</span><br><span class="line"><span class="keyword">from</span> softmax <span class="keyword">import</span> Softmax</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">test_images = x_test[:<span class="number">1000</span>]</span><br><span class="line">test_labels = y_test[:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">pool = MaxPool2()</span><br><span class="line">softmax = Softmax(<span class="number">13</span>*<span class="number">13</span>*<span class="number">8</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">image,label</span>):</span><br><span class="line">    out = conv.forward((image / <span class="number">255</span>) - <span class="number">0.5</span>)</span><br><span class="line">    out = pool.forward(out)</span><br><span class="line">    out = softmax.forward(out)</span><br><span class="line"></span><br><span class="line">    loss = -np.log(out[label])</span><br><span class="line">    acc = <span class="number">1</span> <span class="keyword">if</span> np.argmax(out) == label <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out,loss,acc</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">image,label</span>):</span><br><span class="line">    out, loss, acc = forward(image,label)</span><br><span class="line">    gradient = np.zeros(<span class="number">10</span>)</span><br><span class="line">    gradient[label] = -<span class="number">1</span> / out[label]</span><br><span class="line">    gradient = softmax.backprop(gradient)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss,acc</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start!&quot;</span>)</span><br><span class="line">loss = <span class="number">0</span></span><br><span class="line">num_correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,(im,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(test_images,test_labels)):</span><br><span class="line">    _, l, acc = forward(im,label)</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&#x27;[Step %d] Past 100 steps :Average Loss %.3f | Accuracy %d%%&#x27;</span> %</span><br><span class="line">            (i+<span class="number">1</span>,loss/<span class="number">100</span>,num_correct)</span><br><span class="line">        )</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        num_correct = <span class="number">0</span></span><br><span class="line">    l,acc = train(im,label)</span><br><span class="line">    loss += l</span><br><span class="line">    num_correct += acc</span><br></pre></td></tr></table></figure>
<p>可以看到准确率有明显的提升，说明我们softmax层的反向传播在很好的进行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Start!</span><br><span class="line">[Step <span class="number">100</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.112</span> | Accuracy <span class="number">24</span>%</span><br><span class="line">[Step <span class="number">200</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.940</span> | Accuracy <span class="number">37</span>%</span><br><span class="line">[Step <span class="number">300</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.686</span> | Accuracy <span class="number">50</span>%</span><br><span class="line">[Step <span class="number">400</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.606</span> | Accuracy <span class="number">51</span>%</span><br><span class="line">[Step <span class="number">500</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.451</span> | Accuracy <span class="number">58</span>%</span><br><span class="line">[Step <span class="number">600</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.362</span> | Accuracy <span class="number">65</span>%</span><br><span class="line">[Step <span class="number">700</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.264</span> | Accuracy <span class="number">66</span>%</span><br><span class="line">[Step <span class="number">800</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">1.057</span> | Accuracy <span class="number">75</span>%</span><br><span class="line">[Step <span class="number">900</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">0.978</span> | Accuracy <span class="number">81</span>%</span><br><span class="line">[Step <span class="number">1000</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">0.966</span> | Accuracy <span class="number">78</span>%</span><br></pre></td></tr></table></figure>
<h3 id="池化层传播">池化层传播</h3>
<p>在前向传播的过程中，最大池化层接收一个输入体积，然后通过2x2区域的最大池化，将宽度和高度都减半。而在反向传播中，执行相反的操作：我们将损失梯度的宽度和高度都翻倍，通过将每个梯度值分配到对应的2x2区域的最大值位置：</p>
<figure>
<img src="https://s2.loli.net/2025/08/27/EYAtgKoLmV4yhfU.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>每个梯度都被分配到原始最大值的位置，然后将其他梯度设置为0.</p>
<p>为什么是这这样的呢？在一个2x2区域中，由于我们只关注区域内的最大值，所以对于其他的非最大值，我们可以几乎忽略不计，因为它的改变对我们的输出结果没有影响，所以对于非最大像素，我们有<span
class="math inline">$\frac{\partial L}{\partial
inputs}=0$</span>。另一方面来看，最大像素的<span
class="math inline">$\frac{\partial output}{\partial
input}=1$</span>，这意味着<span class="math inline">$\frac{\partial
L}{\partial output}=\frac{\partial L}{\partial input}$</span></p>
<p>所以对于这一层的反向传播，我们只需要简单的还原，并且填充梯度值到最大像素区域就行了</p>
<h4 id="实现-1">实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self,d_L_d_out</span>):</span><br><span class="line">    <span class="comment"># 这里的self.last_input是前向阶段的数据缓存</span></span><br><span class="line">    d_L_d_input = np.zeros(<span class="variable language_">self</span>.last_input.shape)</span><br><span class="line">    <span class="keyword">for</span> im_region,i,j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">        h,w,f = im_region.shape</span><br><span class="line">        amax = np.amax(im_region,axis=(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i2 <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">            <span class="keyword">for</span> j2 <span class="keyword">in</span> <span class="built_in">range</span>(w):</span><br><span class="line">                <span class="keyword">for</span> f2 <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                    <span class="comment"># 搜寻区域内的最大值并赋梯度值</span></span><br><span class="line">                    <span class="keyword">if</span> im_region[i2,j2,f2] == amax[f2]:</span><br><span class="line">                        d_L_d_input[i*<span class="number">2</span>+i2, j*<span class="number">2</span>+j2,f2] = d_L_d_out[i,j,f2]</span><br><span class="line">    <span class="keyword">return</span> d_L_d_input</span><br></pre></td></tr></table></figure>
<p>这一部分并没有什么权重用来训练，所以只是一个简单的数据还原。</p>
<h3 id="卷积层反向传播">卷积层反向传播</h3>
<p>卷积层的反向传播，我们需要的是卷积层中的滤波器的损失梯度，因为我们需要利用损失梯度来更新我们滤波器的权重，我们现在已经有了<span
class="math inline">$\frac{\partial L}{\partial
output}$</span>，我们现在只需要计算<span
class="math inline">$\frac{\partial output}{\partial
filters}$</span>，所以我们需要知道，改变一个滤波器的权重会怎么影响到卷积层的输出？</p>
<p>实际上修改滤波器的任意权重都可能会导致滤波器输出的整个图像，下面便是很好的示例：</p>
<figure>
<img src="https://s2.loli.net/2025/08/27/iKdATqH2W1Xro37.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/27/AhM2BWCHNaULFDQ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>同样的对任何滤波器权重+1都会使输出增加相应图像像素的值，所以输出像素相对于特定滤波器权重的导数应该就是相应的图像元素。我们可以通过数学计算来论证这一点</p>
<h4 id="计算-1">计算</h4>
<p><span class="math display">$$
\begin{align*}
out(i,j) &amp;= convolve(image,filiter) \\
&amp;= \sum_{x=0}^3{}\sum_{y=0}^{3}image(i+x,j+y)*filiter(x,y) \\
\to \frac{\partial out(i,j)}{\partial filiter(x,y)} &amp;=image(i+x,i+y)
\end{align*}
$$</span></p>
<p>我们将输出的损失梯度引进来，我们就可以获得特定滤波器权重的损失梯度了：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial filiter(x,y)} =
\sum_{i}\sum_{j}\frac{\partial L}{\partial out(i,j)} *
\frac{\partial out(i,j)}{\partial filter(x,y)}
\end{align*}
$$</span> 现在我们可以实现我们卷积层的反向传播了：</p>
<h4 id="实现-2">实现</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">    d_L_d_filters = np.zeros(<span class="variable language_">self</span>.filters.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_filters):</span><br><span class="line">            d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.filters -= <span class="variable language_">self</span>.learn_rate * d_L_d_filters</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>现在我们可以对我们的神经网络进行一个完整的训练了，我们可以看到训练的结果如下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/27/jWL4NGbgkKSMmnq.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>效果还是非常不错的。</p>
<h2 id="完善">完善</h2>
<p>和之前的网络不同，CNN的训练集比较庞大，如果每次启动都要训练遍参数就太麻烦了，所以我们可以再每次训练之后将参数保存下来。下次再要使用就可以直接加载而不用重复训练。所以我们可以编写保存模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelSaver</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">&#x27;MNIST_CNN&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.model_name = model_name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;conv_filters&#x27;</span>: conv.filters,</span><br><span class="line">            <span class="string">&#x27;softmax_weights&#x27;</span>: softmax.weights,</span><br><span class="line">            <span class="string">&#x27;softmax_biases&#x27;</span>: softmax.biases</span><br><span class="line">        &#125;</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(data, f)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;保存参数到<span class="subst">&#123;filename&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                data = pickle.load(f)</span><br><span class="line"></span><br><span class="line">            conv.filters = data[<span class="string">&#x27;conv_filters&#x27;</span>]</span><br><span class="line">            softmax.weights = data[<span class="string">&#x27;softmax_weights&#x27;</span>]</span><br><span class="line">            softmax.biases = data[<span class="string">&#x27;softmax_biases&#x27;</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;模型参数加载成功&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;无可用模型参数&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>如果我们想要自己尝试手写输入，来测试模型的效果，我们可能希望有个手写板，所以我们可以再写一个手写板的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DrawingBoard</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="variable language_">self</span>.root = root</span><br><span class="line">        <span class="variable language_">self</span>.root.title(<span class="string">&quot;画板&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建一个 Canvas 作为画板</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas = tk.Canvas(root, width=<span class="number">280</span>, height=<span class="number">280</span>, bg=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.pack()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绑定鼠标事件</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas.bind(<span class="string">&quot;&lt;B1-Motion&gt;&quot;</span>, <span class="variable language_">self</span>.paint)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化绘图工具</span></span><br><span class="line">        <span class="variable language_">self</span>.image = Image.new(<span class="string">&quot;RGB&quot;</span>, (<span class="number">280</span>, <span class="number">280</span>), <span class="string">&quot;white&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.draw = ImageDraw.Draw(<span class="variable language_">self</span>.image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化画笔颜色和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_color = <span class="string">&quot;black&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_width = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加输出按钮</span></span><br><span class="line">        <span class="variable language_">self</span>.output_button = tk.Button(root, text=<span class="string">&quot;输出&quot;</span>, command=<span class="variable language_">self</span>.output_and_exit)</span><br><span class="line">        <span class="variable language_">self</span>.output_button.pack()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">paint</span>(<span class="params">self, event</span>):</span><br><span class="line">        x1, y1 = (event.x - <span class="variable language_">self</span>.brush_width), (event.y - <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        x2, y2 = (event.x + <span class="variable language_">self</span>.brush_width), (event.y + <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.create_oval(x1, y1, x2, y2, fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line">        <span class="variable language_">self</span>.draw.ellipse([x1, y1, x2, y2], fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_image</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 将图像调整为 28x28 像素</span></span><br><span class="line">        processed_image = <span class="variable language_">self</span>.image.resize((<span class="number">28</span>, <span class="number">28</span>), Image.Resampling.LANCZOS)</span><br><span class="line">        processed_image = ImageOps.grayscale(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将图像转换为 NumPy 数组</span></span><br><span class="line">        image_array = np.array(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确保像素值是整数</span></span><br><span class="line">        image_array = image_array.astype(np.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">output_and_exit</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 处理图像并获取数组</span></span><br><span class="line">        <span class="variable language_">self</span>.image_array = <span class="variable language_">self</span>.process_image()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存图像</span></span><br><span class="line">        processed_image = Image.fromarray(<span class="variable language_">self</span>.image_array)</span><br><span class="line">        processed_image.save(<span class="string">&quot;temp.png&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;图片已保存为 temp.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 退出程序</span></span><br><span class="line">        <span class="variable language_">self</span>.root.destroy()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在我们就可以使用它了，我们先进行训练，然后用保存的参数，来进行手写数字识别，我把整个网络的源代码放在下面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>:</span><br><span class="line">    <span class="comment"># 使用3x3滤波器的卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_filters, learn_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_filters = num_filters</span><br><span class="line">        <span class="variable language_">self</span>.filters = np.random.randn(num_filters, <span class="number">3</span>, <span class="number">3</span>) / <span class="number">9</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.learn_rate = learn_rate</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self, image</span>):</span><br><span class="line">        <span class="comment"># 返回所有可以卷积的3x3的图像区域</span></span><br><span class="line">        h, w = image.shape</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h - <span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(w - <span class="number">2</span>):</span><br><span class="line">                im_region = image[i:(i + <span class="number">3</span>), j:(j + <span class="number">3</span>)]</span><br><span class="line">                <span class="keyword">yield</span> im_region, i, j</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 执行卷积层的前向传播 输出一个26x26x8的三维输出数组</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line">        h, w = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h - <span class="number">2</span>, w - <span class="number">2</span>, <span class="variable language_">self</span>.num_filters))</span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            output[i, j] = np.<span class="built_in">sum</span>(im_region * <span class="variable language_">self</span>.filters, axis=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">        d_L_d_filters = np.zeros(<span class="variable language_">self</span>.filters.shape)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_filters):</span><br><span class="line">                d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.filters -= <span class="variable language_">self</span>.learn_rate * d_L_d_filters</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaxPool2</span>:</span><br><span class="line">    <span class="comment"># 池化尺寸为2的最大池化层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self, image</span>):</span><br><span class="line">        h, w, _ = image.shape</span><br><span class="line">        new_h = h // <span class="number">2</span></span><br><span class="line">        new_w = w // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">                im_region = image[i * <span class="number">2</span>:(i + <span class="number">1</span>) * <span class="number">2</span>, j * <span class="number">2</span>:(j + <span class="number">1</span>) * <span class="number">2</span>]</span><br><span class="line">                <span class="keyword">yield</span> im_region, i, j</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line">        h, w, num_filters = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h // <span class="number">2</span>, w // <span class="number">2</span>, num_filters))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            output[i, j] = np.amax(im_region, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">        d_L_d_input = np.zeros(<span class="variable language_">self</span>.last_input.shape)</span><br><span class="line">        <span class="keyword">for</span> im_region, i, j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="variable language_">self</span>.last_input):</span><br><span class="line">            h, w, f = im_region.shape</span><br><span class="line">            amax = np.amax(im_region, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i2 <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">                <span class="keyword">for</span> j2 <span class="keyword">in</span> <span class="built_in">range</span>(w):</span><br><span class="line">                    <span class="keyword">for</span> f2 <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                        <span class="keyword">if</span> im_region[i2, j2, f2] == amax[f2]:</span><br><span class="line">                            d_L_d_input[i * <span class="number">2</span> + i2, j * <span class="number">2</span> + j2, f2] = d_L_d_out[i, j, f2]</span><br><span class="line">        <span class="keyword">return</span> d_L_d_input</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Softmax</span>:</span><br><span class="line">    <span class="comment"># 全连接softmax激活层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_len, nodes, learn_rate=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(input_len, nodes) / nodes</span><br><span class="line">        <span class="variable language_">self</span>.biases = np.zeros(nodes)</span><br><span class="line">        <span class="variable language_">self</span>.learn_rate = learn_rate</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.last_input_shape = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.last_totals = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.last_input_shape = <span class="built_in">input</span>.shape</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.flatten()</span><br><span class="line">        <span class="variable language_">self</span>.last_input = <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line">        totals = np.dot(<span class="built_in">input</span>, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">        <span class="variable language_">self</span>.last_totals = totals</span><br><span class="line">        exp = np.exp(totals)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> exp / np.<span class="built_in">sum</span>(exp, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, d_L_d_out</span>):</span><br><span class="line">        <span class="comment"># d_L_d_out是这一层的输出梯度,作为参数</span></span><br><span class="line">        <span class="comment"># 返回d_L_d_in作为下一层的参数</span></span><br><span class="line">        d_L_d_w = np.zeros(<span class="variable language_">self</span>.weights.shape)</span><br><span class="line">        d_L_d_b = np.zeros(<span class="variable language_">self</span>.biases.shape)</span><br><span class="line">        d_L_d_input = np.zeros(<span class="variable language_">self</span>.last_input.shape)</span><br><span class="line">        <span class="keyword">for</span> i, gradient <span class="keyword">in</span> <span class="built_in">enumerate</span>(d_L_d_out):</span><br><span class="line">            <span class="keyword">if</span> gradient == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># e^totals</span></span><br><span class="line">            t_exp = np.exp(<span class="variable language_">self</span>.last_totals)</span><br><span class="line">            <span class="comment"># S = sum(e^totals)</span></span><br><span class="line">            S = np.<span class="built_in">sum</span>(t_exp)</span><br><span class="line">            <span class="comment"># total对out[i]的梯度关系</span></span><br><span class="line">            <span class="comment"># 第一次是对所有的梯度进行更新</span></span><br><span class="line">            d_out_d_t = -t_exp[i] * t_exp / (S ** <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 第二次是只对 =i 的梯度进行更新 从而使第一次的更新只针对 !=i 的梯度</span></span><br><span class="line">            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 权重对total的梯度关系</span></span><br><span class="line">            d_t_d_w = <span class="variable language_">self</span>.last_input</span><br><span class="line">            d_t_d_b = <span class="number">1</span></span><br><span class="line">            d_t_d_input = <span class="variable language_">self</span>.weights</span><br><span class="line">            <span class="comment"># total对Loss的梯度关系</span></span><br><span class="line">            d_L_d_t = gradient * d_out_d_t</span><br><span class="line">            <span class="comment"># 权重对Loss的梯度关系</span></span><br><span class="line">            d_L_d_w += d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]</span><br><span class="line">            d_L_d_b += d_L_d_t * d_t_d_b</span><br><span class="line">            d_L_d_input += d_t_d_input @ d_L_d_t</span><br><span class="line">            <span class="comment"># 梯度训练</span></span><br><span class="line">        <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.learn_rate * d_L_d_w</span><br><span class="line">        <span class="variable language_">self</span>.biases -= <span class="variable language_">self</span>.learn_rate * d_L_d_b</span><br><span class="line">        <span class="comment"># 返回梯度</span></span><br><span class="line">        <span class="keyword">return</span> d_L_d_input.reshape(<span class="variable language_">self</span>.last_input_shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelSaver</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name=<span class="string">&#x27;MNIST_CNN&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.model_name = model_name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;conv_filters&#x27;</span>: conv.filters,</span><br><span class="line">            <span class="string">&#x27;softmax_weights&#x27;</span>: softmax.weights,</span><br><span class="line">            <span class="string">&#x27;softmax_biases&#x27;</span>: softmax.biases</span><br><span class="line">        &#125;</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(data, f)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;保存参数到<span class="subst">&#123;filename&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, conv, pool, softmax</span>):</span><br><span class="line">        filename = <span class="string">f&#x27;<span class="subst">&#123;self.model_name&#125;</span>.pkl&#x27;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                data = pickle.load(f)</span><br><span class="line"></span><br><span class="line">            conv.filters = data[<span class="string">&#x27;conv_filters&#x27;</span>]</span><br><span class="line">            softmax.weights = data[<span class="string">&#x27;softmax_weights&#x27;</span>]</span><br><span class="line">            softmax.biases = data[<span class="string">&#x27;softmax_biases&#x27;</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;模型参数加载成功&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;无可用模型参数&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tkinter <span class="keyword">as</span> tk</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image,ImageDraw,ImageOps</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DrawingBoard</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="variable language_">self</span>.root = root</span><br><span class="line">        <span class="variable language_">self</span>.root.title(<span class="string">&quot;画板&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建一个 Canvas 作为画板</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas = tk.Canvas(root, width=<span class="number">280</span>, height=<span class="number">280</span>, bg=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.pack()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绑定鼠标事件</span></span><br><span class="line">        <span class="variable language_">self</span>.canvas.bind(<span class="string">&quot;&lt;B1-Motion&gt;&quot;</span>, <span class="variable language_">self</span>.paint)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化绘图工具</span></span><br><span class="line">        <span class="variable language_">self</span>.image = Image.new(<span class="string">&quot;RGB&quot;</span>, (<span class="number">280</span>, <span class="number">280</span>), <span class="string">&quot;white&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.draw = ImageDraw.Draw(<span class="variable language_">self</span>.image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化画笔颜色和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_color = <span class="string">&quot;black&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.brush_width = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加输出按钮</span></span><br><span class="line">        <span class="variable language_">self</span>.output_button = tk.Button(root, text=<span class="string">&quot;输出&quot;</span>, command=<span class="variable language_">self</span>.output_and_exit)</span><br><span class="line">        <span class="variable language_">self</span>.output_button.pack()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">paint</span>(<span class="params">self, event</span>):</span><br><span class="line">        x1, y1 = (event.x - <span class="variable language_">self</span>.brush_width), (event.y - <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        x2, y2 = (event.x + <span class="variable language_">self</span>.brush_width), (event.y + <span class="variable language_">self</span>.brush_width)</span><br><span class="line">        <span class="variable language_">self</span>.canvas.create_oval(x1, y1, x2, y2, fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line">        <span class="variable language_">self</span>.draw.ellipse([x1, y1, x2, y2], fill=<span class="variable language_">self</span>.brush_color, outline=<span class="variable language_">self</span>.brush_color)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_image</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 将图像调整为 28x28 像素</span></span><br><span class="line">        processed_image = <span class="variable language_">self</span>.image.resize((<span class="number">28</span>, <span class="number">28</span>), Image.Resampling.LANCZOS)</span><br><span class="line">        processed_image = ImageOps.grayscale(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将图像转换为 NumPy 数组</span></span><br><span class="line">        image_array = np.array(processed_image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确保像素值是整数</span></span><br><span class="line">        image_array = image_array.astype(np.uint8)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image_array</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">output_and_exit</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 处理图像并获取数组</span></span><br><span class="line">        <span class="variable language_">self</span>.image_array = <span class="variable language_">self</span>.process_image()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存图像</span></span><br><span class="line">        processed_image = Image.fromarray(<span class="variable language_">self</span>.image_array)</span><br><span class="line">        processed_image.save(<span class="string">&quot;temp.png&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;图片已保存为 temp.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 退出程序</span></span><br><span class="line">        <span class="variable language_">self</span>.root.destroy()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/25/77-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/25/77-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3/" class="post-title-link" itemprop="url">77:初窥深度学习(3)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-25 11:07:52" itemprop="dateCreated datePublished" datetime="2025-08-25T11:07:52+08:00">2025-08-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-26 13:28:22" itemprop="dateModified" datetime="2025-08-26T13:28:22+08:00">2025-08-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一篇文章中我们学习了循环神经网络，我们现在已经基本理解了神经网络怎么去处理数据/序列。可是对于图片、音频、文件之类的数据，我们该怎么去处理呢？相较于数据、序列，对图片使用传统神经网络会导致更大的开销。其他的数据类型也是同理，所以接下来我们将要认识<strong>卷积神经网络</strong>。</p>
<h1 id="卷积神经网络简介">卷积神经网络简介</h1>
<p>卷积神经网络的一个经典应用场景是对图像进行分类，可是我们可不可以使用普通的神经网络来实现呢？可以，但是没必要。对于图像数据处理，我们需要面临两个问题：</p>
<ul>
<li><strong>图像数据很大</strong>
假如我们要处理的图像大小是100x100甚至更大。那么构建一个处理100x100的彩色图像的神经网络，我们需要<code>100x100x3 = 30000</code>个输入特征。我们用一个1024个节点的中间层，意味着我们在一层中就要训练<code>30000x1024 = 30720000</code>个权重。这样会导致我们的神经网络十分庞大</li>
<li><strong>图像特征的位置会改变</strong>
同一个特征可能是在图像中的不同位置，你可能可以训练出一个对于特定图像表现良好的网络。但是当你对图像进行一定的偏移，可能就会导致结果发生错误的改变</li>
</ul>
<p>使用传统的神经网络来解决图像问题，无异于是浪费的。它忽视了图像中任意像素与其邻近像素的上下文关系，图像中的物体是由小范围的局部特征组成的，对每个像素都进行分析，是毫无意义的。</p>
<p>所以我们需要使用卷积神经网络来解决这些问题。</p>
<h2 id="目标">目标</h2>
<p>这一次我们的目标是实现一个手写数字识别的卷积神经网络，用到的是MNIST的手写数字数据集。也就是给定一个图像，将其分类为一个数字。</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/sMCW7taiUzfoTLE.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>MNIST数据集中的每张图片都是28*28的大小，包含一个居中的灰度数字。我们将根据这个数据集来对神经网络进行训练。</p>
<h2 id="卷积">卷积</h2>
<p>我们首先要理解卷积神经网络中的<strong>卷积</strong>是什么意思。卷积实际上是一种加权平均的操作。它的相当于一个滤波器，能够提取原始数据中的某种特定特征。我们往往使用卷积核来进行这个操作。</p>
<p>而神经网络中的卷积层则是根据过滤器实现对局部特征的处理，我们以下面这个操作为例：</p>
<p>对于一个垂直特征的卷积核，我们可以计算出这里的特征值</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/vHyiLxACUaI37Sz.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/25/ANzH7ilPxuv2hY9.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们们可以通过对图像中的数据进行卷积操作从而实现对局部特征的提取。这就和我们将要用到的卷积核有关了。</p>
<h3 id="卷积核">卷积核</h3>
<figure>
<img src="https://s2.loli.net/2025/08/25/Z9CguILrM1hWP7l.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这是一个垂直sobel滤波器，通过它对图像进行卷积操作，我们可以提取出图像的垂直特征：</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/Nrh93wdMmyjUDzK.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>同样的，我们有对应的水平SObel卷积核，可以提取出图像的水平特征：</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/mLjtAvOKxquh9yb.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/08/25/J7UdFDqpAXrI3CZ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>而Sobel滤波器，我们可以理解成边缘检测器。通过提取手写数字边缘的特征，有利于网络在后续更好的进行图像识别。</p>
<h3 id="填充">填充</h3>
<p>对于卷积这一步，我们对一个4x4的输入图像使用一个3x3的滤波器，我们会得到一个2x2的输出图像。如果我们希望输出图像和输入图像保持相同的大小。我们则需要向周围添加0，使得滤波器可以在更多的位置上覆盖</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/C93UiGz2PS4dLMV.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这种操作，我们称之为相同填充。如果不适用任何填充，我们称之为有效填充。</p>
<h3 id="卷积层的使用">卷积层的使用</h3>
<p>我们现在知道卷积层通过使用一组滤波器将输入图像转换为输出图像的卷积层了。我们将使用一个具有8个滤波器的小卷积层作为我们网络中的起始层，意味着，它将28x28的输入图像转换为26x26x8的输出体积：</p>
<figure>
<img src="https://s2.loli.net/2025/08/25/tLZaDENvhopsPH2.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>每个卷积层的8个过滤器产生一个26x26的输出，这是因为我们用到的是3x3的卷积核作为我们的滤波器，所以我们需要训练的权重有<code>3x3x8 = 72</code>个权重</p>
<h3 id="实现">实现</h3>
<p>现在我们尝试用代码实现一个卷积层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>:</span><br><span class="line">    <span class="comment"># 使用3x3滤波器的卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_filters</span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_filters = num_filters</span><br><span class="line">        <span class="comment"># 这里除以3是为了对权重进行初始化</span></span><br><span class="line">        <span class="variable language_">self</span>.filters = np.random.randn(num_filters,<span class="number">3</span>,<span class="number">3</span>) / <span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>我们注意到我们对生成的卷积核中做了一个权重初始化的工作，这是因为：</p>
<ul>
<li>如果初始权重太大，那么输入数据经过卷积计算之后会变得很大，在反向传播的过程中梯度值也会变得很大，从而导致参数无法收敛，即<strong>梯度爆炸</strong></li>
<li>如果初始权重太小，由于激活函数的作用，输入的数据会层层缩小，导致反向传播过程中的梯度值变得绩效。难以实现对权重的有效更新，我们称之为<strong>梯度消失</strong></li>
</ul>
<p>这里我们用到Xavier初始化来解决这个问题，他指出，在保持网络层在初始化时，其输入核和输出的方差应该尽可能的相同。这样信号就可以在网络中稳定的传播。</p>
<p>我们设输入为y输出为x，权重矩阵为W。则有： <span
class="math display">$$
\begin{align*}
Var(W) = \frac{1}{n_{in}}
\end{align*}
$$</span>
其中<code>n_in</code>是输入的节点数量，这里就是<code>3x3</code>，所以初始化时需要<code>/9</code></p>
<p>接下来是实际的卷积部分的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>:</span><br><span class="line">    <span class="comment"># 使用3x3滤波器的卷积层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_filters</span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_filters = num_filters</span><br><span class="line">        <span class="variable language_">self</span>.filters = np.random.randn(num_filters,<span class="number">3</span>,<span class="number">3</span>) / <span class="number">9</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self,image</span>):</span><br><span class="line">        <span class="comment"># 返回所有可以卷积的3x3的图像区域</span></span><br><span class="line">        h,w = image.shape</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(h-<span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(w-<span class="number">2</span>):</span><br><span class="line">                im_region = image[i:(i+<span class="number">3</span>),j:(j+<span class="number">3</span>)]</span><br><span class="line">                <span class="keyword">yield</span> im_region,i,j</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 执行卷积层的前向传播 输出一个26x26x8的三维输出数组</span></span><br><span class="line">        h,w = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h-<span class="number">2</span>,w-<span class="number">2</span>,<span class="variable language_">self</span>.num_filters))</span><br><span class="line">        <span class="keyword">for</span> im_region,i,j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            <span class="comment"># 这里用到的是numpy中隐藏的广播机制，详情参考numpy</span></span><br><span class="line">            <span class="comment"># 这里im_region*self.filters的大小是(8,3,3),求和是对行列求和,所以axis=(1,2)</span></span><br><span class="line">            output[i,j] = np.<span class="built_in">sum</span>(im_region * <span class="variable language_">self</span>.filters,axis=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这里我们很多用法涉及到<code>numpy</code>的一些高级使用，可以在这里参考<a
target="_blank" rel="noopener" href="https://numpy.net.cn/learn/">NumPy</a>现在我们可以检查我们的卷积层是否输出了我们理想的结果:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf		<span class="comment"># 由于MNIST数据集URL地址有问题，所以这里使用keras库</span></span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">output = conv.forward(x_train[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(output.shape)	<span class="comment"># (26,26,8) </span></span><br></pre></td></tr></table></figure>
<h2 id="池化">池化</h2>
<p>图像中的相邻元素往往是相似的，所以卷积层输出中，通常相邻元素产生相似的值。结果导致卷积层输出中包含了大量的冗余信息。为了解决这个问题我们需要对数据进行<strong>池化</strong></p>
<p>它所做的事情很简单，往往是将输出中的值聚合称为更小的尺寸。池化往往是通过简单的操作，如<code>max,min,average</code>实现的。比如下面就是一个池化大小为2的最大池化操作</p>
<figure>
<img src="https://s2.loli.net/2025/08/26/lrUcqBJ4A6MItC7.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>池化将输入的宽度和高度除以池化大小。在我们的卷积神经网络中，我们将在初始卷积层之后放置一个池化大小为2的最大池化层，池化层将<code>26x26x8</code>的输入转化为<code>13x13x8</code>的输出：</p>
<figure>
<img src="https://s2.loli.net/2025/08/26/nkzo2HSwGaCx367.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="实现-1">实现</h3>
<p>我们现在用代码实现和conv类相似的<code>MaxPool2</code>类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaxPool2</span>:</span><br><span class="line">    <span class="comment"># 池化尺寸为2的最大池化层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">iterate_regions</span>(<span class="params">self,image</span>):</span><br><span class="line">        h,w,_ = image.shape</span><br><span class="line">        new_h = h // <span class="number">2</span></span><br><span class="line">        new_w = w // <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_h):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(new_w):</span><br><span class="line">                im_region = image[i*<span class="number">2</span>:(i+<span class="number">1</span>)*<span class="number">2</span>,j*<span class="number">2</span>:(j+<span class="number">1</span>)*<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">yield</span> im_region,i,j</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        h,w,num_filters = <span class="built_in">input</span>.shape</span><br><span class="line">        output = np.zeros((h//<span class="number">2</span>,w//<span class="number">2</span>,num_filters))        </span><br><span class="line">        <span class="keyword">for</span> im_region,i,j <span class="keyword">in</span> <span class="variable language_">self</span>.iterate_regions(<span class="built_in">input</span>):</span><br><span class="line">            <span class="comment"># 这里im_region的大小是(3,3,8)因此我们只需要对行列求最大，故axis=(0,1)</span></span><br><span class="line">            output[i,j] = np.amax(im_region,axis=(<span class="number">0</span>,<span class="number">1</span>))            </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这个类和之前实现的<code>Conv3x3</code>类类似，关键在于从一个给定的图像区域中找到最大值，我们使用数组的最大值方法<code>np.amax()</code>来实现。我们来测试一下池化层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">from</span> maxpool <span class="keyword">import</span> MaxPool2</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">pool = MaxPool2()</span><br><span class="line"></span><br><span class="line">output = conv.forward(x_train[<span class="number">0</span>])</span><br><span class="line">output = pool.forward(output)</span><br><span class="line"><span class="built_in">print</span>(output.shape)	<span class="comment"># (13,13,8)</span></span><br></pre></td></tr></table></figure>
<h2 id="softmax层">Softmax层</h2>
<p>现在我们通过前两层，已经提取出了数字特征，现在我们希望能够赋予其实际预测的能力。对于多分类问题，我们通常使用<code>Softmax</code>层作为最终层——这是一个使用<code>Softmax</code>函数作为激活函数的全连接层（全连接层就是每个节点都与前一层的每个输入相联）</p>
<p>我们将使用一个包含10个节点的<code>Softmax</code>层作为CNN的最后一层，每个节点代表一个数字。层中的每个节点都连接到之前的输出中。在Softmax变化之后，概率最高的数字就是我们的输出。</p>
<figure>
<img src="https://s2.loli.net/2025/08/26/sCU9YwafPzxJlZ4.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="交叉熵损失">交叉熵损失</h3>
<p>我们现在既然可以输出最终的预测结果了，它输出的结果是一个概率，用来量化神经网络的对其预测的信心。同样的，我们也需要一种方法来量化每次预测的损失。这里我们使用交叉熵损失来解决这个问题：
<span class="math display">$$
\begin{align*}
L = -ln(p_c)
\end{align*}
$$</span> 其中c指的是正确的类别，即正确的数字。而<span
class="math inline"><em>p</em><sub><em>c</em></sub></span>代表类别c的预测概率。我们希望损失越低越好，对网络的损失进行量化，有利于后续的神经网络训练。</p>
<h3 id="实现-2">实现</h3>
<p>我们同上步骤，实现一个<code>Softmax</code>层类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Softmax</span>:</span><br><span class="line">    <span class="comment"># 全连接softmax激活层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_len, nodes</span>):</span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(input_len,nodes) / nodes</span><br><span class="line">        <span class="variable language_">self</span>.biases = np.zeros(nodes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 由于输入是一个输入体积,我们用flatten将其展平,变成一个一维的输出向量</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.flatten()</span><br><span class="line">        totals = np.dot(<span class="built_in">input</span>,<span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.biases</span><br><span class="line">        exp = np.exp(totals)</span><br><span class="line">        <span class="keyword">return</span> exp/np.<span class="built_in">sum</span>(exp,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>现在，我们已经完成了CNN的整个前向传播，我们可以简单的测试一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> conv <span class="keyword">import</span> Conv3x3</span><br><span class="line"><span class="keyword">from</span> maxpool <span class="keyword">import</span> MaxPool2</span><br><span class="line"><span class="keyword">from</span> softmax <span class="keyword">import</span> Softmax</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">test_images = x_test[:<span class="number">1000</span>]</span><br><span class="line">test_labels = y_test[:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">conv = Conv3x3(<span class="number">8</span>)</span><br><span class="line">pool = MaxPool2()</span><br><span class="line">softmax = Softmax(<span class="number">13</span>*<span class="number">13</span>*<span class="number">8</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">image,label</span>):</span><br><span class="line">    out = conv.forward((image / <span class="number">255</span>) - <span class="number">0.5</span>)</span><br><span class="line">    out = pool.forward(out)</span><br><span class="line">    out = softmax.forward(out)</span><br><span class="line"></span><br><span class="line">    loss = -np.log(out[label])</span><br><span class="line">    acc = <span class="number">1</span> <span class="keyword">if</span> np.argmax(out) == label <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out,loss,acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start!&quot;</span>)</span><br><span class="line">loss = <span class="number">0</span></span><br><span class="line">num_correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,(im,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(test_images,test_labels)):</span><br><span class="line">    _, l, acc = forward(im,label)</span><br><span class="line">    loss += l</span><br><span class="line">    num_correct += acc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&#x27;[Step %d] Past 100 steps :Average Loss %.3f | Accuracy %d%%&#x27;</span> %</span><br><span class="line">            (i+<span class="number">1</span>,loss/<span class="number">100</span>,num_correct)</span><br><span class="line">        )</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        num_correct = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>我们可以得到下面的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Start!</span><br><span class="line">[Step <span class="number">100</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.566</span> | Accuracy <span class="number">13</span>%</span><br><span class="line">[Step <span class="number">200</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.447</span> | Accuracy <span class="number">13</span>%</span><br><span class="line">[Step <span class="number">300</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.500</span> | Accuracy <span class="number">13</span>%</span><br><span class="line">[Step <span class="number">400</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.520</span> | Accuracy <span class="number">10</span>%</span><br><span class="line">[Step <span class="number">500</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.431</span> | Accuracy <span class="number">9</span>%</span><br><span class="line">[Step <span class="number">600</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.477</span> | Accuracy <span class="number">6</span>%</span><br><span class="line">[Step <span class="number">700</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.491</span> | Accuracy <span class="number">11</span>%</span><br><span class="line">[Step <span class="number">800</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.598</span> | Accuracy <span class="number">7</span>%</span><br><span class="line">[Step <span class="number">900</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.545</span> | Accuracy <span class="number">7</span>%</span><br><span class="line">[Step <span class="number">1000</span>] Past <span class="number">100</span> steps :Average Loss <span class="number">2.610</span> | Accuracy <span class="number">10</span>%</span><br></pre></td></tr></table></figure>
<p>这是因为我们对权重进行了随机初始化，所以现在神经网络的表现更像是随机猜测，所以准确率趋近于10%</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/24/76-%E5%AF%B9%E6%9A%91%E5%81%87%E7%9A%84%E5%8F%8D%E6%80%9D%E4%B8%8E%E4%B8%8B%E4%B8%AA%E5%AD%A6%E6%9C%9F%E7%9A%84%E5%B1%95%E6%9C%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/24/76-%E5%AF%B9%E6%9A%91%E5%81%87%E7%9A%84%E5%8F%8D%E6%80%9D%E4%B8%8E%E4%B8%8B%E4%B8%AA%E5%AD%A6%E6%9C%9F%E7%9A%84%E5%B1%95%E6%9C%9B/" class="post-title-link" itemprop="url">76:对暑假的反思与下个学期的展望</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-08-24 10:42:33 / 修改时间：11:20:08" itemprop="dateCreated datePublished" datetime="2025-08-24T10:42:33+08:00">2025-08-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">日常总结</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天是8月24号，暑假已然过去了2/3，我打算明天就提前返校。我想对我的暑假做一个总结，这个暑假学了很多东西，相较于平时有更多的集中连续的时间可以用了学习，提升个人的能力。在校期间，我更多是用一些零散的时间来通过做项目来学习。但感觉难以长时间的专注在一个知识点上。始终是项目驱动学习，被动的去了解一些内容，暑假的话有大把的时间，可以去了解自己感兴趣的内容。我这个暑假主要学习了这些内容：</p>
<ul>
<li>处理器 及其 流水线机制</li>
<li>程序性能优化简单了解</li>
<li>存储器层次结构的认识</li>
<li>简单算法入门 与 少量的刷题</li>
<li>链接编译过程</li>
<li>深度学习简单神经网络的构造 与 原理认识</li>
<li>异常控制流机制</li>
<li>Linux shell编程 以及 简单指令的使用</li>
</ul>
<p>这么一看学到的东西很多，但是大多数都是浅尝辄止，我打算多多接触之后再来明确自己感兴趣的方向。接下来是对我暑假的批评和反思。从7.3暑假开始到8.24今天，一共是52天，自由可支配（除去睡觉吃饭休息）的时间大概有600+个小时，我每次的学习时长可能也就4~5个小时，零零散散学了25天左右。剩下的时间浪费在打游戏，看动漫，看电影，看小说上面。</p>
<p>我暑假前定了很多目标，基本一个都没达到，超级灰心。但是，这也是意料之中的事情，取乎其上、得乎其中。所以我打算早点返校，好好用这最后的几天拯救一下我的暑假。但是怎么评价这个暑假呢，一直以来我都对自己有着很高的期望，但是通过这个暑假，我很好的认识清楚了自己的懒惰性，我也不太喜欢这样，但是自控力还是太差了，我也会好好的反思自己的行为。其次是这个暑假开拓了很多眼界，我意识到了和别人的差距，我总是把目光拘于学校，年级，班级，寝室，甚至是自己。我在网上看到了，认识到了很多优秀的人，感受到了差距，所以我打算继续前行，去学习各种感兴趣的内容。</p>
<p>对于下个学期的，我也是略有想法。下个学期课程十分繁重，但是大多是专业课程。这个是我的优势，专业课程内容我有很好的基础，我打算不跟着学校的计划走，我打算跟着南京大学的课程设计进行学习，计组方面打算跟着它们的PA学习，操作系统打算跟着JYY老师的课程深入了解一下。数据结构与算法，我打算制定一个刷题计划，要保证每天有一定的算法学习时间。还有科研方面，我打算联系LLF老师尝试写出我的第一篇论文。暑假我看着两位学长保研，我一直以为保研就是对绩点要求很高，但是实际上本科期间的科研成果是及其重要的，我这个暑假一直疏于对这方面的学习。下个学期，我打算花大部分的时间在这上面。</p>
<p>总结下来，下个学期的几个主要任务：</p>
<ul>
<li>利用专业知识的优势，提升绩点成绩，加深对专业知识的掌握理解</li>
<li>提高算法能力，争取在下一学年中能够掌握常见的算法题型</li>
<li>深入学习逆向工程和二进制漏洞审计的，多刷题，强化竞赛能力</li>
<li>掌握最基本的科研能力，要开始入手自己的第一篇论文</li>
<li>学习计算机网络，完善技术栈</li>
</ul>
<p>不过我还是很期待9月份的到来，一个是9.3有大阅兵，我特别想看。还有一个是丝之歌发售，我特别想玩，我打算一出就开始玩，通宵玩，累了就睡，饿了就吃，醒了就玩。我初步估计可能通关需要20~30小时，也就是<code>[9.3,9.6]</code>好好玩四天。剩下的时间就对暑期的计划做一个收尾。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/17/75-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/1.jpg">
      <meta itemprop="name" content="Ylin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ylin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/17/75-%E5%88%9D%E7%AA%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2/" class="post-title-link" itemprop="url">75:初窥深度学习(2)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-17 14:14:27" itemprop="dateCreated datePublished" datetime="2025-08-17T14:14:27+08:00">2025-08-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-08-24 10:41:42" itemprop="dateModified" datetime="2025-08-24T10:41:42+08:00">2025-08-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上一篇博客中，我们完成了一个简单的前馈神经网络，完成了对根据身高体重对性别进行猜测的神经网络，以及对他的训练。但是我们不该止步于此，接下来我们将尝试编写一个RNN循环神经网络，并认识它背后的原理。</p>
<h1 id="循环神经网络简介">循环神经网络简介</h1>
<p>循环神经网络是一种专门用于处理序列的神经网络，因此其对于处理文本方面十分有效。且对于前馈神经网络和卷积神经网络，我们发现：它们都只能处理预定义的尺寸——接受固定大小的输入并产生固定大小的输出。但是循环神经网络可以处理任意长度的序列，并返回。它可以是这样的：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/d2UGIDEpmSKtrv9.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这种处理序列的方式可以实现很多功能。例如，文本翻译，事件评价…
我们的目标是让它完成对一个评论的判断（是正面的还是负面的）。将待分析的文本输入神经网络然后，然后给出判断。</p>
<h2 id="实现方式">实现方式</h2>
<p>我们考虑一个输入为<code>x0,x1,x2,...,xn</code>，输出为<code>y0,y1,y2,..,yn</code>的多对多循环神经网络。这些xi和yi是向量，可以是任意维度。RNNs通过迭代更新一个隐藏状态<code>h</code>，重复这些步骤：</p>
<ul>
<li>下一个隐藏状态h<sub>t</sub>是前一个状态h<sub>t-1</sub>和下一个输入x<sub>t</sub>计算得出的</li>
<li>输出y<sub>t</sub>是由当前的隐藏状态h<sub>t</sub>计算得出的</li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/08/14/BbSIeWQf2KVi1Na.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这就是RNNs为什么是<strong>循环</strong>神经网络的原因，对于上面步骤的每一步中，都使用的是同一个权重。对于一个典型的RNNs，我们只需要使用3组权重就可以计算：</p>
<ul>
<li><strong>W<sub>xh</sub></strong> 用于所有x<sub>t</sub> -&gt;
h<sub>t</sub>的连接</li>
<li><strong>W<sub>hh</sub></strong> 用于所有h<sub>t-1</sub> -&gt;
h<sub>t</sub>的连接</li>
<li><strong>W<sub>hy</sub></strong> 用于所有h<sub>t</sub> -&gt;
y<sub>t</sub>的连接</li>
</ul>
<p>同时我们还需要为两次输出设置偏置：</p>
<ul>
<li><strong>b<sub>h</sub></strong> 计算h<sub>t</sub>时的偏置</li>
<li><strong>b<sub>y</sub></strong> 计算y<sub>t</sub>时的偏置</li>
</ul>
<p>我们将权重表示为矩阵，将偏置表示为向量，从而组合成整个RNNs。我们的输出是：
<span class="math display">$$
\begin{align*}
h_t &amp;= tanh(W_{xh}x_t + W_{hh}h_{t-1}+b_h) \\
y_t &amp;= W_{hy}h_t + b_y
\end{align*}
$$</span>
我们使用<code>tanh</code>作为隐藏状态的激活函数，其图像函数如下：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/sg3wfaHoRbJqPeZ.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="目标与计划">目标与计划</h2>
<p>我们要从头实现一个RNN，执行一个情感分析任务——判断给定的文本是正面消息还是负面的。</p>
<p>这是我们要用的训练集：<a
href="%5Brnn-from-scratch/data.py%20at%20master%20·%20vzhou842/rnn-from-scratch%5D(https://github.com/vzhou842/rnn-from-scratch/blob/master/data.py)">data</a></p>
<p>下面是一些训练集的样例：</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/zHjVRN6fu9Msmlh.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>由于这是一个分类问题，所以我们使用多对一的循环神经网络，即最终只使用最终的隐藏状态来生成一个输出。每个<code>xi</code>都是一个代表文本中一个单词的向量。输出<code>y</code>是一个二维向量，分别代表正面和负面。我们最终使用<code>softmax</code>将其转换为概率。</p>
<figure>
<img src="https://s2.loli.net/2025/08/14/CiG5xHMEQAy1RTV.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h2 id="数据集预处理">数据集预处理</h2>
<p>神经网络无法直接识别单词，我们需要处理数据集，让它变成能被神经网络使用的数据格式。首先我们需要收集一下数据集中所有单词的词汇表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = <span class="built_in">list</span>(<span class="built_in">set</span>([w <span class="keyword">for</span> text <span class="keyword">in</span> train_data.keys() <span class="keyword">for</span> w <span class="keyword">in</span> text.split(<span class="string">&quot; &quot;</span>)]))</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure>
<p><code>vocab</code>是一个包含训练集中出现的所有的单词的列表。接下来，我们将为每一个词汇中的单词都分配一个整数索引，因为神经网络无法理解单词，所以我们要创造一个单词和整数索引的关系：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_to_idx = &#123;w:i <span class="keyword">for</span> i,w <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">idx_to_word = &#123;i:w <span class="keyword">for</span> i,w <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br></pre></td></tr></table></figure>
<p>我们还要注意循环神经网络接收的每个输入都是一个向量<code>xi</code>，我们需要使用<code>one-hot</code>编码，将我们的每一个输入都转换成一个向量。对于一个<code>one-hot</code>向量，每个词汇对应于一个唯一的向量，这种向量出了一个位置外，其他位置都是0，在这里我们将每个<code>one-hot</code>向量中的<code>1</code>的位置，对应于单词的整数索引位置。</p>
<p>也就是说，我们的词汇表中有n个单词，我们的每个输入<code>xi</code>就应该是一个n维的<code>one-hot</code>向量。我们写一个函数，以用来创建向量输入，将其作为神经网络的输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">createInputs</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> text.split(<span class="string">&quot; &quot;</span>):</span><br><span class="line">        v = np.zeros((vocab_size,<span class="number">1</span>))	<span class="comment"># 创建一个vocab_size*1的全零向量</span></span><br><span class="line">        v[word_to_idx[w]] = <span class="number">1</span></span><br><span class="line">        inputs.append(v)</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br></pre></td></tr></table></figure>
<h2 id="向前传播">向前传播</h2>
<p>现在我们开始实现我们的RNN，我们先初始化我们所需的3个权重和2个偏置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> randn	<span class="comment"># 正态分布随机函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size, hidden_size=<span class="number">64</span></span>):</span><br><span class="line">        <span class="comment"># weights</span></span><br><span class="line">        <span class="variable language_">self</span>.Whh = randn(hidden_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Wxh = randn(hidden_size,input_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Why = randn(output_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="comment"># biases</span></span><br><span class="line">        <span class="variable language_">self</span>.bh = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.by = np.zeros((output_size,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>我们通过<code>np.random.randn()</code>从标准正态分布中初始化我们的权重。接下来我们将根据公式：
<span class="math display">$$
\begin{align*}
h_t &amp;= tanh(W_{xh}x_t + W_{hh}h_{t-1}+b_h) \\
y_t &amp;= W_{hy}h_t + b_y
\end{align*}
$$</span> 实现我们的向前传播函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">    h = np.zeros((<span class="variable language_">self</span>.Whh.shape[<span class="number">0</span>],<span class="number">1</span>)) <span class="comment"># 在刚开始我们的h是零向量，在此之前没有先前的h</span></span><br><span class="line">    <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        h = np.tanh(<span class="variable language_">self</span>.Wxh @ x + <span class="variable language_">self</span>.Whh @ h + <span class="variable language_">self</span>.bh) <span class="comment"># @是numpy中的矩阵乘法符号</span></span><br><span class="line">    y = <span class="variable language_">self</span>.Why @ y + <span class="variable language_">self</span>.by</span><br><span class="line">    <span class="keyword">return</span> y,h</span><br></pre></td></tr></table></figure>
<p>现在我们的RNNs神经网络已经可以运行了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> randn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createInputs</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> text.split(<span class="string">&quot; &quot;</span>):</span><br><span class="line">        v = np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">        v[word_to_idx[w]] = <span class="number">1</span></span><br><span class="line">        inputs.append(v)</span><br><span class="line">    <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / <span class="built_in">sum</span>(np.exp(x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNN</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size, hidden_size=<span class="number">64</span></span>):</span><br><span class="line">        <span class="comment"># weights</span></span><br><span class="line">        <span class="variable language_">self</span>.Whh = randn(hidden_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Wxh = randn(hidden_size,input_size) / <span class="number">1000</span></span><br><span class="line">        <span class="variable language_">self</span>.Why = randn(output_size,hidden_size) / <span class="number">1000</span></span><br><span class="line">        <span class="comment"># biases</span></span><br><span class="line">        <span class="variable language_">self</span>.bh = np.zeros((hidden_size,<span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.by = np.zeros((output_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">        h = np.zeros((<span class="variable language_">self</span>.Whh.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">            h = np.tanh(<span class="variable language_">self</span>.Wxh @ x + <span class="variable language_">self</span>.Whh @ h + <span class="variable language_">self</span>.bh)</span><br><span class="line">        y = <span class="variable language_">self</span>.Why @ h + <span class="variable language_">self</span>.by</span><br><span class="line">        <span class="keyword">return</span> y,h</span><br><span class="line"></span><br><span class="line">RNNs = RNN(vocab_size,<span class="number">2</span>)</span><br><span class="line">inputs = createInputs(<span class="string">&#x27;i am very good&#x27;</span>)</span><br><span class="line">out, h = RNNs.forward(inputs)</span><br><span class="line">probs = softmax(out)</span><br><span class="line"><span class="built_in">print</span>(probs)</span><br><span class="line"><span class="comment"># [[0.50000228],[0.49999772]]</span></span><br></pre></td></tr></table></figure>
<p>这里我们用到了softmax函数，softmax函数可以将任意的实值转换为概率（主要用于多分类任务）。它的核心作用是将网络的原始输出，转换为各类别的概率，使得所有概率之和为1。其公式如下
<span class="math display">$$
\begin{align*}
Softmax(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C}e^{z_j}}
\end{align*}
$$</span></p>
<h2 id="反向传播">反向传播</h2>
<p>为了训练我们RNNs，我们首先需要选择一个损失函数。对于分类模型，Softmax函数经常和交叉熵损失函数配合使用。它的计算方式如下：
<span class="math display">$$
\begin{align*}
L = -ln(p_c)
\end{align*}
$$</span>
其中<code>pc</code>是我们的RNNs对正确类别的预测概率（正面或负面）。例如，如果一个正面文本被我们的RNNs预测为90%的正面，那么可以计算出损失为：
<span class="math display">$$
\begin{align*}
L = -ln(0.90) = 0.105
\end{align*}
$$</span></p>
<p>既然有损失函数了，我们就可以使用梯度下降来训练我们的RNN以减小损失。</p>
<h3 id="计算">计算</h3>
<p>首先从计算<span class="math inline">$\frac{\partial L}{\partial
y}$</span>开始，我们有： <span class="math display">$$
\begin{align*}
L &amp;= -ln(p_c) = -ln(softmax(y_c)) \\
\frac{\partial L}{\partial y} &amp;= \frac{\partial L}{\partial p_c}*
\frac{\partial p_c}{\partial y_i} \\
\frac{\partial L}{\partial p_c} &amp;= -\frac{1}{p_c} \\
\frac{\partial p_c}{\partial y_i} &amp;= \begin{cases}
\frac{\partial p_i}{\partial y_i} =
\frac{e^{y_i}\sum_{j}e^{y_j}-(e^{y_i})^2}{(\sum_{j}e^{y_j})^2} =
p_i(1-p_i)&amp;\text{if c=i} \\
\frac{\partial p_c}{\partial y_i} =
\frac{e^{y_i}\sum_{j}e^{y_j}-(e^{y_i})^2}{(\sum_{j}e^{y_j})^2} =
-p_cp_i&amp;\text{if c!=i}
\end{cases} \\
\frac{\partial L}{\partial y} &amp;= \begin{cases}
-\frac{1}{p_i} * p_i(1-p_i) = p_i-1 &amp; \text{if c=i} \\
-\frac{1}{p_c} * (-p_cp_i) = p_i &amp; \text{if c!=i}
\end{cases}
\end{align*}
$$</span>
接下来我们尝试对<code>Why</code>和<code>by</code>的梯度，它们将最终隐藏状态转换为RNNs的输出。我们有：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial W_{hy}} &amp;=
\frac{\partial L}{\partial y} *\frac{\partial y}{\partial W_{hy}} \\
y &amp;= W_{hy}h_n + b_y \\
\\
\frac{\partial y}{\partial W_{hy}} &amp;= h_n \to
\frac{\partial L}{\partial W_{hy}} = \frac{\partial L}{\partial y}h_n \\
\frac{\partial y}{\partial b_{y}} &amp;= 1 \to
\frac{\partial L}{\partial b_{y}} = \frac{\partial L}{\partial y}
\end{align*}
$$</span>
最后我们还需要<code>Whh,Wxh</code>和<code>bh</code>的梯度。由于梯度在每一步中都会被使用，所以根据时间展开和链式法则，我们有：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial W_{xh}} &amp;=
\frac{\partial L}{\partial y}\sum_{t=1}^{T}\frac{\partial y}{\partial
h_t}*\frac{\partial h_t}{\partial W_{xh}} \\
\end{align*}
$$</span>
这是因为L会被<code>y</code>所影响，而<code>y</code>被<code>hT</code>所影响，而<code>hT</code>又依赖于<code>h(T-1)</code>直到递归到<code>h1</code>，因此<code>Wxh</code>通过所有中间状态影响到L，所以在任意时间t，<code>Wxh</code>的贡献为：
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial W_{xh}} \Big|_t &amp;=
\frac{\partial L}{\partial y}*\frac{\partial y}{\partial
h_t}*\frac{\partial h_t}{\partial W_{xh}} \\
\end{align*}
$$</span> 现在我们对其进行计算： <span class="math display">$$
\begin{align*}
h_t &amp;= tanh(W_{xh}x_t + W_{hh}h_{t-1}+b_h) \\
\frac{dtanh(x)}{dx} &amp;= 1-tanh^2(x) \\
\\
\frac{\partial h_t}{\partial W_{xh}} &amp;= (1-h_t^2)x_t \\
\frac{\partial h_t}{\partial W_{hh}} &amp;= (1-h_t^2)h_{t-1} \\
\frac{\partial h_t}{\partial b_h} &amp;= (1-h_t^2) \\
\end{align*}
$$</span> 最后我们需要计算出<span class="math inline">$\frac{\partial
y}{\partial h_t}$</span>。我们可以递归的计算它： $$ $$
由于我们是反向训练的，<span class="math inline">$\frac{\partial
y}{\partial h_{t+1}}$</span>是已经计算的最后一步的梯度<span
class="math inline">$\frac{\partial y}{\partial
h_n}=W_{hh}$</span>。至此为止我们的推导就结束了</p>
<h3 id="实现">实现</h3>
<p>由于反向传播训练需要用到前向传播中的一些数据，所以我们将其进行存储：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">    h = np.zeros((<span class="variable language_">self</span>.Whh.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 数据存储</span></span><br><span class="line">    <span class="variable language_">self</span>.last_inputs = inputs</span><br><span class="line">    <span class="variable language_">self</span>.last_hs = &#123;<span class="number">0</span>:h&#125;</span><br><span class="line">    <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs):</span><br><span class="line">        h = np.tanh(<span class="variable language_">self</span>.Wxh @ x + <span class="variable language_">self</span>.Whh @ h + <span class="variable language_">self</span>.bh)</span><br><span class="line">        <span class="variable language_">self</span>.last_hs[i+<span class="number">1</span>] = h   <span class="comment"># 更新存储</span></span><br><span class="line">    y = <span class="variable language_">self</span>.Why @ h + <span class="variable language_">self</span>.by</span><br><span class="line">    <span class="keyword">return</span> y,h</span><br></pre></td></tr></table></figure>
<p>现在我们可以开始实现<code>backprop()</code>了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self,d_y,learn_rate=<span class="number">2e-2</span></span>):</span><br><span class="line">    <span class="comment"># d_y: 是损失函数对于输出的偏导数 d_L/d_y 的结果</span></span><br><span class="line">    n = <span class="built_in">len</span>(<span class="variable language_">self</span>.last_inputs)</span><br><span class="line">    <span class="comment"># 计算dL/dWhy,dL/dby</span></span><br><span class="line">    d_Why = d_y @ <span class="variable language_">self</span>.last_hs[n].T</span><br><span class="line">    d_by = d_y</span><br><span class="line">    <span class="comment"># 初始化dL/dWhh,dL/dWxh,dL/dbh为0</span></span><br><span class="line">    d_Whh = np.zeros(<span class="variable language_">self</span>.Whh.shape)</span><br><span class="line">    d_Wxh = np.zeros(<span class="variable language_">self</span>.Wxh.shape)</span><br><span class="line">    d_bh = np.zeros(<span class="variable language_">self</span>.bh.shape)</span><br><span class="line">    <span class="comment"># 计算dL/dh</span></span><br><span class="line">    d_h = <span class="variable language_">self</span>.Why.T @ d_y  <span class="comment"># 因为dy/dh = Why 所以 dL/dh = Why * dL/dy</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随时间的反向传播</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(n)):</span><br><span class="line">        <span class="comment"># 通用数据 dL/dh * (1-h^2)</span></span><br><span class="line">        temp = (d_h * (<span class="number">1</span> - <span class="variable language_">self</span>.last_hs[t+<span class="number">1</span>] ** <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># dL/db = dL/dh * (1-h^2)</span></span><br><span class="line">        d_bh += temp</span><br><span class="line">        <span class="comment"># dL/dWhh = dL/dh * (1-h^2) * h_&#123;t-1&#125;</span></span><br><span class="line">        d_Whh += temp @ <span class="variable language_">self</span>.last_hs[t].T</span><br><span class="line">        <span class="comment"># dL/dWxh = dL/dh * (1-h^2) * x</span></span><br><span class="line">        d_Wxh += temp @ <span class="variable language_">self</span>.last_inputs[t].T</span><br><span class="line">        <span class="comment"># Next dL/dh = dL/dh * (1-h^2) * Whh</span></span><br><span class="line">        d_h = <span class="variable language_">self</span>.Whh @ temp</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度剪裁(防止梯度过大导致梯度爆炸问题)</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> [d_Wxh,d_Whh,d_Why,d_by,d_bh]:</span><br><span class="line">        np.clip(d,-<span class="number">1</span>,<span class="number">1</span>,out=d)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降训练</span></span><br><span class="line">    <span class="variable language_">self</span>.Whh -= learn_rate * d_Whh</span><br><span class="line">    <span class="variable language_">self</span>.Wxh -= learn_rate * d_Wxh</span><br><span class="line">    <span class="variable language_">self</span>.Why -= learn_rate * d_Why</span><br><span class="line">    <span class="variable language_">self</span>.bh -= learn_rate * d_bh</span><br><span class="line">    <span class="variable language_">self</span>.by -= learn_rate * d_by</span><br></pre></td></tr></table></figure>
<p>由于这一部分的编写涉及到矩阵的变换，所以在编写时，一定要清楚每个变量的状态，以免造成数学错误。例如，以上程序中<code>@</code>的左乘右乘顺序不能随意改变。</p>
<h2 id="训练">训练</h2>
<p>我们现在需要写一个接口，将我们的数据”喂”给神经网络，并量化损失和准确率，用于训练我们的神经网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">processData</span>(<span class="params">data, backprop=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 打乱数据集 避免顺序偏差</span></span><br><span class="line">    items = <span class="built_in">list</span>(data.items())</span><br><span class="line">    random.shuffle(items)</span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    num_correct =<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> items:</span><br><span class="line">        inputs = createInputs(x)</span><br><span class="line">        target = <span class="built_in">int</span>(y)</span><br><span class="line">        <span class="comment"># 前向传播计算</span></span><br><span class="line">        out,_ = RNN.forward(inputs)</span><br><span class="line">        probs = softmax(out)</span><br><span class="line">        <span class="comment"># 计算损失与准确度</span></span><br><span class="line">        loss -= np.log(probs[target])</span><br><span class="line">        num_correct += <span class="built_in">int</span>(np.argmax(probs) == target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> backprop:</span><br><span class="line">            d_L_d_y = probs</span><br><span class="line">            d_L_d_y[target] -= <span class="number">1</span></span><br><span class="line">            RNN.backprop(d_L_d_y)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss/<span class="built_in">len</span>(data),num_correct /<span class="built_in">len</span>(data)</span><br></pre></td></tr></table></figure>
<p>这里对于<span class="math inline">$\frac{\partial L}{\partial
y}$</span>的初始化我们需要重点关注一下。由于我们使用的是，交叉熵损失+Softmax函数来进行处理。对于输出层，我们有一个简洁的表达式来进行处理:
<span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial y} = probs - onehot(target)
\end{align*}
$$</span> 这里我选用AI的解释来直观的感受为什么这么做:</p>
<figure>
<img src="https://s2.loli.net/2025/08/17/u6G9YxHyca78VNA.png"
alt="image.png" />
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>我们在前面也推导过这个原因 <span class="math display">$$
\begin{align*}
\frac{\partial L}{\partial y} &amp;= \begin{cases}
-\frac{1}{p_i} * p_i(1-p_i) = p_i-1 &amp; \text{if c=i} \\
-\frac{1}{p_c} * (-p_cp_i) = p_i &amp; \text{if c!=i}
\end{cases}
\end{align*}
$$</span> 最后我们编写训练循环，来对我们的内容进行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    train_loss, train_acc = processData(train_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;--- Epoch %d&#x27;</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Train:\tLoss %.3f | Accuracy: %.3f&#x27;</span> % (train_loss, train_acc))</span><br><span class="line"></span><br><span class="line">        test_loss, test_acc = processData(test_data, backprop=<span class="literal">False</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Test:\tLoss %.3f | Accuracy: %.3f&#x27;</span> % (test_loss, test_acc))</span><br></pre></td></tr></table></figure>
<p>执行可以看到完整的训练过程。</p>
<h2 id="使用">使用</h2>
<p>既然完成了训练，那么我们可以尝试与其进行沟通，我们可以写一个接口用于和它进行对话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">probs, mid=<span class="number">0.5</span></span>):</span><br><span class="line">    positive_prob = probs[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Yes,you are positive ^_^&quot;</span> <span class="keyword">if</span> positive_prob &gt; mid <span class="keyword">else</span> <span class="string">&quot;No,you are negative qwq&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;please wait some time to train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    processData(train_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    text = <span class="built_in">input</span>(<span class="string">&quot;please input a sentence: &quot;</span>).lower()</span><br><span class="line">    inputs = createInputs(text)</span><br><span class="line">    out, _ = rnn.forward(inputs)</span><br><span class="line">    probs = softmax(out)</span><br><span class="line">    <span class="built_in">print</span>(predict(probs))</span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2025/08/17/Zt65XDQbBa9yOPT.png" /></p>
<p>哈哈效果还可以，只不过只能检测到训练集中用过的单词。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ylin"
      src="/images/1.jpg">
  <p class="site-author-name" itemprop="name">Ylin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">94</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Ylin07" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ylin07" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/2504_90550008?spm=1010.2135.3001.5343" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;2504_90550008?spm&#x3D;1010.2135.3001.5343" rel="noopener" target="_blank">Ylin's CSDN</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/ylin07" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;ylin07" rel="noopener" target="_blank">Ylin's 博客园</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://bluestar-34.github.io/" title="https:&#x2F;&#x2F;bluestar-34.github.io&#x2F;" rel="noopener" target="_blank">Neroblue's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://haoine.github.io/" title="https:&#x2F;&#x2F;haoine.github.io&#x2F;" rel="noopener" target="_blank">Haoine's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://admintor889.github.io/" title="https:&#x2F;&#x2F;admintor889.github.io&#x2F;" rel="noopener" target="_blank">Cnext's Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://auberginewly.site/" title="https:&#x2F;&#x2F;auberginewly.site&#x2F;" rel="noopener" target="_blank">auberginewly's Blog</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ylin</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">224k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">13:34</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
